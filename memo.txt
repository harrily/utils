1、lsb_release -a
cat /etc/redhat-release	
wang.ying.nan/5678@lstech.comm
2、
ssh-keygen -t rsa     #在ambari-server生成密钥对
for num in seq 1 3
do ssh-copy-id -i /root/.ssh/id_rsa.pub root@hadoop-$num
done

	1.每台依次执行ssh-keygen
	2.cat /root/.ssh/id_rsa.pub >>/root/.ssh/authorized_keys
	3.scp /root/.ssh/authorized_keys  root@10.0.24.205:/root/.ssh/authorized_keys
	4.循环执行2,3直至其中一台有全部主机的authorized_keys，最后分发此authorized_keys至所有主机。

3、 root/5678@lstech.com
	http://10.0.40.203/zentao  wangyingnan/wangyingnan123


	正式环境:root/Tlg.data2020@
	    	bigdata/Tlg.data2020


1、权限没有问题，版本也正确（centOS-7.2 ）
2、核实配置如下。	
	
主机	内存(g)	系统盘(g)+数据盘(g)	
10.0.24.105	62	50+550
10.0.24.106	62	50+550
10.0.24.107	32	50+500
10.0.24.110	32	50+500
10.0.24.111	32	50+500

4、hostnamectl set-hostname  lyzk01

5、hosts文件

10.0.24.105	lymaster01
10.0.24.106	lymaster02
10.0.24.107	lyzk01	lymysql01	kettle
10.0.24.110	lyzk02	lyambari	lymysql02	lyweb01
10.0.24.111	lyzk03	nginx lyweb02

6、ntp
systemctl is-enabled ntpd

systemctl enable ntpd

systemctl start ntpd

7、
systemctl disable firewalld

systemctl stop firewalld

systemctl status firewalld


8、/opt/jdk1.8.0_221
export JAVA_HOME=/opt/jdk1.8.0_221
export CLASSPATH=$JAVA_HOME/lib/
export PATH=$PATH:$JAVA_HOME/bin
export PATH JAVA_HOME CLASSPATH


export JAVA_HOME=/opt/jdk1.8.0_221
export JAVA_BIN=/opt/jdk1.8.0_221/bin
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
9.

net.ipv6.conf.all.disable_ipv6=1
net.ipv6.conf.default.disable_ipv6=1


10、安装kettle ，用户lyadmin/lyadmin123 （lyzk01 ，vnc暂无   ==>  开启lyzk02和lyzk03）
    -、根目录  /opt/data-integration
	-、安装vncServer
	1-、提示安装 libwebkitgtk-1_0-0-2.4.10-7.2.x86_64.rpm  （/opt/kettle_need）
		-、下载对应rpm依赖（或者卸载多余的依赖）
			地址：http://rpmfind.net/linux/rpm2html/search.php?query=libwebp.so.5%28%29%2864bit%29&submit=Search+...&system=&arch=
		-、安装对应以来即可。
			资料参考：https://blog.csdn.net/m0_37618809/article/details/81015492
		-、cd /opt/kettle_need
		rpm -ivh timezone-2018d-48.1.x86_64.rpm --force
		rpm -e --nodeps libicu-50.1.2-15.el7.x86_64
		rpm -ivh libwebp5-0.4.3-7.1.x86_64.rpm
		rpm -ivh libpng16-16-1.6.8-10.1.x86_64.rpm
		rpm -ivh libjpeg8-8.1.2-38.1.x86_64.rpm
		rpm -ivh libicu52_1-data-52.1-15.1.x86_64.rpm
		rpm -ivh libicu52_1-52.1-15.1.x86_64.rpm		
		rpm -ivh libjavascriptcoregtk-1_0-0-2.4.10-7.2.x86_64.rpm
		rpm -ivh geoclue-0.12.99-7.el7.x86_64.rpm
		rpm -ivh libwebkitgtk-1_0-0-2.4.10-7.2.x86_64.rpm

-、解决kettle连接hdfs（HA），手动切换主机ip问题。
1、windows主机hosts文件，添加映射
10.0.24.105 mycluster
10.0.24.106 mycluster
2、kettle连接hdfs时，设置hostname=mycluster， 即可解决hdfs双主切换问题。

11、mysql 
-、
root/lymysql123
lymysql/lymysql
ambari/Ambari-123	
hive/Hive-123

-、lyzk01主机mysql驱动下载失败:yum install mysql-connector-java
-、mysql-bin.000001 |      154 |

CHANGE MASTER TO MASTER_HOST='10.0.24.110', MASTER_USER='repl', MASTER_PASSWORD='Zxt1234!', MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=154

CHANGE MASTER TO MASTER_HOST='10.0.24.107', MASTER_USER='repl', MASTER_PASSWORD='Zxt1234!', MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=154


	
增删改查权限:
GRANT Select,Update,insert,delete ON *.* TO 'lymysql'@"%"  IDENTIFIED BY "lymysql"

FLUSH PRIVILEGES




/**
create database lymysql character set utf8 
  
CREATE USER 'lymysql'@'%'IDENTIFIED BY 'lymysql'

GRANT ALL PRIVILEGES ON *.* TO 'lymysql'@'%'

FLUSH PRIVILEGES

**/

create database oozie character set utf8 
  
CREATE USER 'oozie'@'%'IDENTIFIED BY 'Oozie-123'

GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%'

FLUSH PRIVILEGES



CHANGE MASTER TO MASTER_HOST='10.135.104.74', MASTER_USER='repl', MASTER_PASSWORD='Zxt1234!', MASTER_LOG_FILE='mysql-bin.000002',MASTER_LOG_POS=154




--、mysql  连接 too many
	1-、show variables like "max_connections"

	2-、 set GLOBAL max_connections=500

	参考资料:https://jingyan.baidu.com/article/fc07f989c5c6bd52fee5192c.html

12、ambari 数据源repo文件

#VERSION_NUMBER=2.4.2.0-136
[Updates-ambari-2.4.2.0]
name=ambari-2.4.2.0 - Updates
baseurl=http://10.0.24.110/ambari/AMBARI-2.4.2.0/centos7/2.4.2.0-136/
gpgcheck=1
gpgkey=http://10.0.24.110/ambari/AMBARI-2.4.2.0/centos7/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
enabled=1
priority=1


#VERSION_NUMBER=2.5.3.0-37
[HDP-2.5.3.0]
name=HDP Version - HDP-2.5.3.0
baseurl=http://10.0.24.110/ambari/HDP/centos7/
gpgcheck=1
gpgkey=http://10.0.24.110/ambari/HDP/centos7/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
enabled=1
priority=1
[HDP-UTILS-1.1.0.21]
name=HDP-UTILS Version - HDP-UTILS-1.1.0.21
baseurl=http://10.0.24.110/ambari/HDP-UTILS-1.1.0.21/
gpgcheck=1
gpgkey=http://10.0.24.110/ambari/HDP-UTILS-1.1.0.21/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
enabled=1
priority=1


yum clean all

yum list update

yum makecache

yum repolist


10.0.24.110

vi  /etc/ambari-agent/conf/ambari-agent.ini 


12、lybigdata  = clusterName

host文件：
lymaster01
lymaster02
lyzk01
lyzk02
lyzk03

amari-agent手动注册：
	1.每台都安装
	2.并启动agent查看日志是否注册正常
	
	
13、yum 提示 Cannot find a valid baseurl for repo: base/7/x86_64
mv CentOS-Base.repo CentOS-Base.repo.bak
	-、解决：修改CentOS-Base.repo为163地址。


mysql-community-server

CREATE  TABLE IF NOT EXISTS test_1 (
id string COMMENT 'id',
name string )
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'


CREATE  TABLE IF NOT EXISTS test.test_5 (
id string COMMENT '我是id',
name string )
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'


 load data local inpath '/opt/test/test.txt' into table test.test_1

 load data local inpath '/root/shell/hive/test/mada.txt' into table dwmiddle.dwb_purchase_report_cd_test partition(day='2020-09-06_test')

 load data local inpath '/root/shell/hive/test/mada.txt' overwrite into table dwmiddle.dwb_purchase_report_cd_test partition(day='2020-09-06_test')

 load data local inpath '/root/shell/hive/test/5G.txt' overwrite into table dwmiddle.dwm_dim_cd_by_other_test partition(day='2020-09-06_test',type='test')

 load data local inpath '/root/bc_recyle.txt' into table test.bc_recyle partition(day='2021-04-27')

  load data local inpath '/root/shell/hive/test/5G.txt' into table dwbase.dwb_purchase_report_by_5g partition(day='2020-09-18')



 load data local inpath '/root/shell/hive/test/db.txt' into table dwmiddle.dwm_moqie_db partition(day='2022-01-01')
 
14、 
	-、linux账号  【useradd xxx， passwd xxx】
		root/5678@lstech.com
		lyadmin/lyadmin123
		王英楠	  wangyingnan/wangyingnan123
		曾茂泉	  zengmaoquan/zengmaoquan123
		钟海	  zhonghai/zhonghai123
		周健	  zhoujian/zhoujian123
	-、hive权限问题  jane.zhou1
hadoop fs -mkdir /user/wang.ying.nan

hadoop fs -chown -R wang.ying.nan:hdfs /user/wang.ying.nan


hadoop fs -mkdir /user/zhong.hai

hadoop fs -chown -R zhong.hai:hdfs /user/zhong.hai


hadoop fs -mkdir /user/jane.zhou1

hadoop fs -chown -R jane.zhou1:hdfs /user/jane.zhou1


hadoop fs -mkdir /user/zeng.mao.quan

hadoop fs -chown -R zeng.mao.quan:hdfs /user/zeng.mao.quan


hadoop fs -mkdir /user/admin.ly

hadoop fs -chown -R admin.ly:hdfs /user/admin.ly
	

hadoop fs -mkdir /user/root
hadoop fs -chown -R root:hdfs /user/root			
			
	-、ambari 
		admin	  admin/pCWldx0zqo
		普通用户  lyambari/lyambari

--、add_user.sh
#!/bin/bash

cat $1 | while read line
do
name=`echo $line |awk -F '/' '{print $1}' `
passwd=`echo $line |awk -F '/' '{print $2}'`
echo $name
echo $passwd
useradd $name
echo "$passwd" | passwd --stdin $name &> /dev/null
echo "用户$name创建完成，默认密码是：$passwd"
done



15、nginx启动和配置
按照文档中的安装补助，wget
安装目录:/usr/local/src/nginx-1.10.2
重启:/usr/local/nginx/sbin/nginx -s reload
启动:/usr/local/nginx/sbin/nginx

配置目录：/usr/local/nginx/

如果没有安装apache，可以安装如下软件，
就有htpasswd这个命令了
yum -y install httpd-tools
	
加密：
htpasswd -c /usr/local/src/zip/nginx-1.10.2/conf/conf.d/htpasswd lynginx
认证账号密码：
uatnginx/uatnginx123

17、
启动vncServer：
	vncserver :1
连接密码：lytechvnc   

堡垒机vnc port ： 5899 5900

修改默认端口： vim /usr/bin/vncserver
	-、$vncPort = 5900 + $displayNumber

	   if (!bind(S, pack('S n x12', $AF_INET, 5900 + $n))) {
	   
修改用户配置： vim /etc/sysconfig/vncservers

	VNCSERVERS="1:root"
	VNCSERVERARGS[1]="-geometry 1366x768 -nolisten  tcp"


启动提示beause，删除：/tmp/.X11-unix/X


日志报错查看：cat  /root/.vnc/lyzk01:1.log


-、在lyzk02上安装
:yum install tigervnc-server
:yum install vnc

tigervnc-license-1.8.0-13.el7.noarch
tigervnc-server-minimal-1.8.0-13.el7.x86_64
tigervnc-server-1.8.0-13.el7.x86_64

gtk-vnc2-0.5.2-7.el7.x86_64


gvnc-0.5.2-7.el7.x86_64


18、配置presto

connector.name=hive-hadoop2
hive.metastore.uri=thrift://lymaster02:9083
hive.config.resources=/usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml,/usr/hdp/2.5.3.0-37/hadoop/conf/hdfs-site.xml


node.environment=production
node.id=107
node.data-dir=/opt/presto-server-0.191/presto_data

coordinator=true
node-scheduler.include-coordinator=true
http-server.http.port=18080
query.max-memory=5GB
query.max-memory-per-node=1GB
discovery-server.enabled=true
discovery.uri=http://10.0.24.107:18080

备注:需要每台节点都启动。
关闭: cd /opt/presto-server-0.191/bin/

./launcher stop  (每个节点)


19、
lyzk03

20、告警：HDFS Storage Capacity Usage(Daily)

HDFS存储使用量（每天）	This service-level alert is triggered if the increase in storage capacity usage deviation has grown beyond the specified threshold within a day period.
一天中存储容量使用率增量超过特定阈值时触发此服务级告警。

the variance for this alert is  8629160822B【1G】 which is 89% of the 9673345517B【1.12G】 average (4836672759B【0.56G】 is limit)


21、add partition
alter table YIFEI.ODS_YIFEI_DJNDYT3_ACMMA drop if exists partition (day='2019-09-06') 


alter table YIFEI.ODS_YIFEI_DJNDYT3_ACMMA add partition (day='2019-09-06') location '/warehouse/yifei/ods/ODS_YIFEI_DJNDYT3_ACMMA/day=2019-09-06'



-、kettle导入数据至hive，add partition 权限问题
	-、kettle连接时 ，指定用户
	-、用户添加至 hdfs组， /user/下，
	-、linux使用su，不能在/home目录下，有可能导致su切换失败。
	
	
22、
-、hbase删除表，提示表不存在
	-、删除zookeeper中过期数据
	/usr/hdp/current/zookeeper-client/bin/zkCl
	-、再创建已经再zk删除的表，然后再删除即可。
	
https://www.2cto.com/net/201704/622211.html

22、phoeinx 连接
	create 'test', {NAME => 'f1'}
	-、 So all Phoenix tables are hbase tables, but hbase tables are not necessarily Phoenix tables.
	-、创建表
	CREATE TABLE "test_phoenix3" (id VARCHAR  ,city VARCHAR , population BIGINT CONSTRAINT my_pk PRIMARY KEY (id,city))

	create table test_phoenix (id varchar, name varchar, city varchar CONSTRAINT PK PRIMARY KEY (id,name))

	create table test_phoenix2 (id varchar, name varchar, city varchar CONSTRAINT my_pk PRIMARY KEY (id))

	-、更新表
	UPSERT INTO test_phoenix (id, name, city) values ('111','New York','USA')

	UPSERT INTO test_phoenix (id, name, city) values ('112','Beijing','China')

https://www.cnblogs.com/xiaoliu66007/p/9377922.html
	-、二级索引
		-、
			以下内容将在v1和v2列上创建一个索引,并在索引中包含v3列,以防止从原始数据表中获取该列:
			CREATE INDEX my_index ON test_phoenix(id,name)INCLUDE(city)
		-、
			CREATE INDEX my_index ON test_phoenix (id) INCLUDE (city)
	

			CREATE INDEX my_index ON test_phoenix (id)


	https://github.com/forcedotcom/phoenix/wiki/Secondary-Indexing#mutable-indexing%EF%BC%89%E3%80%82

    select * from DW.DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH WHERE
    "DATE"  BETWEEN ''  AND ''
    and dim='' and material_name in ('','')
    ORDER BY "DATE" ASC , price DESC

drop index index1_c ON hao1


-、phoenix的元数据SYSTEM.CATALOG, SYSTEM.SEQUENCE, SYSTEM.STATS, and SYSTEM.FUNCTION，因为hdfs数据目录更换，元数据丢失，提示表不存在，解决方案
	-、删除zookeeper中过期数据
	-、stop hbasse
	-、 su hdfs；
		hbase clean --cleanAll； （谨慎使用，会删除所有hbase的表和元数据）
	-、restart hbase 即可。
	
-、phoenix创建包含多列主键的表，需要使用constraint pkname primary key

-、hbase添加支持二级索引，hbase-site.xml
old:
	hbase.coprocessor.master.classes=
	hbase.regionserver.wal.codec=org.apache.hadoop.hbase.regionserver.wal.WALCellCodec
new：
	hbase.coprocessor.master.classes=org.apache.phoenix.hbase.index.master.IndexMasterObserver
		hbase.regionserver.wal.codec=org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec
		hbase.region.server.rpc.scheduler.factory.class=org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory
		hbase.rpc.controllerfactory.class=org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory
	hbase.coprocessor.regionserver.classes=org.apache.hadoop.hbase.regionserver.LocalIndexMerger     √ 
	hbase.master.loadbalancer.class=org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer
true_new:
	hbase.regionserver.wal.codec=org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec
	hbase.region.server.rpc.scheduler.factory.class=org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory
	hbase.rpc.controllerfactory.class=org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory

	解决过程：	
		1)、上传phoenix-4.7.0-HBase-1.1-server.jar 至 lymaster01主机 /home/lyadmin/下
		2)、替换lymaster01，lymaster01，lyzk01,lyzk02,lyzk03 主机上的 /usr/hdp/2.5.3.0-37//phoenix/phoenix-server.jar
			备份：mv phoenix-4.7.0.2.5.3.0-37-server.jar  phoenix-4.7.0.2.5.3.0-37-server.jar-backup
			替换：mv phoenix-4.7.0-HBase-1.1-server.jar phoenix-4.7.0.2.5.3.0-37-server.jar
			
			mv phoenix-4.7.0.2.5.3.0-37-server.jar phoenix-4.7.0-HBase-1.1-server.jar
			mv phoenix-4.7.0.2.5.3.0-37-server.jar-backup phoenix-4.7.0.2.5.3.0-37-server.jar
			
		3)、CREATE INDEX MATERIAL_CUT_PRICE_INDEX ON DW.DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH("DATE",DIM,MATERIAL_NAME,PRICE,"YEAR", "MONTH" )INCLUDE(MATERIAL_8CODE)
	总结：
		1)、当前版本是4.7以上，不是4.7以下，无须替换jar包，故只需要配置
		<property>
		  <name>hbase.regionserver.wal.codec</name>
		  <value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
		</property>
		
		<property>
		  <name>hbase.region.server.rpc.scheduler.factory.class</name>
		  <value>org.apache.hadoop.hbase.ipc.PhoenixRpcSchedulerFactory</value>
		  <description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>
		</property>
		<property>
		  <name>hbase.rpc.controllerfactory.class</name>
		  <value>org.apache.hadoop.hbase.ipc.controller.ServerRpcControllerFactory</value>
		  <description>Factory to create the Phoenix RPC Scheduler that uses separate queues for index and metadata updates</description>
		</property>
			
		参考资料:https://phoenix.apache.org/secondary_indexing.html#
		https://www.jianshu.com/p/8155bf732b82 （详细解析二级索引 1、rowkey+phoenix 2、一表多索引）

-、phoenix创建二级索引超时解决：operate timeout
phoenix.query.timeoutMs=1800000 
hbase.regionserver.lease.period = 1200000 
hbase.rpc.timeout = 1200000   【ambari 默认为90s】
hbase.client.scanner.caching = 1000 （这个属性在Ambari中为Number of Fetched Rows when Scanning from Disk=1000）
hbase.client.scanner.timeout.period = 1200000 

参考资料：https://blog.csdn.net/u013850277/article/details/80935545

	-、追加问题， zookeeper的session超时
		初步原因分析--> 调大zookepper的超时时间（以前是90s）
			<property>
				<name>zookeeper.session.timeout</name>
				<value>120000</value>
			</property>
	


23、hive创建orc+snappy格式数据
	
new:'orc.compression' = 'snappy'

CREATE TABLE ly_test2(
  id string COMMENT 'id',
  name string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'


CREATE EXTERNAL TABLE `dwbase.dwb_purchase_report_by_5g`(
  `area` string COMMENT 'from deserializer', 
  `company_name` string COMMENT 'from deserializer', 
  `date` string COMMENT 'from deserializer', 
  `stock_in_audit_time` string COMMENT 'from deserializer', 
  `material_code` string COMMENT 'from deserializer', 
  `supplier_code` string COMMENT 'from deserializer', 
  `supplier_name` string COMMENT 'from deserializer', 
  `bu` string COMMENT 'from deserializer', 
  `material_attr` string COMMENT 'from deserializer', 
  `first_level` string COMMENT 'from deserializer', 
  `secondary_level` string COMMENT 'from deserializer', 
  `third_level` string COMMENT 'from deserializer', 
  `origin_firm` string COMMENT 'from deserializer', 
  `nation` string COMMENT 'from deserializer', 
  `location` string COMMENT 'from deserializer', 
  `material_describe` string COMMENT 'from deserializer', 
  `material_model` string COMMENT 'from deserializer', 
  `batch_number` string COMMENT 'from deserializer', 
  `tax_price` decimal(28,10) COMMENT 'from deserializer', 
  `currency_name` string COMMENT 'from deserializer', 
  `pay_condition` string COMMENT 'from deserializer', 
  `quantity` decimal(19,4) COMMENT 'from deserializer', 
  `unit` string COMMENT 'from deserializer', 
  `exchange_rate` decimal(19,4) COMMENT 'from deserializer', 
  `tax_rate` decimal(19,4) COMMENT 'from deserializer', 
  `account_day` decimal(19,4) COMMENT 'from deserializer', 
  `price` decimal(28,10) COMMENT 'from deserializer', 
  `tax_amount` decimal(28,10) COMMENT 'from deserializer', 
  `amount` decimal(28,10) COMMENT 'from deserializer', 
  `bechmark_price` decimal(28,10) COMMENT 'from deserializer', 
  `bechmark_amount` decimal(28,10) COMMENT 'from deserializer', 
  `cd_amount` decimal(28,10) COMMENT 'from deserializer', 
  `charger` string COMMENT 'from deserializer', 
  `po_number` string COMMENT 'from deserializer', 
  `req_number` string COMMENT 'from deserializer', 
  `branch_of_facoty` string COMMENT 'from deserializer', 
  `line` string COMMENT 'from deserializer', 
  `project` string COMMENT 'from deserializer', 
  `remarks` string COMMENT 'from deserializer', 
  `appoint` string COMMENT 'from deserializer', 
  `ori_proxy` string COMMENT 'from deserializer')
PARTITIONED BY ( 
  `day` string, 
  `type` string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\t'
stored AS orc 
LOCATION
  'hdfs://promycluster/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g'

tblproperties('orc.compression' = 'snappy')

CREATE EXTERNAL TABLE `dwservice.dws_mes_by_manual_rank`(
  `end_date` string COMMENT '效率计算日期', 
  `seq_num` string COMMENT '序号排名', 
  `emp_no` string COMMENT '工号', 
  `emp_name` string COMMENT '员工名称', 
  `team_id` string COMMENT '组别ID', 
  `ydqty` decimal(18,4) COMMENT '产值', 
  `worktime` decimal(18,4) COMMENT '工时H', 
  `labyuedproduc` decimal(18,4) COMMENT '约当效率(K/H)')
COMMENT '手工效率排行榜）'
PARTITIONED BY ( 
  `day` string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\t'
stored AS orc tblproperties('orc.compression' = 'snappy')


insert into table ly_test2 select * from test_1




CREATE EXTERNAL TABLE `dwmiddle.last_update_date`(
  `last_date` string COMMENT '本次分区抽取的最近数据日期' )
COMMENT '账期天数中间表  '
PARTITIONED BY (
   `day` string, 
   `data_sources` string,
   `update_date` string  )
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\t' 
stored AS orc tblproperties('orc.compression' = 'snappy')



CREATE TABLE test.ly_test2(
  id string COMMENT '真的是id',
  name string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'


原因
这是因为SequenceFile的表不能使用load来加载数据，只能导入sequence类型的数据

解决办法
曲线救国

先创建一个临时表（save as textfile），将数据导入进去，
然后再导入这个表里
insert into table ly_test2 select * from test_1


查看hive日志：/tmp/hive/hive.log




24、建模相关账号

-、用友
	-、
		PROD:
		10.100.1.4:jpmfapp
		username: sa
		password:alt-ctrl-del-win2k

	-、
		10.100.1.4:UFDATA_172_2019
		用户:lysa 密码:jpmf+123
	-、
		10.100.1.4
		test db: UFDATA_172_2019
		username: uf172 
		password：abcd-1234

-、易飞
	-、
		username:ly 
		password:2019@ly.com

		192.168.105.2 易飞80
		192.168.0.10 易飞90
		
		59.37.21.140
		
	-、LY数仓-ODS层易飞相关表
		DSCMB  公司
		ACTMA  科目
		CMSMF  币别
		CMSMG  汇率
		CMSMV  职员
		CMSME  部门
		PURMA  供应商
		COPMA  客户
		ACTTA  凭证
		ACTTB  凭证
		ACTLE  科目余额表

		--易飞90
			--DSCSYS90
				select * from DSCMB


			--DJNDYT2
				select * from ACTMA

				select * from CMSMF

				select * from CMSMG

				select * from CMSMV

				select * from CMSME

				select * from PURMA

				select * from COPMA

				select * from ACTTA

				select * from ACTTB

				select * from ACTLE


		--易飞80
			--DSCSYS
				select * from DSCMB


			--DJNDYT3
				select * from ACTMA

				select * from CMSMF

				select * from CMSMG

				select * from CMSMV

				select * from CMSME

				select * from PURMA

				select * from COPMA

				select * from ACTTA

				select * from ACTTB

				select * from ACTLE

		
-、EAS [neweas_75]
	-、	
		oracle 数据库 ：192.168.3.233:1521/lseas
		username: EAS_LINK
		password: eas_link
		
25、代办
	-、禅道任务--cc钉邮给余经理 √
	wangyingnan/yingnan123
	-、svn账号不能使用  √
	https://10.0.8.18/svn/lyproject/datacenter/data_warehouse
	-、摇号是否过期。√
	-、dbServer  	√
	-、数据抽取--尽量抽取最新的数据。
		
26、
-、hive多符号解决，执行MR提示“Class org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe not found”？
	-、办法1
		将hive-contrib.jar包拷贝到yarn的lib目录下
		hive 的jar移动至  hadooo-yarn中 /usr/hdp/2.5.3.0-37/hadoop-yarn/lib

		cp /usr/hdp/2.5.3.0-37/hive/lib/hive-contrib-1.2.1000.2.5.3.0-37.jar /usr/hdp/2.5.3.0-37/hadoop-yarn/lib

		cp /usr/hdp/2.5.3.0-37/hive/lib/hive-contrib-1.2.1000.2.5.3.0-37.jar /usr/hdp/2.5.3.0-37/hadoop-mapreduce/lib

			会将jar同时更新/usr/hdp/current/hadoop-yarn-xx/.. 下（貌似需要重启yarn）
		rm -rf  /usr/hdp/2.5.3.0-37/hadoop-yarn/lib/hive-contrib.jar


		ln -s  hive-contrib-1.2.1000.2.5.3.0-37.jar hive-contrib.jar
		
		在上一级目录执行：
		cp -r /usr/hdp/2.5.3.0-37/hadoop-yarn/lib/hive-contrib-1.2.1000.2.5.3.0-37.jar /usr/hdp/2.5.3.0-37/hadoop-yarn/

		ln -s /usr/hdp/2.5.3.0-37/hadoop-yarn/hive-contrib-1.2.1000.2.5.3.0-37.jar	/usr/hdp/2.5.3.0-37/hadoop-yarn/hive-contrib.jar

	 -、办法2
		 Class org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe not found
		 set hive.aux.jars.path=
		file:///usr/hdp/2.5.3.0-37/hive/lib/hive-contrib-1.2.1000.2.5.3.0-37.jar

	-、办法3(临时解决)
		add jar hdfs://mycluster:8020/user/root/hive-contrib-1.2.1000.2.5.3.0-37.jar
	
	-、办法4（最终解决）
		在${HIVE_HOME}【/usr/hdp/2.5.3.0-37/hive】中创建文件夹 auxlib，然后将hive-contrib-1.2.1000.2.5.3.0-37.jar文件放入该文件夹中。
		参考资料：https://blog.csdn.net/qianshangding0708/article/details/50381966
-、多字符分隔符建表语句： 
create table if not exists test.student3(sno string comment '学生编号',name string )comment '学生表' row format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' with serdeproperties ('field.delim'='^#')


	
	
	hadoop distcp -skipcrccheck -update hdfs://mycluster/warehouse/eas/ods/ods_eas_new_eas75_t_sm_supplyinfo  hdfs://10.0.24.204:8020/apps/hive/warehouse/eas.db/ods_eas_new_eas75_t_sm_supplyinfo
	
	CREATE EXTERNAL TABLE `eas.ods_eas_new_eas75_t_sm_supplyinfo`(
  `data_loadtime` string COMMENT 'from deserializer', 
  `fid` string COMMENT 'from deserializer', 
  `fcreatorid` string COMMENT 'from deserializer', 
  `fcreatetime` string COMMENT 'from deserializer', 
  `flastupdateuserid` string COMMENT 'from deserializer', 
  `flastupdatetime` string COMMENT 'from deserializer', 
  `fcontrolunitid` string COMMENT 'from deserializer', 
  `fname_l1` string COMMENT 'from deserializer', 
  `fname_l2` string COMMENT 'from deserializer', 
  `fname_l3` string COMMENT 'from deserializer', 
  `fnumber` string COMMENT 'from deserializer', 
  `fdescription_l1` string COMMENT 'from deserializer', 
  `fdescription_l2` string COMMENT 'from deserializer', 
  `fdescription_l3` string COMMENT 'from deserializer', 
  `fsimplename` string COMMENT 'from deserializer', 
  `fpurchaseorgid` string COMMENT 'from deserializer', 
  `fsupplierid` string COMMENT 'from deserializer', 
  `fmaterialitemid` string COMMENT 'from deserializer', 
  `fmmateiralrule` string COMMENT 'from deserializer', 
  `fmeasureunitid` string COMMENT 'from deserializer', 
  `fsuppliermaterialnumber` string COMMENT 'from deserializer', 
  `fsuppliermaterialname` string COMMENT 'from deserializer', 
  `fsuppliermaterialrule` string COMMENT 'from deserializer', 
  `fpurmeasureunitid` string COMMENT 'from deserializer', 
  `feffectualdate` string COMMENT 'from deserializer', 
  `funeffectualdate` string COMMENT 'from deserializer', 
  `fleadtime` decimal(10,0) COMMENT 'from deserializer', 
  `fqtyorderbottom` decimal(28,16) COMMENT 'from deserializer', 
  `fqtyordertop` decimal(28,16) COMMENT 'from deserializer', 
  `fisuseable` decimal(10,0) COMMENT 'from deserializer', 
  `fauditorid` string COMMENT 'from deserializer', 
  `fauditdate` string COMMENT 'from deserializer', 
  `fmaxprice` decimal(28,16) COMMENT 'from deserializer', 
  `fminprice` decimal(28,16) COMMENT 'from deserializer', 
  `flastprice` decimal(28,16) COMMENT 'from deserializer', 
  `fprice` decimal(28,16) COMMENT 'from deserializer', 
  `flastpurtime` string COMMENT 'from deserializer', 
  `fistaxprice` decimal(10,0) COMMENT 'from deserializer', 
  `fisbatchtemp` decimal(10,0) COMMENT 'from deserializer', 
  `fdiscountrate` decimal(28,16) COMMENT 'from deserializer', 
  `ftaxrate` string COMMENT 'from deserializer', 
  `fmaterialpurchasetype` decimal(10,0) COMMENT 'from deserializer', 
  `foperationno` string COMMENT 'from deserializer', 
  `findemnityprice` decimal(21,8) COMMENT 'from deserializer', 
  `ffixedexpenses` decimal(17,4) COMMENT 'from deserializer', 
  `ffixingqty` decimal(21,8) COMMENT 'from deserializer', 
  `ffromqty` decimal(21,8) COMMENT 'from deserializer', 
  `ftoqty` decimal(21,8) COMMENT 'from deserializer', 
  `frecorgid` string COMMENT 'from deserializer', 
  `fassistpropertyid` string COMMENT 'from deserializer', 
  `fcurrencyid` string COMMENT 'from deserializer', 
  `fpricesource` decimal(10,0) COMMENT 'from deserializer', 
  `fsourcebillno` string COMMENT 'from deserializer', 
  `fsourcebillentryseq` decimal(10,0) COMMENT 'from deserializer', 
  `cfyield` string COMMENT 'from deserializer', 
  `cfloss` string COMMENT 'from deserializer', 
  `cfnewtaxrate` decimal(28,16) COMMENT 'from deserializer')
PARTITIONED BY ( 
  `day` string)
row format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' with serdeproperties ('field.delim'='^#')



-- ROW FORMAT DELIMITED    FIELDS TERMINATED BY '^#' 



	

-、hive终端提示:Failed to connect to server: lymaster01/10.0.24.105:8032: retries get failed due to exceeded maximum allowed retries number: 0
java.net.ConnectException: 拒绝连接
	: 有可能是yarn 高可用切换导致。Resource manager HA主备切换，日志级别是warning，且 cannot be suppressed as of now， so不用处理，或者通过jar改变日志级别处理。
		参考资料：https://blog.csdn.net/zhouyuanlinli/article/details/81772100
	

-、beeline连接hive
beeline -u jdbc:hive2://lymaster02:10000 -n root	

!connect jdbc:hive2://lymaster02:10000

bin/beeline  -u jdbc:hive2://hadoop001:10000 -n root
 minlog-1.2.jar，objenesis-1.2.jar，reflectasm-1.07-shaded.jar
-、设置hive执行引擎
set hive.execution.engine=mr


-、hive启动卡住。，调用debug模式调试
hive -hiveconf hive.root.logger=debug,console
	-、发现 hadoop-yarn 日志 提示错误：
		Application application_1568622810022_0019 failed 2 times due to AM Container for appattempt_1568622810022_0019_000002 exited with exitCode: -1000 
		 File does not exist: hdfs://mycluster/tmp/hive/root/_tez_session_dir/dbedfcc7-d2de-4e32-a090-84acce507a5e/hive-contrib-1.2.1000.2.5.3.0-37.jar
		 
		卡顿任务的yarn任务概况：
			User: 	root
			Name: 	HIVE-1fa11078-4eb9-475d-b3ac-00b38723e8e8
			Application Type: 	TEZ
			Application Tags: 	
			Application Priority: 	0 (Higher Integer value indicates higher priority)
			YarnApplicationState: 	ACCEPTED: waiting for AM container to be allocated, launched and register with RM.
			Queue: 	default
			FinalStatus Reported by AM: 	Application has not completed yet.
			Started: 	星期二 九月 17 10:05:31 +0800 2019
			Elapsed: 	6mins, 45sec
			Tracking URL: 	ApplicationMaster
			Log Aggregation Status 	NOT_START
			Diagnostics: 	[星期二 九月 17 10:09:41 +0800 2019] Application is added to the scheduler and is not yet activated. Queue's AM resource limit exceeded. Details : AM Partition = <DEFAULT_PARTITION>
 AM Resource Request = <memory:4096, vCores:1>
 Queue Resource Limit for AM = <memory:10240, vCores:1>
 User AM Resource Limit of the queue = <memory:10240, vCores:1>
 Queue AM Resource Usage = <memory:8192, vCores:3>

			Unmanaged Application: 	false
			Application Node Label expression: 	<Not set>
			AM container Node Label expression: 	<DEFAULT_PARTITION> 
		
		解决：
			-、其中:AM Limit = 48(3个node) * 0.2 = 9.6G
			yarn.scheduler.capacity.maximum-am-resource-percent（调大） = 0.2
			增大 Memory allocated for all YARN containers on a node = 24G
			减少Container的memory的大小。
			-、降低driver的memory的大小。
	-、/tmp/root/hive.log 
	 Localizing resource because it does not exist: file:/usr/hdp/2.5.3.0-37/hive/auxlib/hive-contrib-1.2.1000.2.5.3.0-37.jar to dest: hdfs://mycluster/tmp/hive/root/_tez_session_dir/ac725591-a34a-4220-bb7e-220c59689dc7/hive-contrib-1.2.1000.2.5.3.0-37.jar
	 
-- 查看yarn任务日志
yarn logs -applicationId  application_1582567689398_0006

strerr日志： uatdata04
/opt/hadoop/yarn/log/application_1578592840675_0010/container_e03_1578592840675_0010_01_000001/stderr

27、
唐经理，你好，我是数据中心的工程师王英楠，根据上次提供的基础表，发现EAS、用友、易飞相关的字段的值展示的方式不一致，例如EAS中FAccountID，易飞LE001，以及用友ccode是代表同一含义（科目），但是这些字段分别表示的值却是FAccountID=1D6jbpbXQ5iqntNR56RBSJ2pmCY=，LE001=1002.09，ccode=1122001。我这边想知道这些不同系统相同含义的字段是怎样来关联的，或者不同系统中的这些值是怎么定义的。另一方面报表如需展示此字段（科目），是以那个系统的数值方式为准？还是不用系统的都要同时展示呢？


是不是可以这样理解：
1、建模，不同系统相同含义的字段，基本无须关联，只需合并不同系统的字段为同一字段，然后做好映射（即这些数值来源于那个系统，代表什么含义以及原字段的信息）
2、财务所需的报表展示时，需要指定数据来源于那个系统。
3、不同系统中相同含义的字段的数值显示不一致，目前也无需统一数值（也无法统一数值，因为不知道他们原本数值之间的关系），展示时标记其来自于那个系统即可。


28、
DW ：data warehouse 翻译成数据仓库
DW数据分层，由下到上为 DWD,DWB,DWS
DWD：data warehouse detail 细节数据层，有的也称为 ODS层，是业务层与数据仓库的隔离层
DWB：data warehouse base 基础数据层，存储的是客观数据，一般用作中间层，可以认为是大量指标的数据层。
DWS：data warehouse service 服务数据层，基于DWB上的基础数据，整合汇总成分析某一个主题域的服务数据，一般是宽表。

拥有缺失表：
GL_AccAttachs  


-、DECIMAL和int如果不存在为null还是默认为0？
	-、DECIMAL 默认为 NULL 0.0000
	   float 默认为 NULL 0.0000000
	   int  默认为  NULL
	-、同时出现decimal，floor，int类型，优先合并为declmal或floor保留精度。
	   
-、EAS中 T_GL_VoucherEntry 与 T_GL_Voucher怎么关联？
	-、暂时找不到关联，union all 两张表
-、同时出现md，mc 且数值不一致， 取那个？
	-、md为0，取md，反之亦然
	
-、mb,cbegind_c(me,cendd_c)


存在问题：
	科目余额表
		EAS库：
			已确定关联：eas.ods_eas_new_eas75_t_gl_accountbalance.FPeriodID = eas.ods_eas_new_eas75_t_bd_period.fid
			未知关联：  eas.ods_eas_new_eas75_t_gl_accountbalance的FaccountID 与eas.ods_eas_new_eas75_t_bd_accountview未找到关联字段。
						eas.ods_eas_new_eas75_t_gl_accountbalance的FCurrencyID 与eas.ods_eas_new_eas75_t_bd_currency 未找到关联字段。
						eas.ods_eas_new_eas75_t_gl_accountbalance的FOrgUnitID 与eas.ods_eas_new_eas75_t_org_company 未找到关联字段。				
	凭证表：
		EAS库：	
			-、eas.ods_eas_new_eas75_t_gl_voucherentry 与eas.ods_eas_new_eas75_t_bd_accountview 未找到关联字段。
		用友：
			-、yongyou.ods_yongyou_ufdata_072_2019_gl_accattachs 表数据为空.
			-、字段原币金额（md/mc），在用友库中yongyou.ods_yongyou_ufdata_072_2019_gl_accvouchz中 md与mc字段互补。
			
创建Hive基础表语句：
	CREATE EXTERNAL TABLE IF NOT EXISTS base.dwb_base_t_voucher(
	  finaorg string COMMENT '财务组织',
	  reserved2 string COMMENT '预留字段2')
	PARTITIONED BY (
	  day string)
	ROW FORMAT DELIMITED
	  FIELDS TERMINATED BY '^#'
	 STORED AS orc 
	 LOCATION 'hdfs://mycluster/warehouse/base/dwb/DWB_base_t_voucher'
	 tblproperties('orc.compression' = 'snappy')



	CREATE EXTERNAL TABLE IF NOT EXISTS base.dwb_base_t_balance(
	  year string COMMENT '会计年度',
	  period string COMMENT '会计期间',
	  account string COMMENT '科目',
	  reserved2 string COMMENT '预留字段2')
	PARTITIONED BY (
	  day string)
	ROW FORMAT DELIMITED  FIELDS TERMINATED BY '^#'
	 STORED AS orc
	 LOCATION 'hdfs://mycluster/warehouse/base/dwb/DWB_base_t_balance'
	 tblproperties('orc.compression' = 'snappy')

	 
汇总：	
		1-、因基础表部分关联关系尚不清晰，故所建基础表仍以hive为主，数据存储格式为snappy+orc。			
		2-、不存在的字段值处理：
			-、数值（decimal，int，float）尽量用null来表示（除非确定不会产生歧义），以免在不了解业务的情况产生歧义。
			-、字符串用''表示，节省空间。

		3-、创建基础表，尽量预留字段，以免后期添加字段修改表结构。
		4-、若字段值为decimal，int，float等数值类型，统一使用精度高的类型。
		5-、存在String类型的数值 = 000001234.23，数值转换时注意保留精度。（强转int，会导致数值变为0）
			
C:\Users\wang.ying.nan\.dbeaver4\General\Scripts

  
 29.hive元数据mysql 更换（lymaster02 --> lymysql01）
 30.hive注释汉字乱码解决（并未真正解决【多分隔符乱码未解决】）：
	1-、 
		-、hive 设置连接jdbc的编码 ?createDatabaseIfNotExist=true&amp
useUnicode=true&amp
characterEncoding=UTF-8
		-、修改hive元数据的编码。
		-、设置：Database URL
			jdbc:mysql://lymysql01/hive?createDatabaseIfNotExist=true&useUnicode=true&characterEncoding=UTF-8
		参考资料：https://blog.csdn.net/dwt1415403329/article/details/83062336
		
		以下方案系测试使用，并没有什么真是作用。
		方案1：
			1-、删除索引 CREATE INDEX PCS_STATS_IDX ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME,PARTITION_NAME) USING BTREE

				-、drop index PCS_STATS_IDX on PART_COL_STATS 
  不起作用，需要删除索引所在的表。
		方案2：
			1-、修改ambari 中hive的script脚本中编码 
		
					0.5.0-to-0.6.0
					0.6.0-to-0.7.0
					0.7.0-to-0.8.0
					0.8.0-to-0.9.0
					0.9.0-to-0.10.0
					0.10.0-to-0.11.0
					0.11.0-to-0.12.0
					0.12.0-to-0.13.0
					0.13.0-to-0.14.0
					0.14.0-to-1.1.0
					1.1.0-to-1.2.0
					1.2.0-to-1.2.1000
					
				一、操作目录：lymaster02   （备份目录  /opt/ambariHivemetaBackup/ambari-hiveMeta-mysql-backup/）
					/usr/hdp/2.5.3.0-37/hive/scripts/metastore/upgrade/mysql
	
					1)-、注释掉创建索引的语句, 注释 insert into version 的冲突。
						grep 'CREATE INDEX' hive-*schema-*.mysql.sql
						grep 'CREATE INDEX' *-HIVE-*.mysql.sql
						grep 'CREATE INDEX' upgrade-*-to-*.mysql.sql
						
							sed -i 's/^CREATE INDEX/-- CREATE INDEX /g' hive-*schema-*.mysql.sql
							#sed -i 's/^CREATE INDEX/-- CREATE INDEX /g'	*-HIVE-*.mysql.sql
							
						grep 'INSERT INTO VERSION' hive-*schema-*.mysql.sql
						grep 'INSERT INTO VERSION' *-HIVE-*.mysql.sql
						grep 'INSERT INTO VERSION' upgrade-*-to-*.mysql.sql
						
							sed -i 's/^INSERT INTO VERSION/-- INSERT INTO VERSION /g' hive-*schema-*.mysql.sql
							sed -i 's/^INSERT INTO VERSION/-- INSERT INTO VERSION /g' *-HIVE-*.mysql.sql
						
					2)-、创建表添加 CREATE TABLE IF NOT EXISTS
	
						grep 'CREATE TABLE' hive-*schema-*.mysql.sql
						grep 'CREATE TABLE' *-HIVE-*.mysql.sql
						grep 'CREATE TABLE' upgrade-*-to-*.mysql.sql
					
							sed -i 's/^CREATE TABLE \([^I][^F]\)/CREATE TABLE IF NOT EXISTS \1/g' hive-*schema-*.mysql.sql
							sed -i 's/^CREATE TABLE \([^I][^F]\)/CREATE TABLE IF NOT EXISTS \1/g' *-HIVE-*.mysql.sql
					
					3)-、hive元数据 修改注释等设置编码（每个脚本后追加 ）
					
							cat tmp.txt >> hive-*schema-*.mysql.sql
						
						alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8

						alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

						alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 

						alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8

						alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

					4)-、
						for file in  ls /usr/hdp/2.5.3.0-37/hive/scripts/metastore/upgrade/mysql/
						do

								echo $file
								echo "===="
								echo "
									alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8

									alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

									alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 

									alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8

									alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

								"
								>> /usr/hdp/2.5.3.0-37/hive/scripts/metastore/upgrade/mysql/$file
						done
				二、操作目录寻找错误，实际上是：/usr/hdp/2.5.3.0-37/hive2/scripts/metastore/upgrade/mysql/hive-schema-2.1.0.mysql.sql
			（备份目录   原数据： /opt/ambariHivemetaBackup/ambari-hive2Meta-mysql-backup/
						修改后数据：/opt/ambariHivemetaBackup/ambari-hive2Meta-mysql-backup-repair/）
					1-、grep 'CREATE INDEX' hive-*schema-2.1.0.mysql.sql
						sed -i 's/^CREATE INDEX/-- CREATE INDEX /g' hive-*schema-2.1.0.mysql.sql
					2-、grep 'CREATE TABLE' hive-*schema-2.1.0.mysql.sql | grep -v 'IF NOT EXISTS'
						sed -i 's/^CREATE TABLE \([^I][^F]\)/CREATE TABLE IF NOT EXISTS \1/g' hive-*schema-2.1.0.mysql.sql
					3-、grep 'INSERT INTO VERSION' hive-*schema-2.1.0.mysql.sql
						sed -i 's/^INSERT INTO VERSION/-- INSERT INTO VERSION /g'	hive-*schema-2.1.0.mysql.sql
					4-、修改编码
						alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8

						alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

						alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 

						alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8

						alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

						
					测试：hive -e "load data local inpath '/root/unicode_test.txt' overwrite into table test.unicode_test" 

					utf8	
					utf8_general_ci
					hive -e "load data local inpath '/root/test/test_supply/purchase_info' overwrite into table test.purchase_info" 

					
		2-、hive 重启，每次都会执行 -initSchema -dbType
				
			修改ambari中initinal 的代码
				/usr/hdp/current/hive-server2-hive2/bin/schematool -initSchema -dbType mysql -uhive -pHive-123 -verbose'
				
				-、此处判断 是否需要skip，如果时not_if ,则跳过initSchema
				File "/var/lib/ambari-agent/cache/common-services/HIVE/0.12.0.2.0/package/scripts/hive.py", line 320, in hive （ user = params.hive_user）
			
			1)、请假一个月后，为什么question: hive 重启，每次都会执行 -initSchema -dbType ？	
				-:以前重启不会，因为skip此过程，因为 due to not_if 
				
				总结：因为hive的mysql元数据verion貌似被修改，导致错误。实际上是2.1.0 ，以前确实1.2.0【ambari会自动对比version，如果元数据信息未修改，则skip initSchema】
			
Could not retrieve transation read-only status server

A Gauge with name [init_total_count_tables] already exists.  The old gauge will be overwritten, but this is not recommended

31、vim操作手册
	-、20gg / 20G  调至20行
	-、:/set number
		ctrl+f 下一页
		ctrl+b 前一页
	-、:/wang  搜索某个单词，n 下一个标记  N 前一个标记
    -、在vim中有3中方法可以跳转到指定行（首先按esc进入命令行模式）：
		1、ngg/nG （跳转到文件第n行，无需回车）
		2、:n （跳转到文件第n行，需要回车）
		3、vim +n filename （在打开文件后，跳转到文件的第n行）
	-、将dev 替换 为 prod: 
:%s/eas/liangcaihr/g 

:%s/10.0.24.114/10.0.24.113/g
:%s/10.0.24.113/10.0.24.114/g

:%s/ekgrp/purchase_group_code/g
:%s/eas/ods_mould/g 

:%s/'3','17'/'3','17','15','16'/g 

:%s/table=/table_name=/g 


:%s/buyer/regexp_extract(A.buyer,'([\\\u4E00-\\\u9FA5]+)',1)/g 
:%s/regexp_extract(A.buyer,'([\\4E00-\\9FA5]+)',1)/buyer/g 


:%s/B.sales_unit/if(B.sales_unit is null or B.sales_unit = '',C.stand_unit_number,B.sales_unit) as sales_unit/g 

	示例：:%s/factory/company_no/g 
	
:%s/sap.ODS_SAP_ZDMMR027/dwbase.dwb_sap_jiegoujian_by_cd/g 

:%s/`,5,2/`,6,2/g
:/`,5,2

:/dwmiddle.dwb_purchase_report_cd
:%s/dwmiddle.dwb_purchase_report_cd/dwmiddle.dwb_purchase_report_cd_tmp/g


:%s/buyer_id/buyer/g

nohup sh test2.sh 2020-02-04 >> test2.log 2>&1 &

-- hive时间进行比较。
select * from t_proxy_user_log_partition where datediff(to_date(device_gmt_time),'2013-12-18')>=0 and datediff(to_date(device_gmt_time),'2013-12-25')<=0


-- hive 使用字符串between时间（末尾要直到当天结尾。）
    A.stock_in_audit_time between '2019/12/16' and '2020/01/15 23:59:59'
	
32、 hive不支持group_concat，但是可用以下方式转换 (2018-10-12 14:13:03)
转载

	分类： HIVE
mysql实现:
select A,GROUP_CONCAT(B)
from table
group by A



hive实现转换：
SELECT A,concat_ws(',', collect_set(B)) 
FROM table 
GROUP BY B
	

 collect_set(mgr)[0] 也可以使用。

 -、hive空值判断
	hive中空值判断基本分两种
	（1）NULL 与 \N
	hive在底层数据中如何保存和标识NULL，是由 alter table name SET SERDEPROPERTIES('serialization.null.format' = '\N')
 参数控制的
	比如：

		1.设置 alter table name SET SERDEPROPERTIES('serialization.null.format' = '\N')
 
		则：底层数据保存的是'\N',通过查询显示的是'NULL'
		这时如果查询为空值的字段可通过 语句：a is null 或者 a='\\N'

			  2.设置 alter tablename SET SERDEPROPERTIES('serialization.null.format' = 'NULL')
 

		则：底层数据保存的是'NULL',通过查询显示的是'NULL'
		这时如果查询为空值的字段可通过 语句：a is null 或者 a='NULL'

	（2）'' 与 length（xx）=0
	'' 表示的是字段不为null且为空字符串，此时用 a is null 是无法查询这种值的，必须通过 a=''  或者 length(a)=0 查询 


33、
	-、hive 发生额，原币和本位币是否写错？待验证  √
	-、以上步骤是单个币种数据的汇总，最后所有的币种这样处理之后都要汇总到AGE01表里。√
		-、即将我理解的是他把BESG这个表做了4个处理，分别计算期初，期末和本期借方和贷方数
最后把4段处理的结果写入AGE01
	-、 逻辑中去掉资产的需求。 √
	-、 客户描述： 关联表ods_sap_zfsd_013 的kunnr字段，取name1字段
		供应商描述：关联表ods_sap_zfmm_015 的lifnr字段，取name1字段
		公司描述: 关联表ods_sap_zfmm_015 的bukrs字段，取txt40字段
		币别转换表：关联表dwbservice.dws_currency的currency_id字段。取currency_name

			-、在最初小表中进行 join，取name？
	-、最后去掉 12个列都为null和""的行。  √
	-、数值精确到那个位数? 保留两位？√


科目余额计算逻辑整理：
	财务提供：
		1-、计算2019年度，计算相关指标的期初余额（原币/本位币）、本期发生额（原币/本位币）、期末余额值（原币/本位币）。	
		2-、关联相关表，合并相关指标的简介/详情。
		3-、数值精确到两位（四舍五入）。
		4-、计算逻辑去掉资产这一个标准。
			
	大数据平台待优化：依据实际数据，结合特成ui，适当增加相应维度，提供更多的数据支撑，展示丰富的ui。
		1-、新增月份维度。（年份维度，细化至月份/季度，计算相应指标值）【柱状图】
		2-、新增年份或者月份，相关指标比例值。【饼状图】
		3-、新增相关指标值，按照月份或者年份的数值走向图。【折线图】
		4-、新增根据科目的某一指标值作排行。 【表格】
		5-、新增某一年的所有科目期初余额总和，本期发生额总和,期末余额总和，客户数量总和等。【右上角-合计表格】

1-、新收购的公司单独分给一个团队
2-、专门的运维团队
3-、确定的需求
4-、专门的研发

34、 2019-10-29
多个hive表join ，需不需要分开join，还是直接  a join b on xxx  join  c？
     mycluster/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd/day=2020-01-01/.hive-staging_hive_2020-01-06_16-36-48_626_2603372294236111876-1/-ext-10000/000000_0 to dest path:hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd/day=2020-01-01/000000_0 returned false
35、  工具提示：MoveTask error没有其他的错误提示时，使用MR跑任务，提示报错如下
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. java.io.IOException: rename for src pat？
原因：删除外部表时，数据还在，所以重新插入时，move报错，删除dest目录下的数据，即可

drop table eas.ods_eas_new_eas75_t_bd_material

alter table  eas.ods_eas_new_eas75_t_bd_material drop partition(day='2019-11-14') 

hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_MATERIAL/day=2019-11-14/*

hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_CT_LS_MATCH/day=2019-11-14/*
hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_SUPPLIER/day=2019-11-14/*
hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_IM_PURINWAREHSBILL/day=2019-11-14/*
hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_IM_PURINWAREHSENTRY/day=2019-11-14/*
hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_ORG_STORAGE/day=2019-11-14/*
hadoop fs -rmr hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_SM_PURORDER/day=2019-11-14/*

-、如何删除外部表数据？
	1、alter table xx partition(day='2019-10-29')
	2、查询hive分区的存储目录
	LOCATION
	  'hdfs://mycluster/warehouse/sap/ods/ODS_SAP_ZFFI_006'
	3、删除hdfs数据
	hadoop fs -rmr hdfs://mycluster/warehouse/sap/ods/ODS_SAP_ZFFI_006/day=2019-10-29/*
	
	
-、shell遍历数组
#!/bin/bash
array=(A B C)
for(( i=0
i<${#array[@]}
i++)) do
#${#array[@]}获取数组长度用于循环
echo ${array[i]}

done


-、读取文件
cat filename | while read line
do
    echo $line

done


-- hdfs  查看文件中的文件，按照文件大小倒序排序
倒序:hadoop fs -ls -h -S    
正序:hadoop fs -ls -h -S -r 	

-- hive shell中  【hive -e 后面命令使用单引号而不是双引号，里面的路径使用双引号】
 hive -e '  split(material_code,"\\|") '    可以识别   (参数指定方式： "'${l_date}'")
 hive -e "  split(material_code,'\\|') "    不能识别   需要'\\\\|' 才能识别
hive cli中 两种都可以识别。
总结：  hive -e 外面双引号， 里面的单引号，如果用到转义符号，不起作用。所以外面要用单引号，里面双引号（使得转义符号生效）
参考资料：https://www.jianshu.com/p/981c50858040
-- 、 hive -e  中  | 需要4个转义符号\\\\才生效，  hiveCLi两个即可

-- shell中单引号和双引号
单引号：

可以说是所见即所得：即将单引号内的内容原样输出，或者描述为单引号里面看到的是什么就会输出什么。

双引号：

把双引号内的内容输出出来；如果内容中有命令、变量等，会先把变量、命令解析出结果，然后在输出最终内容来。

不加引号：

不会将含有空格的字符串视为一个整体输出, 如果内容中有命令、变量等，会先把变量、命令解析出结果，然后在输出最终内容来，如果字符串中带有空格等特殊字符，则不能完整的输出，需要改加双引号，一般连续的字符串，数字，路径等可以用。

https://blog.51cto.com/sunyu/799589



-- hive 去除空格

hive hql去除数据中的空格内容
原创xyhshen 最后发布于2019-01-19 10:31:53 阅读数 6474  收藏
展开
如果直接使用trim，那只能将标准的英文空格删掉

如果使用regexp_replace替换掉\\s，那就可以吧其他空白符号一起去掉

当然了如果还有中文的，可以这么做
regexp_replace(NVL(column,''),'[\\s]+|[\\u3000]+|[\,]','') as column
――――――――――――――――
版权声明：本文为CSDN博主「xyhshen」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/xyhshen/java/article/details/86549563

-- ls  传参数赋权限
ls | grep sh | xargs chmod 755


-- 、 shell 比对文本

comm命令

使用场景：
需要求两个文件的共同的记录列表或者只出现在第一个文件但不出现在第二个文件的记录列表
使用方法：
一定要是先排序，即sort
示例： comm -3 file1 file2 
参数：
  -1   显示第一个文件中有第二个文件中没有的列
  -2   显示第二个文件中有第一个文件中没有的列
  -3   显示共有的列

例子
comm - 12     就只显示在两个文件中都存在的行；
comm - 23    只显示在第一个文件中出现而未在第二个文件中出现的行；
comm - 13    只显示在第二个文件中出现而未在第一个文件中出现的行；
comm - 123  则什么也不显示。

-- 、 去除每一行的空格
sed -r 's/^[ \t]+(.*)[ \t]+$//g' supplier.txt > supplier-remove.txt

-- grep  精准匹配

# 若第二个文件未匹配到数据，则由此处理。  
 grep "\<$column_1_compar\>" 即精准匹配
	flag=`cat $out_put_file | grep "\<$column_1_compar\>"`
	
36、解决多维度问题？
		方案1：建立宽表，大数据提供宽表，
				-、宽表要定期维护？
				-、宽表提供给java后端，流程变复杂
		方案2：计算所有的维度数据，入库
				-、sql需要脚本拼接
				-、维度越多，排列组合的可能性更多，复杂度增高。
				-、数据更新的话，对应的算法结果也要更新【数据每天更新的话，每天都要更新计算维度表？？？？】
		方案3：前端需求直接抛给大数据，大数据这边拼接sql，然后查询返回。
		
	shell地址：/root/shell/creat_sql/	
	
	若实时统计则需要新增字段：updateTime
	

37、hive解决中文乱码。show create table 乱码
round one：
	1-、下载其他版本的hive-exec-2.1.0.2.5.3.0-37.jar替换现有的
	 cp /opt/hive-exec-1.2.1000.2.4.3.0-227.jar /usr/hdp/2.5.3.0-37/hive/lib/

	 
	 mv /usr/hdp/2.5.3.0-37/hive/lib/hive-exec-1.2.1000.2.4.3.0-227.jar hive-exec-1.2.1000.2.5.3.0-37.jar

	 
	 mv /usr/hdp/2.5.3.0-37/hive/lib/hive-exec-1.2.1000.2.5.3.0-37.jar  hive-exec-1.2.1000.2.5.3.0-37.jar-backup

round two：	 
	2-、修改源码  (修改源码，打包，去除execjar )
	【D:\ly\dingdownload\hive-release-HDP-2.5.3.0-tag  D:\ly\dingdownload\manual-hive-exe-jar\hive-exec-1.2.1000.2.5.3.0-37.jar】)
	 
		apache源码
		http://hadoop.apache.org/releases.html

		CDH源码
		http://archive-primary.cloudera.com/cdh5/cdh/5/

		HDP源码

		http://s3.amazonaws.com/public-repo-1.hortonworks.com/index.html#/HDP/centos6/2.x/updates/2.1.7.0/tars


		git源码地址：
		https://github.com/hortonworks/hive-release/releases/tag/HDP-2.5.3.0-tag

		git bash 解决中文乱码：窗口-option-text-GBK
		maven 打包流程:
			doc打包命令：
				D:\ly\firefox\apache-maven-3.6.2-bin\apache-maven-3.6.2\bin\mvn clean package -Phadoop-2 -DskipTests
			git打包命令：
				/d/ly/firefox/apache-maven-3.6.2-bin/apache-maven-3.6.2/bin/mvn clean package -Phadoop-2 -DskipTests
				
		-、打包执行info
			[INFO] ------------------------------------------------------------------------
			[INFO] Reactor Build Order:
			[INFO]
			[INFO] Hive                                                               [pom]
			[INFO] Hive Shims Common                                                  [jar]
			[INFO] Hive Shims 0.20S                                                   [jar]
			[INFO] Hive Shims 0.23                                                    [jar]
			[INFO] Hive Shims Scheduler                                               [jar]
			[INFO] Hive Shims                                                         [jar]
			[INFO] Hive Common                                                        [jar]
			[INFO] Hive Serde                                                         [jar]
			[INFO] Hive Metastore                                                     [jar]
			[INFO] Hive Ant Utilities                                                 [jar]
			[INFO] Spark Remote Client                                                [jar]
			[INFO] Hive Query Language                                                [jar]
			[INFO] Hive Service                                                       [jar]
			[INFO] Hive Accumulo Handler                                              [jar]
			[INFO] Hive JDBC                                                          [jar]
			[INFO] Hive Beeline                                                       [jar]
			[INFO] Hive CLI                                                           [jar]
			[INFO] Hive Contrib                                                       [jar]
			[INFO] Hive HBase Handler                                                 [jar]
			[INFO] Hive HCatalog                                                      [pom]
			[INFO] Hive HCatalog Core                                                 [jar]
			[INFO] Hive HCatalog Pig Adapter                                          [jar]
			[INFO] Hive HCatalog Server Extensions                                    [jar]
			[INFO] Hive HCatalog Webhcat Java Client                                  [jar]
			[INFO] Hive HCatalog Webhcat                                              [jar]
			[INFO] Hive HCatalog Streaming                                            [jar]
			[INFO] Hive HWI                                                           [jar]
			[INFO] Hive ODBC                                                          [pom]
			[INFO] Hive Shims Aggregator                                              [pom]
			[INFO] Hive TestUtils                                                     [jar]
			[INFO] Hive Packaging                                                     [pom]
			[INFO]
			[INFO] ------------------------< org.apache.hive:hive >------------------------
			[INFO] Building Hive 1.2.1                                               [1/31]
			[INFO] --------------------------------[ pom ]---------------------------------
		-、部分jar下载缓慢，手动下载替换（本地仓库默认地址：C:\Users\wang.ying.nan\.m2\repository\）
			https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0/
			Progress (4): 0.4/1.3 MB | 0.9/3.0 MB | 0.2/7.8 MB | 0.5/3.4 MB
		-、提示 类找不到（包找不到），org.apache.hadoop.ipc.CallerContext？
			-、解决:此类在jar包C:\Users\wang.ying.nan\.m2\repository\org\apache\hadoop\hadoop-common\2.6.0\hadoop-common-2.6.0.jar 中，因为2.6的版本没有此类，故替换为2.7版本，完美解决。
		-、 Failed to execute goal on project hive-exec: Could not resolve dependencies for project org.apache.hive:hive-exec:jar:1.2.1: Could not find artifact org.apache.calcite.avatica:avatica:jar:1.7.2-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots)
			方案一、下载 avatica-1.7.1.jar 替换  ×
			方案二、修改pom.xml ,将avatica依赖jar修改为 不带SNAPSHOT版本的，完美解决。

		-、/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java: 找不到org.apache.tez.client.CallerContext
		/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ColumnarSplitSizeEstimator.java: 找不到org.apache.hadoop.mapred.split.SplitSizeEstimator
		/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[2006,72] : 不兼容的类型: byte[]无法转换为java.lang.String ===>  writeBytes()改为write()


		-、一般提示“方法不会覆盖或实现超类型的方法”，有可能是版本过高或者过低导致的。

	3、show create 乱码 ，直接修改hive元数据表 
		alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8

		alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

		alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8 

		alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8

		alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8

	
	综上所述 ：  改 hive的mysql连接编码+ 元数据编码 + hive-exec替换 
		备注：hive-exec替换 要在所有的主机上都执行，没有替换的主机，show 还是乱码。
38、presto 解决  multiDelimitSerDe问题。
svn客户端：


39、安装nomachine
	-、官网下载：https://www.nomachine.com/download/download&id=6 
		下载rpm文件
	-、sudo rpm -i nomachine_6.8.1_1_x86_64.rpm
	-、自动启动，默认端口4000，5353
	-、日志目录：/usr/NX/var/log


查看端口由那个程序占用
	lsof -i:5905
查看防火墙：
	systemctl status firewalld.service
vnc查看系统错误日志：
	tail -n 100 /var/log/secure
kettle提示错误：
	/root/.swt/lib/linux/x86_64/libswt-mozilla-gtk-4335.so: libxpcom.so: 无法打开共享对象文件: 没有那个文件或目录

40、整理eas相关字段：
	
你好，为了更好达到计算自动化/半自动话，需要了解手动录入数据的来源，基于此，手动录入的部分项，有如下疑问：
	1-、基价(未税)：冲压取上一年度最后一季度加权平均价 
		1、计算的每一项的权值是多少？
			：没有权值，冲压取上一年度最后的一个基准价格，不是平均价。
		2、计算的每一项的价格取自于那里？
			：入库数据自带的单价
		3、怎么区分数据是否来自于冲压?
			：查看采购员，以及入库仓位（余经理可以提供架构组织BU，来区分冲压-cnc-模切）
	2-、基价金额：上一年度基准价*审核入库数量	
		1、基准价是否就是“基价(未税)”？
			：是的
		2、审核入库数量取自于那里?
			：是否就是T_IM_PurInWarehsEntry.FWrittenOffQty	已核销数量
	3-、CD金额： 计算公式 （ 上年度未税价格-当年未税价格）* 入仓数量)
		1、未税价格是否就是“未税单价”？
			:是
		2、入仓数量取自于那个值？
			：本次收货数量
	4-、账期天数：供应商的付款账期天数
		1、供应商的付款账期天数是如何得来的？
			：取自付款条件里面的天数
	5-、类别，项目以及跟单的责任人是否必须手动输入？
			：基本上需要手动输入，跟单可以系统导出（采购员）
	
****************************************************************************************************************************************
****************************************************************************************************************************************
****************************************************************************************************************************************
****************************************************************************************************************************************
****************************************************************************************************************************************
****************************************************************************************************************************************

41、处理ambari-agent问题： （due to EOF occurred in violation of protocol (_ssl.c:661)" ）？
error：due to EOF occurred in violation of protocol (_ssl.c:661)
Two-way SSL authentication failed. Ensure that server and agent certificates were signed by the same CA and restart the agent.
In order to receive a new agent certificate, remove existing certificate file from keys directory.
	As a workaround you can turn off two-way SSL authentication in server configuration(ambari.properties)

------------------------------------------------------------	
lyzk02 ： 
备份所在目录：
/opt/keys-backup-ambari-server
/opt/keys-backup-ambari-server-2

/var/lib/ambari-server/keys 备份目录所有者结构
-rwxrwxrwx. 1 ambari root    803 9月   4 13:44 ca.config
-rw-r--r--  1 ambari ambari 7134 9月   4 14:14 ca.crt
-rw-r--r--  1 ambari ambari 1647 9月   4 14:14 ca.csr
-rw-r--r--  1 ambari ambari 3311 9月   4 14:14 ca.key
drwx------. 3 ambari root   4096 11月  8 15:04 db
-rw-r--r--  1 ambari ambari 5677 9月   4 14:14 keystore.p12
-rw-r--r--  1 ambari ambari 4158 11月  8 15:04 lyzk01.crt
-rw-r--r--  1 ambari ambari  534 11月  8 15:04 lyzk01.csr
-rw-------  1 ambari root     50 9月   4 13:57 pass.txt

/var/lib/ambari-server/keys/db 备份目录所有者结构
-rw-r--r--  1 ambari ambari 162 11月  8 15:04 index.txt
-rw-r--r--  1 ambari ambari  21 11月  8 15:04 index.txt.attr
-rw-r--r--  1 ambari ambari  21 11月  8 15:04 index.txt.attr.old
-rw-r--r--  1 ambari ambari 123 11月  8 15:04 index.txt.old
drwx------. 2 ambari root    45 11月  8 15:04 newcerts
-rw-r--r--  1 ambari ambari   3 11月  8 15:04 serial
-rw-r--r--  1 ambari ambari   3 9月   4 14:28 serial.old

/var/lib/ambari-server/keys/db/newcerts 备份目录所有者结构
-rw-r--r-- 1 ambari ambari 7134 9月   4 14:14 01.pem
-rw-r--r-- 1 ambari ambari 4158 9月   4 14:28 02.pem
-rw-r--r-- 1 ambari ambari 4158 11月  8 15:04 03.pem
------------------------------------------------------------

完整错误日志：
INFO 2019-11-08 16:57:44,546 security.py:100 - SSL Connect being called.. connecting to the server
INFO 2019-11-08 16:57:44,550 security.py:67 - Insecure connection to https://lyzk02:8441/ failed. Reconnecting using two-way SSL authentication..
INFO 2019-11-08 16:57:44,550 security.py:189 - Server certicate exists, ok
INFO 2019-11-08 16:57:44,550 security.py:197 - Agent key exists, ok
INFO 2019-11-08 16:57:44,550 security.py:205 - Agent certificate exists, ok
INFO 2019-11-08 16:57:44,550 security.py:100 - SSL Connect being called.. connecting to the server
ERROR 2019-11-08 16:57:44,553 security.py:87 - Two-way SSL authentication failed. Ensure that server and agent certificates were signed by the same CA and restart the agent.
In order to receive a new agent certificate, remove existing certificate file from keys directory. As a workaround you can turn off two-way SSL authentication in server configuration(ambari.properties)
Exiting..
ERROR 2019-11-08 16:57:44,553 Controller.py:212 - Unable to connect to: https://lyzk02:8441/agent/v1/register/lyzk01
Traceback (most recent call last):
  File "/usr/lib/python2.6/site-packages/ambari_agent/Controller.py", line 165, in registerWithServer
    ret = self.sendRequest(self.registerUrl, data)
  File "/usr/lib/python2.6/site-packages/ambari_agent/Controller.py", line 497, in sendRequest
    raise IOError('Request to {0} failed due to {1}'.format(url, str(exception)))
IOError: Request to https://lyzk02:8441/agent/v1/register/lyzk01 failed due to EOF occurred in violation of protocol (_ssl.c:618)
ERROR 2019-11-08 16:57:44,554 Controller.py:213 - Error:Request to https://lyzk02:8441/agent/v1/register/lyzk01 failed due to EOF occurred in violation of protocol (_ssl.c:618)

1)-、vim /etc/ambari-agent/conf/ambari-agent.ini
新增一项：
[security]
force_https_protocol=PROTOCOL_TLSv1_2
2)-、 vim /etc/python/cert-verification.cfg  
【同时修改 ambari-server的为disable，但是server未重启，不知道效果如何已改为disable。 没效果】
disble

1-、修改vim /usr/lib/python2.6/site-packages/ambari_agent/Controller.py
496行



-、重启ambari-server ，lymaster01 agent日志（正常日志）
INFO 2019-11-08 17:43:48,948 Controller.py:277 - Heartbeat with server is running...
INFO 2019-11-08 17:43:48,950 NetUtil.py:62 - Connecting to https://10.0.24.110:8440/connection_info
INFO 2019-11-08 17:43:48,993 security.py:100 - SSL Connect being called.. connecting to the server
INFO 2019-11-08 17:43:49,020 security.py:61 - SSL connection established. Two-way SSL authentication is turned off on the server.
INFO 2019-11-08 17:43:49,024 Controller.py:326 - RegistrationCommand received - repeat agent registration

-、/var/lib/ambari-server/keys/ca.config
[ ca ]
default_ca             = CA_CLIENT
[ CA_CLIENT ]
root_dir = /
dir                    = $root_dir/var/lib/ambari-server/keys/db
certs                  = $dir/certs
new_certs_dir          = $dir/newcerts

database               = $dir/index.txt
serial                 = $dir/serial
default_days           = 365

default_crl_days       = 7
default_md             = sha256

policy                 = policy_anything

[ policy_anything ]
countryName            = optional
stateOrProvinceName    = optional
localityName           = optional
organizationName       = optional
organizationalUnitName = optional
commonName             = optional
emailAddress           = optional

[ jdk7_ca ]
subjectKeyIdentifier = hash
authorityKeyIdentifier = keyid:always,issuer:always
basicConstraints = CA:true

	1-、修改ca.config的jdk=8
	2-、删除/var/lib/ambari-server/keys/ 和/var/lib/ambari-agent/keys/
	3-、重启ambari-agent
	
		1、ambari-server 服务，生成ca.crt报错(解密错误)	
			openssl ca -config /var/lib/ambari-server/keys/ca.config -in /var/lib/ambari-server/keys/lyzk01.csr -out /var/lib/ambari-server/keys/lyzk01.crt -batch -passin pass:**** -keyfile /var/lib/ambari-server/keys/ca.key -cert /var/lib/ambari-server/keys/ca.crt
			
			
			Using configuration from /var/lib/ambari-server/keys/ca.config
			unable to load CA private key
			139952471873424:error:06065064:digital envelope routines:EVP_DecryptFinal_ex:ba decrypt:evp_enc.c:592:
			139952471873424:error:0906A065:PEM routines:PEM_do_header:bad decrypt:pem_lib.c488:
			
			-、修改密码后，正确
			openssl ca -config /var/lib/ambari-server/keys/ca.config -in /var/lib/ambari-server/keys/lyzk01.csr -out /var/lib/ambari-server/keys/lyzk01.crt -batch -passin pass:oO9G1YewY4xTE0zie1kS650ruz6l930i5RyE7x03bhF91vgn0G -keyfile /var/lib/ambari-server/keys/ca.key -cert /var/lib/ambari-server/keys/ca.crt
			
			Using configuration from /var/lib/ambari-server/keys/ca.config
			I am unable to access the //var/lib/ambari-server/keys/db/newcerts directory
			//var/lib/ambari-server/keys/db/newcerts: No such file or directory


		2020-04-01
			-、单独生成lymaster02的ca.crt
			 openssl ca -config /var/lib/ambari-server/keys/ca.config -in /var/lib/ambari-server/keys/lymaster01.csr -out /var/lib/ambari-server/keys/lymaster01.crt -batch -passin pass:oO9G1YewY4xTE0zie1kS650ruz6l930i5RyE7x03bhF91vgn0G -keyfile /var/lib/ambari-server/keys/ca.key -cert /var/lib/ambari-server/keys/ca.crt
			
		2、提示key不匹配(ambari-agent)
				IOError: Request to https://lyzk02:8441/agent/v1/register/lyzk01 failed due to [X509: KEY_VALUES_MISMATCH] key values 
				mismatch (_ssl.c:2593)
				ERROR 2019-11-13 09:03:11,708 Controller.py:213 - Error:Request to https://lyzk02:8441/agent/v1/register/lyzk01 failed due to [X509: KEY_VALUES_MISMATCH] key values mismatch (_ssl.c:2593)
				
			-、每次/var/lib/ambari-agent/keys/ 目录下生成的keys文件都不一样，为什么？用了上一次的那个，key就匹配了？
				-、删除 /var/lib/ambari-agent/keys/目录下所有文件，重启ambari-agent，key值不匹配解决 --> 继续报错
				 ：failed due to EOF occurred in violation of protocol (_ssl.c:618)

2019-12-13  待执行。。。  ，先解决key不匹配的问题。

		2020-03-23
		1、复制 keys文件夹，注意chown 
		2、复制以前的 crt，crs，key  ，还是会报 no cipher suites in common  【ambari-server】 ,  
    raise IOError('Request to {0} failed due to {1}'.format(url, str(exception)))IOError: Request to https://10.0.24.110:8441/agent/v1/register/lymaster01 failed due to ()   【ambari-agent】
		3、尝试不用ssl来通信.
		
1-、ambari-server  添加	
	vim /etc/ambari-agent/conf/ambari-agent.ini
	新增一项：
	[security]
	force_https_protocol=PROTOCOL_TLSv1_2
		
****************************************************************************************************************************************
****************************************************************************************************************************************
****************************************************************************************************************************************
****************************************************************************************************************************************


42、安装svn，创建kettle文件夹
rpm -qa subversion
yum install -y subversion
svnserve --version
svn checkout https://10.0.8.18/svn/lyproject/datacenter/data_warehouse/trunk

43、oracle使用desc：
	1、sqlplus.exe + oci.dll ,无效果?

44、
SQL 错误 [2] [08S01]: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1572493614064_0340_1_11, diagnostics=[Task failed, taskId=task_1572493614064_0340_1_11_000007, diagnostics=[TaskAttempt 0 failed, info=[Error: exceptionThrown=java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:56)
	at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:46)
	at org.apache.tez.runtime.library.common.shuffle.MemoryFetchedInput.<init>(MemoryFetchedInput.java:38)
	at org.apache.tez.runtime.library.common.shuffle.impl.SimpleFetchedInputAllocator.allocate

45、"SELECT name,score "
        + "FROM ("
            + "SELECT "
               + "name,"
               + "score, "
               + "row_number() OVER (PARTITION BY name ORDER BY score DESC ) rank"
               + " FROM scores"
        + ")  sub_scores "
        + " WHERE rank = 1
		
46、hive不支持select子查询，替换
select A.*,B.cd_total_amount from dwservice.dws_cd_by_purchaser_month  A
left join (select purchaser,year,sum(cd_amount) as cd_total_amount
from dwservice.dws_cd_by_purchaser_month group by purchaser,year) B 
on A.purchaser = B.purchaser and A.year = B.year


47、
	1、hadoop元数据目录 （NameNode directories）
	2、数据节点目录一定要挂载到大的磁盘点。（DataNode directories
		文件系统                 容量  已用  可用 已用% 挂载点
		/dev/mapper/centos-root   50G   46G  4.5G   92% /
		devtmpfs                  16G     0   16G    0% /dev
		tmpfs                     16G  156K   16G    1% /dev/shm
		tmpfs                     16G  104M   16G    1% /run
		tmpfs                     16G     0   16G    0% /sys/fs/cgroup
		/dev/mapper/centos-home  442G  106G  336G   25% /home
		/dev/sda1                497M  157M  340M   32% /boot
		tmpfs                    3.2G  8.0K  3.2G    1% /run/user/0
		/dev/sr0                 4.1G  4.1G     0  100% /run/media/root/CentOS 7 x86_64
		tmpfs                    3.2G  4.0K  3.2G    1% /run/user/1001
		tmpfs                    3.2G   32K  3.2G    1% /run/user/1020


		drwxr-xr-x    5 root root    44 9月   4 17:19 hadoop   		 16G
		drwxr-xr-x.  14 root root  4096 11月 12 17:01 opt			 10G
		dr-xr-x---.  25 root root  4096 11月 13 10:13 root     		 0.6G
		drwxr-xr-x.  15 root root  4096 11月  7 14:00 usr		 	 7.1G
		drwxr-xr-x.  22 root root  4096 11月  7 14:00 var		     12G
	3、NodeManager 还提供了检测磁盘好坏的机制。
	检测的磁盘目录主要是 yarn.nodemanager.local-dirs 和 yarn.nodemanager.log-dirs 参数指定的目录，这两个目录分别用于存储应用程序运行的中间结果，比如MapReduce作业中Map Task的中间输出结果）和日志文件存放目录列表。这两个参数都可以配置多个目录，多个目录之间使用逗号分隔。
	如果这两个参数配置的目录不可用的比例达到一定的设置，则认为该节点不健康。某个目录可不可用的定义是：运行 NodeManager 节点的进程是否对这个目录可读、可写、可执行。如果这些条件都满足，这个目录则健康，否则该目录就被放入 failedDirs 列表里面。本地目录健康检测主要涉及到以下几个参数：
    yarn.nodemanager.disk-health-checker.interval-ms：本地目录健康检测线程执行的频率，默认值为2分钟；
    yarn.nodemanager.disk-health-checker.enable：是否启用本地目录健康检测，默认值是启用；
	yarn.nodemanager.disk-health-checker.min-healthy-disks：正常目录数目相对于总目录总数的比例，低于这个值则认为此节点处于不正常状态，默认值为0.25。

以上两种检测机制都会随着 NodeManager 节点启动而运行，并且检测到的状态会随心跳信息发送到 ResourceManager 端，然后 ResourceManager 端会根据相关的信息得到当前节点的可用情况，一旦发现这个节点不健康，则会标记此节点的状态为 NodeState.UNHEALTHY ，此后将不会忘这个节点分配任务，直到该节点状态正常
 -、异常退出hive执行程序，产生临时文件，
	1、删除hadoop fs -rm -r -skipTrash /tmp/hive/root/*
	2、hdfs dfs -expunge  
 https://cloud.tencent.com/developer/article/1363423
	26G     ./subdir198
	32G     ./subdir199
	22G     ./subdir200
	25G     ./subdir201
	32G     ./subdir202
	22G     ./subdir203
	129M    ./subdir204
	136M    ./subdir205

	4、hive参数的单位
		1)、mapred.max.split.size 单位【byte】（100 000 000 = 100M）
			常见使用案例：
				1、SQL里含有耗时udf
				2、数据量较大，但是mapper数量较少（<100）
				3、ORC压缩比太高，所以128M对应的数据行数过多
			总结：
				1、block是物理上的数据分割，而split是逻辑上的分割。
				2、如果没有特别指定，split size 就等于 HDFS 的 block size 。
				3、用户可以在M/R 程序中自定义split size。
				4、一个split 可以包含多个blocks，也可以把一个block应用多个split操作。
				5、一个split不会包含两个File的Block,不会跨越File边界
				6、有多少个split，就有多少个mapper。
原文链接：https://blog.csdn.net/tch918/article/details/89061860

		2)、hive shell 中 date 只能在''（单引号中识别 hive -e ''）
			''    date
			"" '${l_date}'  \date\
48、
虚拟机和物理机选择。
		1、hadoop集群（hive）对磁盘io开销比较大，10台物理机比1台物理机（10台虚拟机）磁盘io快很多。
		2、1台物理机（10台虚拟机），存在物理机挂掉，整个集群崩溃的情况。而10台物理机的集群，理论上主机挂掉一半以上照样可以提供服务。
		3、虚拟化主机无法划分正确的机架（rack）来让hadoop合理的分布数据块存放位置。
		4、虚拟机是共享物理机的cpu，虚拟机之间cpu峰值存在相互影响。
	综上所述，相对于虚拟机的优点，缺点是灾难性的，故选择物理机。






-- 显示模组：
	1、美金为单位，财务转换为人民币，  汇率不一样， 美金单价。
	2、一个物料 对应多个供应商。
	3、物料类型对应物料名？     √
	4、付款章程类型， 是 转账还是其他现金？.  √
	5、计算方式，  当月的计算，还是  月底  结算。    √
	6、汇总的 放各个BG的排行。  √
设备 ---> 杂项 固定资产。
	7、采购员可能会轮岗    
	8、公司下面的不同BG    <->
	9、项目号下面的CD金     <->
	10、数据出现严重异常，提示。    <->
	11、采购员去掉  -->  供应商采购员。   √
	3

数据优化待解决：
	1、一个物料对应多个供应商。【需要关联供应商】
	5、计算方式，改为取上年最大的未税单价（price）。
	2、美金为单位，财务转换为人民币，汇率不一样，美金单价。
	3、月结类型统计的计算方式:       --> 汤加丽沟通        √
	比如月结60天，采购额是算当月，还是60天后的那个月？
			-、采购额算在当期。
	4、采购员可能会轮岗  √  剔除
	6、EAS，yifei，sap数据如何核定准确性？
	
	7、Y轴单位 是否要和图表详情的单位一致（同为“百万”）？  √
	8、bug是否要在群里共享？  √
			-、采购组人员统一反馈。

	
	9-、月结需要关联 fname
	10-、T_BD_SupplierCompanyInfo  供应商公司表，关联 t_Bd_settlementType 查询结算方式。


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[root@lymaster01 ~]# hadoop fs -du -h  /warehouse/eas/ods
53.3 M   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_CT_LS_MATCH
24.8 K   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_ODS_EAS_NEW_EAS75_T_BD_PAYCONDITION
3.8 M    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_ODS_EAS_NEW_EAS75_T_BD_Person
30.1 M   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_ODS_EAS_NEW_EAS75_T_SM_PURREQUEST
16.9 M   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_ACCOUNTVIEW
67.5 K   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_CASHFLOWITEM
3.3 K    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_CURRENCY
569.9 K  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_CUSTOMER
5.2 K    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_EXCHANGEAUX
435.7 M  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_MATERIAL
26.6 K   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_MATERIALGROUP
81.5 M   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_MATERIALGROUPDETIAL
5.6 K    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_MEASUREUNIT
0        /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_PAYCONDITION
6.6 K    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_PAYCONDITIONENTRY
85.3 K   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_PERIOD
3.7 M    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_PERSON
559      /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_SETTLEMENTTYPE
3.1 M    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_SUPPLIER
15.4 M   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_SUPPLIERCOMPANYINFO
1.0 K    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_BD_VOUCHERTYPES
1.5 M    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_DB_LOCATION
857.7 K  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_DB_WAREHOUSE
706.6 M  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_GL_ACCOUNTBALANCE
345.7 M  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_GL_VOUCHER
767.9 M  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_GL_VOUCHERENTRY
1.2 G    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_IM_PURINWAREHSBILL
3.0 G    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_IM_PURINWAREHSENTRY
1.1 M    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_ORG_ADMIN
26.3 K   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_ORG_COMPANY
19.3 K   /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_ORG_STORAGE
535.0 M  /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_SM_PURORDER
1.2 G    /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_SM_PURORDERENTRY
0        /warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_SM_PURREQUEST
0        /warehouse/eas/ods/ods_eas_new_eas75_T_BD_MATERIALGROUP
0        /warehouse/eas/ods/ods_eas_new_eas75_T_BD_MATERIALGROUPDETIAL

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-------------------------------------------------------------------------------------------------------------
	
	1、汇总BG 
		-、原左上角图去掉，改为每个BG的CD金额排行.
		    detail: 
				1)、BG按照CD金额倒序(按时间段)
				2)、鼠标指向显示，CD金额，CD比例,总金额。(按时间段)
		【SELECT SUM(CD_AMOUNT) AS  CD_AMOUNT_ALL , SUM(CD_AMOUNT)/sum(TOTAL_AMOUNT) AS CD_PROPORTION_ALL,
sum(TOTAL_AMOUNT) AS  TOTAL_AMOUNT_ALL,dim,BG
FROM DW.DWS_CD_BY_AMOUNT_MONTH WHERE "DATE" BETWEEN '2019-01' AND '2019-12' AND  dim != 'unknown' 
GROUP BY DIM,BG ORDER BY SUM(CD_AMOUNT) DESC
】
			
	2、所有BG
		1-、去掉左下角的图，改为供应商的CD金额展示.
			detail:
				1)、供应商按照CD金额倒序.(按时间段)
				2)、鼠标指向显示，CD金额，CD比例,总金额。(按时间段)
		2-、物料类型的详情改为显示物料的详情.
			detail:
				1)、详情表，新增物料名称字段，查询返回
		3-、右下角图，新增付款类型.
			detail:
				1)、详情表，新增付款类型字段，查询返回
		4-、每个柱状图，条形图，详情金额改为 “百万”单位.
		5-、每个报表下载excel，弹出框可以选择时间段导出.
		
	3、异常数据报警 (用户提供超出多少范围报警).
	4、按区域划分，如：华东华西(区域划分规划用户提供)；
	
	
49、oozie安装 
derby ： oozie/Oozie-123

-、安装提示警告
Some service configurations are not configured properly. We recommend you review and change the highlighted configuration values. Are you sure you want to proceed without correcting configurations?
Type 	Service 	Property 	Value 	Description
1)-、Warning 	MapReduce2 	mapreduce.map.java.opts 	-Xmx1638m 	
Value is less than the recommended default of -Xmx4505m
Larger heap-size for child jvms of maps.
      ：mapreduce.map.java.opts 其实就是启动 JVM 虚拟机时，传递给虚拟机的启动参数，而默认值 -Xmx200m 表示这个 Java 程序可以使用的最大堆内存数，一旦超过这个大小，JVM 就会抛出 Out of Memory 异常，并终止进程。而 mapreduce.map.memory.mb 设置的是 Container 的内存上限，这个参数由 NodeManager 读取并进行控制，当 Container 的内存大小超过了这个参数值，NodeManager 会负责 kill 掉 Container。
	  也就是说，mapreduce.map.java.opts一定要小于mapreduce.map.memory.mb
	按照如下方式设置：
	set mapreduce.map.child.java.opts="-Xmx3072m"(注:-Xmx设置时一定要用引号，不加引号各种错误
	set mapreduce.map.memory.mb=3288
2)-、Warning 	MapReduce2 	mapreduce.map.memory.mb 	2048 	
Value is less than the recommended default of 5632
Value is less than the recommended minimum of 5632
Virtual memory for single Map task
	:参考资料-https://blog.csdn.net/u014665013/article/details/80923044

3)-、Warning 	MapReduce2 	mapreduce.reduce.java.opts 	-Xmx3276m 	
Value is less than the recommended default of -Xmx9011m
Larger heap-size for child jvms of reduces.
Warning 	MapReduce2 	mapreduce.reduce.memory.mb 	4096 	
Value is less than the recommended minimum of 5632
Value is less than the recommended default of 11264
Virtual memory for single Reduce task

4)-、Warning 	MapReduce2 	mapreduce.task.io.sort.mb 	1146 	
Value is less than the recommended default of 2047
The total amount of buffer memory to use while sorting files, in megabytes. By default, gives each merge stream 1MB, which should minimize seeks.
	解读：?Mapper中的Kvbuffer的大小默认100M，可以通过mapreduce.task.io.sort.mb（default：100）参数来调整。可以根据不同的硬件尤其是内存的大小来调整，调大的话，会减少磁盘spill的次数此时如果内存足够的话，一般都会显著提升性能。spill一般会在Buffer空间大小的80%开始进行spill（因为spill的时候还有可能别的线程在往里写数据，因为还预留空间，有可能有正在写到Buffer中的数据），可以通过mapreduce.map.sort.spill.percent（default：0.80）进行调整，Map Task在计算的时候会不断产生很多spill文件
――――――――――――――――
版权声明：本文为CSDN博主「aijiudu」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/aijiudu/article/details/72353510
	
5)-、Warning 	MapReduce2 	yarn.app.mapreduce.am.command-opts 	-Xmx1638m -Dhdp.version=${hdp.version} 	
Value is less than the recommended default of -Xmx4505m
Java opts for the MR App Master processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc Usage of -Djava.library.path can cause programs to no longer function if hadoop native libraries are used. These values should instead be set as part of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and mapreduce.reduce.env config settings.
	解读：AM的默认jvm堆内存。
	set yarn.app.mapreduce.am.command - opts = - Xmx10000m
  -- 


6)-、Warning 	MapReduce2 	yarn.app.mapreduce.am.resource.mb 	2048 	
Value is less than the recommended default of 5632
Value is less than the recommended minimum of 5632
The amount of memory the MR AppMaster needs.
	解读：AM的container内存。
	set yarn.app.mapreduce.am.resource.mb = 10000
  -- MR ApplicationMaster占用的内存量

7)-、Warning 	Tez 	tez.am.resource.memory.mb 	4096 	
Value is less than the recommended default of 5632
The amount of memory to be used by the AppMaster. Used only if the value is not specified explicitly by the DAG definition.

8)-、Warning 	Tez 	tez.runtime.io.sort.mb 	540 	
Value is less than the recommended default of 1486
The size of the sort buffer when output needs to be sorted

9)-、Warning 	Tez 	tez.runtime.unordered.output.buffer.size-mb 	153 	
Value is less than the recommended default of 422
The size of the buffer when output does not require to be sorted

10)-、Warning 	Tez 	tez.task.resource.memory.mb 	2048 	
Value is less than the recommended default of 5632
The amount of memory to be used by launched tasks. Used only if the value is not specified explicitly by the DAG definition.

11)-、Warning 	Hive 	hive.auto.convert.join.noconditionaltask.size 	572662306 	【byte】 550M
Value is less than the recommended default of 1574821341  [1.5G]
If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. However, if it is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than this size, the join is directly converted to a mapjoin(there is no conditional task).
	解析：Hive对于mapjoin是默认开启的
		set hive.auto.convert.join = true
 --是否自动转换为mapjoin
		set hive.mapjoin.smalltable.filesize = 25000000
 --小表的最大文件大小，默认为25000000，即25M
		set hive.auto.convert.join.noconditionaltask = true
 --是否将多个mapjoin合并为一个
		set hive.auto.convert.join.noconditionaltask.size = 10000000
 --多个mapjoin转换为1个时，所有小表的文件大小总和的最大值。
		
12)-、Warning 	Hive 	hive.tez.container.size 	2048 	
Value is less than the recommended default of 5632
By default, Tez uses the java options from map tasks. Use this property to override that value.


13)-、Warning 	Ambari Metrics 	hbase_master_heapsize 	3072 	
Consider allocating 4096 MB to hbase_master_heapsize in ams-hbase-env to use up some unused memory on host
HBase Master Heap Size. In embedded mode, total heap size is sum of master and regionserver heap sizes.

14)-、Warning 	Ambari Metrics 	hbase_master_xmn_size 	1024 	
Value is greater than the recommended maximum Xmn size of 768 (20% of hbase_master_heapsize + hbase_regionserver_heapsize)
HBase Master maximum value for young generation heap size.

15)-、Warning 	Ambari Metrics 	metrics_collector_heapsize 	512 	
Consider allocating 768 MB to metrics_collector_heapsize in ams-env to use up some unused memory on host
Metrics Collector Heap Size

50、磁盘挂载 ，设置为xfs  
	1-、fdisk -l //查看磁盘信息；
	2-、fdisk /dev/sdb
		新建分区输入n，p,1，一路默认；
	3-、格式化分区为XFS：
		mkfs.xfs -f /dev/sdb1
	4-、先创建一个目录，然后将 /dev/sdb1 挂载到该目录下
		mount -t xfs /dev/sdb1 /opt
	5-、修改 /etc/fstab 来进行自动加载
		加入下列行到/etc/fstab：修改 /etc/fstab 来进行自动加载，加入下列行到/etc/fstab：加入下列行到/etc/fstab：修改 /etc/fstab 来进行自动加载，加入下列行到/etc/fstab：
	echo '/dev/sdb1 /opt xfs defaults 0 0' >> /etc/fstab
https://blog.csdn.net/Maplematics/article/details/87893862

	磁盘挂载 -> 2TB 以上
	parted用于对磁盘（或RAID磁盘）进行分区及管理，与fdisk分区工具相比，支持2TB以上的磁盘分区，并且允许调整分区的大小。
	1-、对磁盘进行分区
		parted /dev/sdb mklabel gpt mkpart /dev/sdb1 xfs 1 6047.3G
	2-、格式化分区(-f  强制执行)
		mkfs.xfs -f /dev/sdb1  
	3-、挂载分区（-t  指定档案系统的型态，）
		mount -t  xfs /dev/sdb1 /opt
	4-、最后设置开机自动挂载：
		echo '/dev/sdb1 /opt xfs defaults 0 0' >> /etc/fstab

		日志：
			[root@prodata01 ~]# parted /dev/sdb mklabel gpt mkpart /dev/sdb1 xfs 1 6047.3G
				信息: You may need to update /etc/fstab.  （即需要更新开机启动挂载。）
				
				
51.1 、磁盘扩容

一、扩容根目录
	1、fdisk /dev/sda
	2、新建两个分区，且 设置类型 => 8e
		[root@lyzk01 ~]# fdisk -l
			Disk /dev/sda: 1073.7 GB, 1073741824000 bytes, 2097152000 sectors
			Units = sectors of 1 * 512 = 512 bytes
			Sector size (logical/physical): 512 bytes / 512 bytes
			I/O size (minimum/optimal): 512 bytes / 512 bytes
			Disk label type: dos
			Disk identifier: 0x000ca41b

			   Device Boot      Start         End      Blocks   Id  System
			/dev/sda1   *        2048     1026047      512000   83  Linux
			/dev/sda2         1026048  1048575999   523774976   8e  Linux LVM
			/dev/sda3      1048576000  1258291199   104857600   8e  Linux LVM
			/dev/sda4      1258291200  2097151999   419430400   8e  Linux LVM
	3、reboot 
	3、pvcreate /dev/sda3
		 Physical volume "/dev/sda3" successfully created
	4、vgextend centos /dev/sda3
	5、lvresize -L +100G /dev/mapper/centos-root
	6、xfs_growfs /dev/mapper/centos-root
	
https://www.cnblogs.com/lillcol/p/11452915.html
https://www.jb51.net/article/144291.htm  (8e  ,扩容)
https://blog.csdn.net/yangzhengquan19/article/details/83788277
	
二、新增挂载数据节点。

	3、pvcreate /dev/sda4
	4、vgextend centos /dev/sda4
	5、lvresize -L +400G /dev/mapper/centos-home
	6、xfs_growfs /dev/mapper/centos-home
	
	
51、设置为“^#” 分隔符 报错
SQL 错误 [2] [08S01]: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 4, vertexId=vertex_1576314161589_0043_1_04, diagnostics=[Task failed, taskId=task_1576314161589_0043_1_04_000002, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {"key":{"reducesinkkey0":"110-3M3304BC-0500","reducesinkkey1":"LSC02","reducesinkkey2":"2015"},"value":{"_col0":"华南","_col1":"","_col2":"","_col3":"01.01.08","_col4":"深圳市领略数控设备有限公司","_col5":"201511","_col6":"2015/11/16 17:04:39.580000000","_col7":"11","_col8":"","_col9":"LLSZPO150917034","_col10":"领胜城科技(江苏)有限公司","_col11":"领胜城","_col12":"3M3304BC","_col13":"灰色 导电胶带","_col14":"500mmx100m","_col15":15000,"_col16":"O","_col17":"BB01","_col18":"人民币","_col19":100.571599,"_col20":17,"_col21":1,"_col22":85.958632,"_col23":1508573.99,"_col24":1289379.48,"_col25":"105","_col26":"11","_col27":"/OQAAAAFAJ6A733t","_col28":"01.01.08","_col29":"深圳市领略数控设备有限公司","_col30":"/OQAAAAFAJ6A733t","_col31":"PRA1509029  10/30港车到料"}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:347)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:194)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:185)
	at java.security.AccessController.doPrivileged(Native Method)

52、hdfs误删数据恢复
  -、误删数据在当前用户（操作） 回收站目录下， 将数据mv回以前目录即可
  /user/root/.Trash/Current/app/hive/warehouse/dwmiddle.db
  
53、hive -e 加上${l_date}  
		-、双引号  ，里面单引号
		-、单引号 ， 里面单引号，外面双引号。
54、hive 大表关联小表 优化。

55、观察cpu使用率（yarn）
    是否是此任务导致集群CPU使用率：98.5%  
		application_1576699432278_0003
		
		case when :		
			方法一：

			case 
			when tb1.os = 'android' then 'android'
			when tb1.os = 'ios' then 'iPhone'
			else 'PC'
			end as os,

				1
				2
				3
				4
				5

			方法二：

			case tb1.os
			when 'android' then 'android'
			when 'ios' then 'iPhone'
			else 'PC'
			end as os,
56、step脚本呢执行报错：
ogging initialized using configuration in file:/etc/hive/2.5.3.0-37/0/hive-log4j.properties
Exception in thread "main" java.lang.RuntimeException: org.apache.tez.dag.api.SessionNotRunning: TezSession has already shutdown. Application application_1576736000097_0021 failed 2 times due to ApplicationMaster for attempt appattempt_1576736000097_0021_000002 timed out. Failing the application.

57、hbase修改timout相关配置之后，hbase Master日志刨析.
2019-12-21 10:29:32,762 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2019-12-21 10:29:47,771 WARN  [master/lymaster01/10.0.24.105:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/lymaster01,16000,1576747278473
zookeeper.RecoverableZooKeeper: ZooKeeper getData failed after 7 attempts
2019-12-21 10:29:32,608 WARN  [master/lymaster01/10.0.24.105:16000] zookeeper.ZKUtil: master:16000-0x26f1cccbf2e003e, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/master
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/master

.....
2019-12-21 10:31:39,772 INFO  [master/lymaster01/10.0.24.105:16000] regionserver.HRegionServer: master/lymaster01/10.0.24.105:16000 exiting
2019年 12月 23日 星期一 09:41:34 CST Starting master on lymaster01

	-、修改了Hbase的hbase-site.xml 中的phoenix 相关的配置之后。
			phoenix.query.timeoutMs=1800000(ms)  【ambari 默认为60s】
			hbase.regionserver.lease.period = 1200000(ms) 
			hbase.rpc.timeout = 120000 (ms)   【ambari 默认为90s】
			hbase.client.scanner.caching = 1000 （这个属性在Ambari中为Number of Fetched Rows when Scanning from Disk=1000）
			hbase.client.scanner.timeout.period = 1200000 
		根据日志提示，貌似是zookeeper与hbase连接的session失效导致的。
		-、初步原因分析--> 调大zookepper的超时时间（以前是90s）
			<property>
				<name>zookeeper.session.timeout</name>
				<value>120000</value>
			</property>
	
	
	
日志2：
 2019-12-24 02:30:45,471 WARN  [lymaster01:16000.activeMasterManager-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
.....3
2019-12-24 02:30:48,838 ERROR [master/lymaster01/10.0.24.105:16000] zookeeper.ZooKeeperWatcher: master:16000-0x36f3179c9dd006f, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181, baseZNode=/hbase-unsecure Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/master
	
58、单独安装phoenix 【phoenix仓库 http://archive.apache.org/dist/phoenix/】

	1、tar -zxvf /opt/apache-phoenix-4.9.0-HBase-1.1-bin.tar.gz
	2、cd /opt/apache-phoenix-4.9.0-HBase-1.1-bin/bin

	3、mv hbase-site.xml  hbase-site.xml-backup

	4、cp /usr/hdp/2.5.3.0-37/hbase/conf/hbase-site.xml ./

	5、chown bigdata:bigdata hbase-site.xml



	安装Phoenix 4.14.0
	phoenix安装到hnode1节点

	1. 下载Phoenix安装包apache-phoenix-4.14.0-HBase-1.2-bin.tar.gz，将其上传到/usr/local/src/software目录下
	2. 解压
	cd /usr/local/src/software
	tar zxvf apache-phoenix-4.14.0-HBase-1.2-bin.tar.gz -C /usr/local/src/
	ln -s apache-phoenix-4.14.0-HBase-1.2-bin/ phoenix

	3. 然后将phoenix的phoenix-4.14.0-HBase-1.2-server.jar拷贝到每一个RegionServer【hnode1、hnode2、hnode3节点】的classpath（即HBASE_HOME/lib目录下）
	cd /usr/local/src/phoenix
	scp phoenix-4.14.0-HBase-1.2-server.jar root@hnode1:/usr/local/src/hbase/lib/
	scp phoenix-4.14.0-HBase-1.2-server.jar root@hnode2:/usr/local/src/hbase/lib/
	scp phoenix-4.14.0-HBase-1.2-server.jar root@hnode3:/usr/local/src/hbase/lib/

	4. hbase-site.xml中进行phoenix的相关配置
	cd /usr/local/src/hbase/conf
	vim hbase-site.xml

	#### 在尾部添加下面内容
	<!--############### phoenix相关配置 ##############-->
	<!-- 配置不检查 -->
	<property>
			<name>hbase.table.sanity.checks</name>
			<value>false</value>
	</property>
	<!-- 使用phoenix创建索引需要配置此项 -->
	<property>
			<name>hbase.regionserver.wal.codec</name>
			<value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
	</property>
	<!-- 开启phoenix的schema与hbase的namespace对应关系 -->
	<property>
			<name>phoenix.schema.isNamespaceMappingEnabled</name>
			<value>true</value>
	</property>
	<!-- 是否将phoenix的系统表的schema映射到hbase的namespace -->
	<!-- 即phoenix里的SYSTEM (该schema下的6张表)映射到hbase的SYSTEM (该namespace下的6张表) -->
	<property>
			<name>phoenix.schema.mapSystemTablesToNamespace</name>
			<value>true</value>
	</property>
	<!--############### phoenix相关配置 ##############-->

	### 再将hbase-site.xml分发到其它RegionServer上
	cd /usr/local/src/hbase/conf
	scp hbase-site.xml root@hnode2:/usr/local/src/hbase/conf/
	scp hbase-site.xml root@hnode3:/usr/local/src/hbase/conf/


	5. 将hbase-site.xml拷贝到phoenix的bin目录下
	cd /usr/local/src/phoenix/bin/
	mv hbase-site.xml hbase-site.xml.template

	cd /usr/local/src/hbase/conf
	cp hbase-site.xml /usr/local/src/phoenix/bin/

	6. 重启HBase服务
	cd /usr/local/src/hbase/bin
	./stop-hbase.sh
	./start-hbase.sh


	7、升级ambari中phoenix版本（自己搭建phoenix，替换hbase中lib的jar）：
		删除原有的Hbase相关phoenix软连接 [lymaster01,lyzk01,lyzk02,lyzk03]【 phoenix-server.jar -> /usr/hdp/2.5.3.0-37//phoenix/phoenix-server.jar 】
		1-、替换hbase的lib下的phoenix-server.jar,为高版本的jar.
			cd   /usr/hdp/2.5.3.0-37/hbase/lib
			cp /opt/apache-phoenix-4.14.0-HBase-1.2-bin/phoenix-4.14.0-HBase-1.2-server.jar  /usr/hdp/2.5.3.0-37/phoenix
			rm -rf  phoenix-server.jar
			ln -s /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.14.0-HBase-1.2-server.jar phoenix-server.jar
			
			ln -s /opt/apache-phoenix-4.14.0-HBase-1.2-bin/phoenix-4.14.0-HBase-1.2-server.jar phoenix-server.jar

			恢复：
			rm -rf  phoenix-server.jar
			ln -s /usr/hdp/2.5.3.0-37//phoenix/phoenix-server.jar phoenix-server.jar

					1)-、regionserver异常1：
					WARN  [regionserver/lyzk03/10.0.24.111:16020] regionserver.HRegionServer: error telling master we are up
				com.google.protobuf.ServiceException: java.io.IOException: Call to lymaster01/10.0.24.105:16000 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=54, waitTime=10001, operationTimeout=10000 expired.
						解决：/usr/hdp/2.5.3.0-37/hbase/lib  放了两个相同类型的jar ， phoenix-server.jar  和phoenix-4.14.0-HBase-1.2-server.jar  = =
					
		2-、
			1)、修改高版本的名字 为当前集群的phoenix名字.		
				cd /usr/hdp/2.5.3.0-37/phoenix
				mv phoenix-4.7.0.2.5.3.0-37-server.jar phoenix-4.7.0.2.5.3.0-37-server.jar-backup
				mv phoenix-4.14.0-HBase-1.2-server.jar	phoenix-4.7.0.2.5.3.0-37-server.jar
				
				恢复：	
				mv phoenix-4.7.0.2.5.3.0-37-server.jar phoenix-4.14.0-HBase-1.2-server.jar	
				mv phoenix-4.7.0.2.5.3.0-37-server.jar-backup phoenix-4.7.0.2.5.3.0-37-server.jar 
			
			2)、去掉phoenix目录可能会影响的phoenix-server.jar（事实证明不影响）
				mkdir /opt/phoenix-backup
				mv /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0.2.5.3.0-37-server.jar-backup /opt/phoenix-backup/
				mv /usr/hdp/2.5.3.0-37/phoenix/phoenix-4.7.0-HBase-1.1-server.jar /opt/phoenix-backup/
				恢复：	
				cp /opt/phoenix-backup/phoenix-4.7.0.2.5.3.0-37-server.jar-backup /usr/hdp/2.5.3.0-37/phoenix/
				cp /opt/phoenix-backup/phoenix-4.7.0-HBase-1.1-server.jar  /usr/hdp/2.5.3.0-37/phoenix/
			
		3-、 执行上述“1-、”中的恢复
			 
			 恢复：执行2-2) 2-1)
			 
		总结：1-、单纯替换hbase中lib的phoenix-server.jar为高版本，regionServer启动报错，ipc超时（1000ms）
			  ambari的phoenix时集成进hbase的，rs的报错(7-1-1)，暂时未解决。
			  2-、其他解决方案：
				方案一：
					1、备份phoenix的数据
					2、升级整个ambari中的phoenix。
				方案二：
					1、降低jar为apache-phoenix-4.14.0-HBase-1.1-bin.tar
					2、重试7-1-..
							rm -rf  phoenix-server.jar
							ln -s /opt/apache-phoenix-4.14.0-HBase-1.1-bin/phoenix-4.14.0-HBase-1.1-server.jar phoenix-server.jar
						恢复：7-1
						
					错误日志[D:\ly\note\20191104_purchase\log\reginserver.log]：
					2019-12-24 12:22:46,836 ERROR [RS_OPEN_REGION-lyzk02:16020-0] index.Indexer: Must be too early a version of HBase. Disabled coprocessor
					....
					org.apache.hadoop.metrics2.MetricsException: Metrics source RegionServer,sub=PhoenixIndexer already exists!
					...
					2019-12-24 12:22:47,068 INFO  [RS_OPEN_PRIORITY_REGION-lyzk02:16020-2] coordination.ZkOpenRegionCoordination: Opening of region {ENCODED => ec61eac44ad95d739eaa3849c937612d, NAME => 'SYSTEM:CATALOG,,1567993584309.ec61eac44ad95d739eaa3849c937612d.', STARTKEY => '', ENDKEY => ''} failed, transitioning from OFFLINE to FAILED_OPEN in ZK, expecting version 0  【encoding ，默认解析时需要？】
				
				方案三：
					1、继续降低jar的版本 [4.12 --> 4.9 --> 4.8]
					2、重试7-1-..
						rm -rf  phoenix-server.jar
						ln -s /opt/apache-phoenix-4.8.0-HBase-1.1-bin/phoenix-4.8.0-HBase-1.1-server.jar phoenix-server.jar
						
						ln -s /opt/apache-phoenix-4.9.0-HBase-1.1-bin/phoenix-4.9.0-HBase-1.1-server.jar phoenix-server.jar

						结果：
							-、使用4.8版本，替换phoenix-server.jar之后，reginserver正常启动，系统正常使用，但是分页依然有异常。
							-、要保证hbase的lib下的jar和phoenix的版本一致，phoenix命令行才能启动。
							-、不需要启动ambari的phoenix-query-server，系统照常使用（应该是使用hbase的lib下的phoenix-server.jar进行解析。）
						结果2：
							-、使用4.9版本，reginserver正常启动，系统正常使用，但是正常（不用加order）。[加order by 主键，有异常，order by其他字段，无异常]
							-、reginserver日志提示 
							“2019-12-25 11:17:14,015 ERROR [B.fifo.QRpcServer.handler=0,queue=0,port=16020] coprocessor.MetaDataEndpointImpl: Old client is not compatible when system tables are upgraded to map to namespace”

							
59、hbase客户端连接，jar里面添加hbase-site.xml [D:\ly\firefox\apache-phoenix-4.9.0-HBase-1.1-bin\apache-phoenix-4.9.0-HBase-1.1-bin\phoenix-4.9.0-HBase-1.1-client.jar]
[D:\ly\firefox\phoenix-4.7.0.2.5.3.0-37-client_hbasexml.jar]

60、
mysql5.7.11对应的JDBC驱动是5.1版本。

mysql 5.7 用8.0版本的驱动可以，5.1版本也可以，5.5、5.6、5.7都不可以。

MySQL Connectors 官方文档 上面只有version8.0和version5.1两个版本的文档

version8.0文档上有说明：Connector/J 8.0 provides compatibility with all the functionality of MySQL 5.5, 5.6, 5.7, and 8.0表示兼容。

ambari-server setup --jdbc-db=mysql --jdbc-driver=/opt/mysql-connector-java-5.1.27.jar
:
Using python  /usr/bin/python
Setup ambari-server
Copying /opt/mysql-connector-java-5.1.27.jar to /var/lib/ambari-server/resources
If you are updating existing jdbc driver jar for mysql with mysql-connector-java-5.1.27.jar. Please remove the old driver jar, from all hosts. Restarting services that need the driver, will automatically copy the new jar to the hosts.
JDBC driver was successfully initialized.
Ambari Server 'setup' completed successfully.

61、uat的ambari安装注意事项：
	1、/var/lib/ambari-server/keys/ca.config 文件的CA修改为jdk8
	2、hive注释乱码 和 hive支持多分隔符
		-、
		-、参考26条记录
	3、phoenix解决二级索引和分页问题
		-、uat相关phoenix 以及ipc配置。（安装4.9版本和ipc设置 解决分页问题，）
			1、hbase，和regionserver的指定lib目录下放置建立phoenix server服务
				ln -s  /opt/phoenix-4.9.0-HBase-1.1-server.jar  /usr/hdp/2.5.3.0-37/hbase/lib/phoenix-server.jar
			2、重启集群
			3、错误提示： Error: ERROR 726 (43M10): Inconsistent namespace mapping properties. Cannot initiate
			集群hbase-site.xml 中添加 
				phoenix.schema.isNamespaceMappingEnabled=ture
			4、dbeaner工具，添加属性：
				phoenix.schema.isNamespaceMappingEnabled=True
				phoenix.schema.mapSystemTablesToNamespace=True
			5、
				uat中hbase相关配置添加：
					hbase.client.operation.timeout=3600000
					hbase.client.scanner.timeout.period=3600000
					hbase.table.sanity.checks=false
					phoenix.query.keepAliveMs=3600000
					phoenix.schema.isNamespaceMappingEnabled=true
					phoenix.schema.mapSystemTablesToNamespace=true
					HBase RPC Timeout=3600000
					Phoenix Query Timeout=3600000
		-、二级索引解决见本文搜索关键字查找。

62、Package does not match intended download. Suggestion: run yum --enablerepo=base clean metadata
	-- yum clean all 解决

62.2-、tez error 数据转换

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, 
vertexId=vertex_1577709896135_0038_1_00, diagnostics=[Task failed, taskId=task_1577709896135_0038_1_00_000000, diagnostics=[TaskAttempt 0 failed, 
info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: 
Hive Runtime Error while processing row 

62.3 -、Hbase 的ipc超时  
		-、cellsPerHeartbeatCheck定义了心跳发送的周期，该值由"hbase.cells.scanned.per.heartbeat.check"配置，
	默认是10000，表示的是每scan出10000个cell，则服务端向客户端发送一条心跳。
		-、phoenix.query.keepAliveMs=1200000
		   phoenix.query.timeoutMs=1200000
		   
Hortonworks recommends the following best practices for timeout values, located in the HBase-site.xml file:
Setting	Value
hbase.rpc.timeout	300000
hbase.client.scanner.timeout.period	300000
zookeeper.session.timeout	300000
phoenix.query.timeoutMs	300000
phoenix.query.keepAliveMs	300000

https://www.e-learn.cn/content/wangluowenzhang/1032399

	-- 解决： 只修改了集群的配置，client需要替换client的jar包中的hbase-site.xml文件。

63、UAT大数据环境搭建


10.0.24.200  uatdata01  uatzk01 uatmysql01      uatweb01        uatnginx
10.0.24.201  uatdata02  uatzk02 uatambari       uatweb02
10.0.24.202  mysql-node2	uatdata03  uatzk03
10.0.24.203  mycat-node	uatdata04  uatkettle	uatmysql02  
10.0.24.204  uatmaster01        uatmycluster
10.0.24.205  uatmaster02        uatmycluster


设置主机名称：
	hostnamectl set-hostname lydata02
	
配置全域名：vim /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=lydata01

设置禁用ip6：vim /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6=1
net.ipv6.conf.default.disable_ipv6=1
	sysctl -p 执行禁用配置：
	
java-home:/usr/local/jdk
-、mysql HA

mysql> mysql> SHOW MASTER STATUS

+------------------+----------+--------------+------------------+-------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+------------------+----------+--------------+------------------+-------------------+
| mysql-bin.000001 |      154 |              |                  |                   |
+------------------+----------+--------------+------------------+-------------------+


CHANGE MASTER TO MASTER_HOST='10.0.24.200', MASTER_USER='repl', MASTER_PASSWORD='Zxt1234!', MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=154


CHANGE MASTER TO MASTER_HOST='10.0.24.203', MASTER_USER='repl', MASTER_PASSWORD='Zxt1234!', MASTER_LOG_FILE='mysql-bin.000002',MASTER_LOG_POS=154


-、ambari-server /usr/share/java 有mysql的驱动。mysql-connector-java-5.1.46.jar
需要重命名为 mysql-connector-java.jar

-、、上一步还未解决，可修改服务器(ambari-server)的java安全机制。
vim 	$JAVA_HOME/jre/lib/security/java.security

新增：也修改agent的java安全机制。



uatdata01
uatdata02
uatdata03
uatdata04
uatmaster01
uatmaster02

uatdata01
uatdata02
mysql-node2
mycat-node
uatmaster01
uatmaster02


-、Ambari安装hive组件出现need string or buffer, NoneType found

首先在ambari-server端执行

ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar
如果执行后依旧没有反应。可能是目录下没有此文件，只需要将jar包放到相应的文件目录下即可。

在mysql中为Hive设置用户：
――――――――――――――――
版权声明：本文为CSDN博主「稻草一根」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/zhou_shaowei/article/details/75053460


create database oozie character set utf8 
  
CREATE USER 'oozie'@'%'IDENTIFIED BY 'Oozie-123'

GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%'

FLUSH PRIVILEGES


-、hbase启动报错， 参考日志，以及查看./zkCLi.sh 删除/hbase-un,重启hbase。

参考资料:https://blog.csdn.net/u013982921/article/details/82470005


-----------------------------------<<<<<<  again setup 之后 >>>>>>>>>>>>>>>----------------------------------------------
-、hivemserver2  挂掉
log：
2020-01-03 01:54:11,379 INFO  [main-SendThread(uatdata02:2181)]: zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server uatdata02/10.0.24.201:2181. Will not attempt to authenticate using SASL (unknown error)
2020-01-03 01:54:11,380 INFO  [main-SendThread(uatdata02:2181)]: zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to uatdata02/10.0.24.201:2181, initiating session
2020-01-03 01:54:11,380 INFO  [main-SendThread(uatdata02:2181)]: zookeeper.ClientCnxn (ClientCnxn.java:run(1138)) - Unable to reconnect to ZooKeeper service, session 0x16f663239460010 has expired, closing socket connection
2020-01-03 01:54:18,759 ERROR [main-EventThread]: server.HiveServer2 (HiveServer2.java:process(338)) - Failed to close the persistent ephemeral znode
java.io.IOException: org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hiveserver2/serverUri=uatmaster02:10000
version=1.2.1000.2.5.3.0-37
sequence=0000000000


-、zk的 warning
2020-01-02 20:21:31,204 - INFO  [CommitProcessor:1:ZooKeeperServer@617] - Established session 0x16f663239460011 with negotiated timeout 40000 for client /10.0.24.202:41668
2020-01-02 20:21:32,598 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@357] - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x16f663239460011, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
        at java.lang.Thread.run(Thread.java:748)
2020-01-02 20:21:32,599 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /10.0.24.202:41668 which had sessionid 0x16f663239460011
2020-01-02 20:21:41,044 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.24.202:41672
2020-01-02 20:21:41,046 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /10.0.24.202:41672
......
2020-01-03 10:00:49,484 - INFO  [Thread-821:NIOServerCnxn@1007] - Closed socket connection for client /10.0.24.202:59770 (no session established for client)
2020-01-03 10:01:49,477 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.24.202:59785
2020-01-03 10:01:49,477 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /10.0.24.202:59785
2020-01-03 10:01:49,478 - INFO  [Thread-822:NIOServerCnxn@1007] - Closed socket connection for client /10.0.24.202:59785 (no session established for client)
2020-01-03 10:02:49,478 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.24.202:59808
2020-01-03 10:02:49,478 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /10.0.24.202:59808


-- 外网给的答案：
this issue is fixed. root cause is /etc/hosts on worker nodes. we had to add /etc/hosts to test another component and i did not follow the order , 
FQDN is not followed by ip. that was the reason RS freaked-out.
https://community.cloudera.com/t5/Support-Questions/Master-failed-to-complete-initialization-after-900000ms/m-p/189368


-- 业务数据剧增，导致master启动initial时间过长。
https://cloud.tencent.com/developer/article/1488474


------------ ------ ------ ------ ------ ------ ------ <<<<<<：总结>>>>>>>>>>>>>------------- ------ ------ ------ ------------- ------ 
-、当前版本还是ambari-server还是使用jdk1.7的比较好，下载ambari更高的版本可以使用jdk1.8

rpm -e --nodeps extjs-2.2-1.noarch

rpm -e --nodeps snappy-devel-1.1.0-3.el7.x86_64



-、Grafana Admin Password:admin
-、mysql  [10.0.24.203 ,10.0.24.200]
root/1234

-、linux （uat）添加用户

useradd admin.ly/admin.ly123
useradd zeng.mao.quan/zeng.mao.quan123
useradd zhong.hai/zhong.hai123
useradd jane.zhou1/jane.zhou1123
useradd wang.ying.nan/wang.ying.nan123

-、uatnginx/uatnginx123 


-、linux脚本添加新用户
编写脚本：
#!/bin/bash

cat $1 | while read line
do
name=echo $line |awk -F '/' '{print $1}' 
passwd=echo $line |awk -F '/' '{print $2}'
echo $name
echo $passwd
useradd $name
echo "$passwd" | passwd --stdin $name &> /dev/null
echo "用户$name创建完成，默认密码是：$passwd"
done

==============================================================   日志报错   	====================================================================


-、hiveserver2.log   【hive -hiveconf hive.root.logger=debug,console】
2020-01-03 18:53:20,403 WARN  [HiveServer2-Handler-Pool: Thread-39]: metrics2.CodahaleMetrics (CodahaleMetrics.java:addGauge(299)) - A Gauge with name [init_total_count_dbs] already exists.  The old gauge will be overwritten, but this is not recommended

-、客户端报错--  /tmp/hive/hive.log
2020-01-03 19:00:11,081 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(957)) - Localizing resource because it does not exist: file:/usr/hdp/2.5.3.0-37/hive/lib/hive-exec-1.2.1000.2.5.3.0-37.jar to dest: hdfs://uatmaster01:8020/user/hive/.hiveJars/hive-exec-1.2.1000.2.5.3.0-37-73c4176cfc45ceebf176bd04debfa4f205ac40e276e00c41be5c084b0a22f7c5.jar

2020-01-03 19:00:10,842 INFO  [main]: tez.DagUtils (DagUtils.java:getHiveJarDirectory(881)) - Jar dir is null/directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars

-、hive-err.log
Fri Jan 03 19:15:44 CST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Fri Jan 03 19:15:44 CST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.

解决方法：
jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&characterEncoding=utf-8&useSSL=false

-、beeline可以连接： beeline -u jdbc:hive2://10.0.24.205:10000 -n hadoop
	
	

2020-01-06  --> hiveCLi 可以使用  ，  hive2 abort， hbase abort
一、hbase和hiveserver2每天定时挂掉
1-、hbase 挂掉 []
hbase error:
This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it

2020-01-06 23:48:57,065 FATAL [main-EventThread] master.HMaster: Master server abort: loaded coprocessors are: [org.apache.hadoop.hbase.backup.master.BackupController]

2020-01-06 23:48:57,065 FATAL [main-EventThread] master.HMaster: Exception reading unassigned node for region=1588230740
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/region-in-transition/1588230740

Exception reading unassigned node for region=1588230740

zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x36f783a9aa80007, likely server has closed socket, closing socket connection and attempting reconnect

Cannot submit [ServerShutdownHandler-uatdata02,16020,1578270573995-11] because the executor is missing. Is this process shutting down?
zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 7 attempts

2020-01-06 07:43:54,402 WARN  [uatmaster02:16000.activeMasterManager] retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.delete over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.PathIsNotEmptyDirectoryException): /apps/hbase/data/WALs/uatdata04,16020,1578048647969-splitting is non empty': Directory is not empty


-- 出现session expired的时间：
2020-01-03 23:45:07
2020-01-06 23:46:51
2020-01-07 23:30:29
2020-01-08 23:36:10
2020-01-09 23:43:27

	-、可能是fullGC 导致：
		https://blog.csdn.net/yjb7268888/article/details/52980570  [fullGC]
		https://blog.csdn.net/xuguokun1986/article/details/70884696  [修改zookeeper session timeout]
		http://www.openskill.cn/question/435 [图解]
		https://blog.csdn.net/liu16659/article/details/82430396 [GC]
		https://blog.csdn.net/sinat_29480069/article/details/81503562 [GC 日志解析]
		https://blog.csdn.net/zlfprogram/article/details/74066700  [full GC 宕机 GC日志中是否有 promotion failed和concurrent mode failure]
	-、集群时间不同步或者 zookeeper中的数据不一致
		https://blog.csdn.net/crq1205/article/details/82772923	
	-、加长zookeeper的超时时间。
		https://blog.csdn.net/qq_28652401/article/details/83510046
	-、关于session expired
		https://www.coder4.com/archives/3181
	-、hiveserver2的GC调优
		https://blog.csdn.net/mnasd/article/details/82690414
		https://blog.csdn.net/xiaolong_4_2/article/details/84323990
	
	-、hfile.block.cache.size 的原因
		查看Regionserver中heapsize设置为32G，hfile.block.cache.size=0.4,使得block size=heapsize * hfile.block.cache.size *0.85=10.88G
		由此可以block cache发生了CMS GC。目前使用的BlockCache优点是直接采用jvm提供的HashMap来管理Cache，简单可依赖；内存用多少占多少，JVM会帮你回收淘汰的BlOCK占用的内存。缺点更明显：
		a.一个Block从被缓存至被淘汰，基本伴随着Heap中的位置从New区晋升到Old区
		b.晋升在Old区的Block被淘汰后，最终由CMS进行垃圾回收，随之带来的是Heap碎片 ，old 区域变大导致cms 时间过长。
		c.因为碎片问题，随之而来的是GC时晋升失败的FullGC，我们的线上系统根据不同的业务特点，因为这个而发生FullGC的频率，有1天的，1周的，1月半年的都有。对于高频率的， 在运维上可以通过在半夜手工触发FullGC来缓解
		d.如果缓存的速度比淘汰的速度快，很不幸现在的代码有OOM的风险(这个可以修改下代码避免)
		https://blog.yoodb.com/sugarliny/article/detail/1306
		https://www.2cto.com/database/201503/384922.html
	
	1.1、zookeeper报错 ：EndOfStreamException: Unable to read additional data from client sessionid 0x16f783a9a9c0009, likely client has closed socket

	1.2、regionserver报错:
2020-01-06 23:53:21,557 WARN  [regionserver/uatdata01/10.0.24.200:16020] regionserver.HRegionServer: Failed deleting my ephemeral node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/uatdata01,16020,1578270573155
	1.3、是否是因为linux的unlimited限制导致挂掉（以及影响测试环境的mr时间？）
	
	1.4、ambari-metric报错（controller）：
	WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR 
		java.lang.NullPointerException
	1.5、hiveserver-2报错：
		KeeperErrorCode = Session expired for /hiveserver2/serverUri=uatmaster02:10000
version=1.2.1000.2.5.3.0-37
sequence=0000000003


2-、hiveserver2挂掉

1and2感觉是zookeeper的session过期导致的 [持续观察。]


3-、测试解决
	-、增大zookeeper的session时间
			MaxSessionTimeout=120000
	-、删除zookeeper目录下的  hiveserver2 测试
	-、貌似不是GC的问题
	-、Hbase GC日志提示 Heap par new generation   total 309056K (300M)
	-、总结 ：是由于GC导致的

hiveserver2的日志(GC)：
2020-01-08 23:27:46,015 INFO  [org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@36359723]: common.JvmPauseMonitor (JvmPauseMonitor.java:run(193)) - Detected pause in JVM or host machine (eg GC): pause of approximately 3328ms
No GCs detected
......
2020-01-08 23:31:50,036 INFO  [org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@36359723]: common.JvmPauseMonitor (JvmPauseMonitor.java:run(193)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1302ms
No GCs detected
2020-01-08 23:31:52,737 INFO  [org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@36359723]: common.JvmPauseMonitor (JvmPauseMonitor.java:run(193)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2200ms
No GCs detected
2020-01-08 23:31:55,015 INFO  [org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@36359723]: common.JvmPauseMonitor (JvmPauseMonitor.java:run(193)) - Detected pause in JVM or host machine (eg GC): pause of approximately 1278ms
No GCs detected

hbase的日志(GC)：
2020-01-03 23:34:35,159 WARN  [master/uatmaster02/10.0.24.205:16000] util.Sleeper: We slept 189492ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
......
2020-01-06 23:38:22,914 WARN  [master/uatmaster02/10.0.24.205:16000] util.Sleeper: We slept 192929ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
......
2020-01-07 23:25:58,004 WARN  [master/uatmaster02/10.0.24.205:16000] util.Sleeper: We slept 194060ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired

参考资料：https://blog.csdn.net/xjping0794/article/details/78376293

二-、hive重启之后，会有spark的任务在跑，怎么取消。 [1、导致hiveCli进不去， 2、hiveserver2中执行的TEZ任务，appending]
[相关日志：D:\ly\note\20191104_purchase\uat_log\uat_hbase_gc\hive-jos.log]
错误提示： /tmp/root/hive.log
2020-02-13 16:56:26,002 WARN  [main]: ipc.Client (Client.java:handleConnectionFailure(886)) - Failed to connect to server: uatmaster01/10.0.24.204:8032: retries get failed due to exceeded maximum allowed retries number: 0
java.net.ConnectException: Connection refused

	-、spark的任务是 sparkthrift 和 sparkthrift2  ，因为Operation category READ is not supported in state standby.
	导致appending。（因为启动了8个application任务，导致append）
20/01/09 18:36:15 INFO RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.getFileInfo over uatmaster01/10.0.24.204:8020. Trying to failover immediately.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby

  资料：spark继承HA 
	https://www.cppentry.com/bencandy.php?fid=116&id=221660
	-、以前异常退出的sparkthrift 在hdfs://uatmycluster/spark2-history目录下，不能正常加载【hdfs://uatmycluster/spark2-history/application_1578552094353_0005.inprogress】
	-、为什么启动sparkthrift服务，对应的sparksubmit任务，一直在yarn任务那里挂着？
	
	-、spark  启动的时候
	yarn错误日志：
	20/01/10 13:40:26 WARN Client: Failed to connect to server: uatmaster01/10.0.24.204:8032: retries get failed due to exceeded maximum allowed retries number: 0 java.net.ConnectException: Connection refused
	AM is not registered for known application attempt: appattempt_1583193148938_0002_000001 or RM had restarted after AM registered . AM should re-register.
解决：应该是spark应用启动的时候，请求到RM备份主机上，所以连接不上。

	
三、
解决方案：
	1-、调整hbase  ，
		-、zk session-timeout  -- > 2minute
		-、设置hbase-env template
			1-、add -XX:+UseParNewGC
			{% if security_enabled %}
			export HBASE_OPTS="$HBASE_OPTS -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ErrorFile={{log_dir}}/hs_err_pid%p.log -Djava.security.auth.login.config={{client_jaas_config_file}} -Djava.io.tmpdir={{java_io_tmpdir}}"
			export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx{{master_heapsize}} -Djava.security.auth.login.config={{master_jaas_config_file}} $JDK_DEPENDED_OPTS"
			export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70  -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}} -Djava.security.auth.login.config={{regionserver_jaas_config_file}} $JDK_DEPENDED_OPTS"
			export PHOENIX_QUERYSERVER_OPTS="$PHOENIX_QUERYSERVER_OPTS -Djava.security.auth.login.config={{queryserver_jaas_config_file}}"
			{% else %}
			export HBASE_OPTS="$HBASE_OPTS -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ErrorFile={{log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{java_io_tmpdir}}"
			export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx{{master_heapsize}} $JDK_DEPENDED_OPTS"
			export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70  -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}} $JDK_DEPENDED_OPTS"
			{% endif %}
			2-、change -XX:MaxPermSize=256m
			{% if java_version < 8 %}
				JDK_DEPENDED_OPTS="-XX:PermSize=128m -XX:MaxPermSize=256m"
			{% endif %}
			
			
			-- regionserver.log (uatdata01)
				2020-01-03 23:45:06,512 WARN  [main-SendThread(uatdata02:2181)] zookeeper.ClientCnxn: Session 0x36f6b0636910007 for server uatdata02/10.0.24.201:2181, unexpected error, closing socket connection and attempting reconnect
					java.io.IOException: Broken pipe
			--  uatmaster02 ,竟然有个hmaster进程 3476 org.apache.hadoop.hbase.master.HMaster  【Metrics Collector】
	
	2、调大zookeeper的session(MaxSessionTimeout)超时时间 为2.5个小时。
		-、hiveserver2 
			2020-01-14 00:24:11,979 INFO  [org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@40717ed]: common.JvmPauseMonitor (JvmPauseMonitor.java:run(193)) - Detected pause in JVM or host machine (eg GC): pause of approximately 4272ms
			No GCs detected
			......
			2020-01-14 03:07:14,159 ERROR [main-EventThread]: nodes.PersistentEphemeralNode (PersistentEphemeralNode.java:deleteNode(323)) - Deleting node: /hiveserver2/serverUri=uatmaster02:10000
version=1.2.1000.2.5.3.0-37
sequence=0000000009
			org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hiveserver2/serverUri=uatmaster02:10000
version=1.2.1000.2.5.3.0-37
sequence=0000000009
				结论-、GC 导致 连接失效， 2.5小时候，连接不上。
		-、hbase日志 
			Master：
			2020-01-14 00:24:14,974 WARN  [master/uatmaster02/10.0.24.205:16000] util.Sleeper: We slept 200949ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
			......
			2020-01-14 00:27:13,054 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/region-in-transition/1588230740
			...
			2020-01-14 00:27:13,744 ERROR [B.priority.fifo.QRpcServer.handler=17,queue=1,port=16000] master.MasterRpcServices: Region server uatdata01,16020,1578893028947 reported a fatal error:
			ABORTING region server uatdata01,16020,1578893028947: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected
 currently processing uatdata01,16020,1578893028947 as dead 
			....
			Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.YouAreDeadException): org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected
 currently processing uatdata01,16020,1578893028947 as dead server
				结论
					-、GC 
					-、uatdata01  dead server。
			
			Slave1：
				2020-01-14 00:25:08,299 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3944ms
				No GCs detected
				......
				2020-01-14 00:27:12,090 WARN  [regionserver/uatdata01/10.0.24.200:16020-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
				org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
				...
				2020-01-14 00:27:13,731 FATAL [regionserver/uatdata01/10.0.24.200:16020] regionserver.HRegionServer: ABORTING region server uatdata01,16020,1578893028947: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected
 currently processing uatdata01,16020,1578893028947 as dead server

	3、
		-、恢复zk的MaxSessionTimeout
		-、调整regionserver的 heapsize
		-、开启regionserver 失败重启 restart 替换 abort
		
		为什么regionserver 和Zookeeper的session expired? 可能的原因有
			1. 网络不好。
			2. Java full GC， 这会block所有的线程。如果时间比较长，也会导致session expired.
			怎么办？	
			1. 将Zookeeper的timeout时间加长。
			2. 配置“hbase.regionserver.restart.on.zk.expire” 为true。 这样子，遇到ZooKeeper session expired ，regionserver将选择 restart 而不是 abort
			具体的配置是，在hbase-site.xml中加入
			<property>
				<name>zookeeper.session.timeout</name>
				<value>90000</value>
			</property>
			
			<property>
				<name>hbase.regionserver.restart.on.zk.expire</name>
				<value>true</value>
				<description>
				Zookeeper session expired will force regionserver exit.
				Enable this will make the regionserver restart.
				</description>
			</property>
			为了避免java full GC suspend thread 对Zookeeper heartbeat的影响，我们还需要对hbase-env.sh进行配置。
			将
			export HBASE_OPTS="$HBASE_OPTS -XX:+HeapDumpOnOutOfMemoryError \ -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode"
			修改成
			export HBASE_OPTS="$HBASE_OPTS -XX:+HeapDumpOnOutOfMemoryError \ -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled \ -XX:+CMSInitiatingOccupancyFraction=70 \ -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseParNewGC -Xmn256m"
			
				-Xmx8g -Xms8g CXmn128m ：最大堆内存8G，最小堆内存8G(堆的初始化内存)，新生代内存-Xmn128m。
				-XX:+UseParNewGC ： 设置对于新生代的垃圾回收器类型，这种类型是会停止JAVA进程，然后再进行回收的，但由于新生代体积比较小，持续时间通常只有几毫秒，因此可以接受。
				-XX:+UseConcMarkSweepGC ：设置老生代的垃圾回收类型，如果用新生代的那个会不合适，即会导致JAVA进程停止的时间太长，用这种不会停止JAVA进程，而是在JAVA进程运行的同时，并行的进行回收。
				-XX:CMSInitiatingOccupancyFraction ：设置CMS回收器运行的频率，避免前两个参数引起JAVA进程长时间停止，设置了这个之后，不需要停止JAVA进程，但是会提高CPU使用率。
				
				当前集群设置 hbase-env template :
				export HBASE_OPTS="$HBASE_OPTS -verbose:gc -Xloggc:$HBASE_LOG_DIR/hbase.gc.log -XX:ErrorFile=$HBASE_LOG_DIR/hs_err_pid.log -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:CMSInitiatingOccupancyFraction=70"
			 
		参考资料：http://wenda.chinahadoop.cn/question/4872’
		
		小结：
		uatdata01:
			-、[ParNew: 1359997K->36143K(1504064K)] 1359997K->36143K(8221504K) 
			-、2020-01-14 23:30:28,665 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3082ms
			No GCs detected
			-、2020-01-14 23:35:35,208 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/rs
			
			-、2020-01-15 23:28:36,462 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 	3620ms
			No GCs detected
			-、2020-01-15 23:35:56,413 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/rs
			
		uatdata02:
			-、ParNew: 1359085K->22607K(1504064K)] 1359085K->22607K(8221504K)
			-、2020-01-14 23:32:29,453 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3661ms
			No GCs detected
			-、2020-01-14 23:35:35,207 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/rs
		uatdata03:
			-、[ParNew: 1366480K->29634K(1504064K)] 1366480K->29634K(8221504K)
			-、2020-01-14 23:32:14,267 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1580ms
			No GCs detected
			-、2020-01-14 23:35:35,484 INFO  [regionserver/uatdata03/10.0.24.202:16020-SendThread(uatdata03:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session  0x26fa3df96850001has expired, closing socket connection		
		uatdata04:
			-、[ParNew: 1364088K->29137K(1504064K)] 1364088K->29139K(8221504K)
			-、2020-01-14 23:37:34,072 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1878ms
			No GCs detected
			-、2020-01-14 23:32:16,107 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/rs
			
		uatmaster02:
			-、[ParNew: 278398K->3556K(309056K)] 290335K->15494K(995840K)
			-、2020-01-14 23:29:27,026 WARN  [master/uatmaster02/10.0.24.205:16000] util.Sleeper: We slept 202092ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
			-、2020-01-14 23:35:35,209 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/rs

	
			-、2020-01-15 23:30:19,605 WARN  [master/uatmaster02/10.0.24.205:16000] util.Sleeper: We slept 203273ms instead of 3000ms, this is likely due to a -、long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
			2020-01-15 23:35:56,413 WARN  [main-EventThread] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=uatdata01:2181,uatdata02:2181,uatdata03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/rs
	4、hiveserver2 安装到别的主机上试试
		-、怀疑是微服务，后端占用jvm内存 （因为测试服务器上，跑大量MR,会导致HBase GC严重，导致zk的session expired，rs下架）
		-、调大 hMaster 内存为2G
		
		-、难道是 state standby  切换导致
		
	5、删除zk中hiveserver2.
	  设置-、zookeeper.session.timeout
			hbase.rpc.timeout
		-、zk maxsession
		
		
	6-、官网解答： http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
		146.2.7. ZooKeeper SessionExpired events
		Master or RegionServers shutting down with messages like those in the logs:

		WARN org.apache.zookeeper.ClientCnxn: Exception
		closing session 0x278bd16a96000f to sun.nio.ch.SelectionKeyImpl@355811ec
		java.io.IOException: TIMED OUT
			   at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
		WARN org.apache.hadoop.hbase.util.Sleeper: We slept 79410ms, ten times longer than scheduled: 5000
		INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server hostname/IP:PORT
		INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/IP:PORT remote=hostname/IP:PORT]
		INFO org.apache.zookeeper.ClientCnxn: Server connection successful
		WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x278bd16a96000d to sun.nio.ch.SelectionKeyImpl@3544d65e
		java.io.IOException: Session Expired
			   at org.apache.zookeeper.ClientCnxn$SendThread.readConnectResult(ClientCnxn.java:589)
			   at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:709)
			   at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:945)
		ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: ZooKeeper session expired
		The JVM is doing a long running garbage collecting which is pausing every threads (aka "stop the world"). Since the RegionServer’s local ZooKeeper client cannot send heartbeats, the session times out. By design, we shut down any node that isn’t able to contact the ZooKeeper ensemble after getting a timeout so that it stops serving data that may already be assigned elsewhere.

		Make sure you give plenty of RAM (in hbase-env.sh), the default of 1GB won’t be able to sustain long running imports.

		Make sure you don’t swap, the JVM never behaves well under swapping.

		Make sure you are not CPU starving the RegionServer thread. For example, if you are running a MapReduce job using 6 CPU-intensive tasks on a machine with 4 cores, you are probably starving the RegionServer enough to create longer garbage collection pauses.

		Increase the ZooKeeper session timeout

		If you wish to increase the session timeout, add the following to your hbase-site.xml to increase the timeout from the default of 60 seconds to 120 seconds.

		<property>
		  <name>zookeeper.session.timeout</name>
		  <value>120000</value>
		</property>
		<property>
		  <name>hbase.zookeeper.property.tickTime</name>
		  <value>6000</value>
		</property>
		Be aware that setting a higher timeout means that the regions served by a failed RegionServer will take at least that amount of time to be transferred to another RegionServer. For a production system serving live requests, we would instead recommend setting it lower than 1 minute and over-provision your cluster in order the lower the memory load on each machines (hence having less garbage to collect per machine).

		If this is happening during an upload which only happens once (like initially loading all your data into HBase), consider bulk loading.

		See ZooKeeper, The Cluster Canary for other general information about ZooKeeper troubleshooting.
		
-、当前服务的hbase-env.sh
export HBASE_MANAGES_ZK=false

JDK_DEPENDED_OPTS="-XX:PermSize=128m -XX:MaxPermSize=256m"

export HBASE_OPTS="$HBASE_OPTS -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ErrorFile=/var/log/hbase/hs_err_pid%p.log -Djava.io.tmpdir=/tmp"
export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xmx2048m $JDK_DEPENDED_OPTS"
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmn1632m -XX:CMSInitiatingOccupancyFraction=70  -Xms8192m -Xmx8192m $JDK_DEPENDED_OPTS"

# HBase off-heap MaxDirectMemorySize
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS "
export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS "

	-、设置了-XX:+UseParNewGC  ，但是 regionserver 的 -Xmn1632m（新生代内存太大），可以尝试 将此值调低测试。
	参考资料：https://www.cnblogs.com/cenyuhai/p/3235101.html

-- -- -- - - - - - -
-- 暂时通过restapi来控制 hiveserver2 以及 hbase服务的重启
curl -u admin:pCWldx0zqo http://10.0.24.200:8080/api/v1/clusters

-- 获取某个服务的component
curl -u admin:pCWldx0zqo -H "X-Requested-By: ambari" -X GET http://10.0.24.200:8080/api/v1/clusters/uatmycluster/services/HIVE/components/
curl -u admin:pCWldx0zqo -H "X-Requested-By: ambari" -X GET http://10.0.24.200:8080/api/v1/clusters/uatmycluster/services/HBASE/components/
-- 重新启动hiveserver2
curl -u admin:pCWldx0zqo -H 'X-Requested-By: ambari' -X POST -d '{
"RequestInfo":{
  "command":"RESTART",
  "context":"Restart HiveServer2 on uatmycluster",
  "operation_level":{
	 "level":"HOST",
	 "cluster_name":"uatmycluster"
  }
},
"Requests/resource_filters":[
  {
	 "service_name":"HIVE",
	 "component_name":"HIVE_SERVER",
	 "hosts":"uatmaster02"
  }
]
}' http://10.0.24.200:8080/api/v1/clusters/uatmycluster/requests


-- 重启 hbase的所有服务

curl -u admin:pCWldx0zqo -H "X-Requested-By: ambari" -X PUT -d '{"RequestInfo":{"context":"START HBASE ALL "},"Body":{"ServiceInfo":{"state":"STARTED"}}}' http://10.0.24.200:8080/api/v1/clusters/uatmycluster/services/HBASE


-- 单独重启 hbase master  ，hregionserver
curl -u admin:pCWldx0zqo -H 'X-Requested-By: ambari' -X POST -d '{
"RequestInfo":{
  "command":"RESTART",
  "context":"Restart HbaseMaster on uatmycluster",
  "operation_level":{
	 "level":"HOST",
	 "cluster_name":"uatmycluster"
  }
},
"Requests/resource_filters":[
  {
	 "service_name":"HBASE",
	 "component_name":"HBASE_MASTER",
	 "hosts":"uatmaster02"
  }
]
}' http://10.0.24.200:8080/api/v1/clusters/uatmycluster/requests


curl -u admin:pCWldx0zqo -H 'X-Requested-By: ambari' -X POST -d '{
"RequestInfo":{
  "command":"RESTART",
  "context":"Restart HREGIONSERVER on uatmycluster",
  "operation_level":{
	 "level":"HOST",
	 "cluster_name":"uatmycluster"
  }
},
"Requests/resource_filters":[
  {
	 "service_name":"HBASE",
	 "component_name":"HBASE_REGIONSERVER",
	 "hosts":"uatdata01"
  }
]
}' http://10.0.24.200:8080/api/v1/clusters/uatmycluster/requests


uatmaster02主机：
脚本 执行命令：
*/3 * * * * source /etc/profile && cd /opt/shell/cluster_monitor  && sh moniter_hiveserver2.sh >> moniter_hiveserver2.log 2>&1
*/3 * * * * source /etc/profile && cd /opt/shell/cluster_monitor  && sh moniter_hmaster.sh >> moniter_hmaster.log 2>&1
*/3 * * * * source /etc/profile && cd /opt/shell/cluster_monitor  && sh moniter_rs01.sh >> moniter_rs01.log 2>&1
*/3 * * * * source /etc/profile && cd /opt/shell/cluster_monitor  && sh moniter_rs02.sh >> moniter_rs02.log 2>&1
*/3 * * * * source /etc/profile && cd /opt/shell/cluster_monitor  && sh moniter_rs03.sh >> moniter_rs03.log 2>&1
*/3 * * * * source /etc/profile && cd /opt/shell/cluster_monitor  && sh moniter_rs04.sh >> moniter_rs04.log 2>&1

参考资料：https://www.cnblogs.com/felixzh/p/10710204.html
	
===============================
=======================
======================
正式环境 ，NameNode，RS 监控
主机：promater02
目录： /home/shell/cluster_monitor


测试环境 服务 监控 ：  
	NameNode,zkfc
	ResourceManager,
	nodemanager,
	datanode,journalNode,
	HMaster,HRegionServer,
	HiveServer2,HiveMeta,
	zk,
	
目录：/home/shell/cluster_moniter
【ssh root@$hostname " su - hive -c ' nohup hive --service hiveserver2 & '"】
	
		question：
			1、lymaster02， NN进程存货，但是日志报错
				2021-10-29 06:49:10,915 INFO  ha.EditLogTailer (EditLogTailer.java:triggerActiveLogRoll(271)) - Triggering log roll on remote NameNode lymaster01/10.0.24.105:8020
				2021-10-29 06:51:42,696 WARN  ha.EditLogTailer (EditLogTailer.java:triggerActiveLogRoll(276)) - Unable to trigger a roll of the active NN
				java.net.SocketTimeoutException: Call From lymaster02/10.0.24.106 to lymaster01:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.0.24.106:45855 remote=lymaster01/10.0.24.105:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
				...
				2021-10-29 06:52:52,838 WARN  client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 70046 ms (timeout=90000 ms) for a response for selectInputStreams. No responses yet.
				2021-10-29 06:52:52,937 INFO  timeline.HadoopTimelineMetricsSink (AbstractTimelineMetricsSink.java:emitMetricsJson(135)) - Unable to connect to collector, http://lymaster02:6188/ws/v1/timeline/metrics
				This exceptions will be ignored for next 100 times
				...
				2021-10-29 06:53:12,792 WARN  namenode.FSEditLog (JournalSet.java:selectInputStreams(280)) - Unable to determine input streams from QJM to [10.0.24.107:8485, 10.0.24.110:8485, 10.0.24.111:8485]. Skipping.
				java.io.IOException: Timed out waiting 90000ms for a quorum of nodes to respond.
				...
				2021-10-29 06:54:39,052 WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:run(192)) - Detected pause in JVM or host machine (eg GC): pause of approximately 70831ms
				GC pool 'ParNew' had collection(s): count=1 time=46ms
				2021-10-29 06:55:14,870 WARN  ha.EditLogTailer (EditLogTailer.java:triggerActiveLogRoll(276)) - Unable to trigger a roll of the active NN
				java.net.SocketTimeoutException: Call From lymaster02/10.0.24.106 to lymaster01:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.0.24.106:46260 remote=lymaster01/10.0.24.105:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
				...
				2021-10-29 07:00:33,288 WARN  util.JvmPauseMonitor (JvmPauseMonitor.java:run(192)) - Detected pause in JVM or host machine (eg GC): pause of approximately 35979ms
				No GCs detected
				...
				
			
	
-、测试telnet 
telnet 10.0.24.110 8080
telnet 10.0.24.200 8080

韩工，你好，大数据相关的，10.0.24.20x  网段，可以ping通，端口也可以访问。
但是10.0.24.10x网段，可以ping通，端口不可以访问。

测试：
大数据：
10.0.24.106
10.0.24.107
10.0.24.110
10.0.24.111
java+前端：
10.0.40.112
10.0.40.113
10.0.40.114
10.0.40.115
10.0.40.116

uat：
大数据+java+前端
10.0.24.200
10.0.24.201
10.0.24.202
10.0.24.203
10.0.24.204
10.0.24.205


余姐，除了10.0.24.105，10.0.24.200这两台已开通权限的主机外，还需要开通以下主机的所有权限（包括端口）：
测试相关主机：
	大数据：
	10.0.24.106
	10.0.24.107
	10.0.24.110
	10.0.24.111
	java+前端：
	10.0.40.112
	10.0.40.113
	10.0.40.114
	10.0.40.115
	10.0.40.116

uat相关主机：
	大数据+java+前端：
	10.0.24.200
	10.0.24.201
	10.0.24.202
	10.0.24.203
	10.0.24.204
	10.0.24.205
	
-、phoenix索引相关操作指南

1、创建（覆盖索引）
CREATE INDEX INDEX_1_DWS_MANAGEMENT_OUTPUTVALUE_DETAIL
ON DW.DWS_MANAGEMENT_OUTPUTVALUE_DETAIL (VOUCHERDATE,DIM) INCLUDE
(BG,ZBG,WEEK,OUTPUT_VALUE,"YEAR","MONTH","DATE")

2、删除
DROP INDEX INDEX_1_DWS_MATERIAL_NAME  ON DW.DWS_MATERIAL_NAME

3、查看是否生效
explain select material_name from DW.DWS_CD_BY_MATERIAL_MONTH




70、测试 lyzk02  rs挂掉

zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect

nohup sh tart.sh 2020-02-23 true >> start.log 2>&1 &


hive空的情况：
C如果null参与算术运算，则该算术表达式的值为null。（例如：+，-，*，/ 加减乘除）

C如果null参与比较运算，则结果可视为false。（例如：>=,<=,<>  大于，小于，不等于）

C如果null参与聚集运算，则聚集函数都置为null。除count(*)之外。

--hive 	not in 坑
	1、buyer not in('123',null)   如果在not in子查询中有null值的时候,则不会返回数据
	2、buyer not in ('123','456') 中buyer字段中为null的都不会被查出来。
	
	示例：
supplier_name
supplier_code
first_level
buyer


72.1、hbase  ，  lyzk02，lyzk03  ,Connection reset by peer ,导致写入hbaes数据出错。
		-、参考资料：https://www.aboutyun.com/thread-8522-1-1.html
		-、因为zk的连接数设置，导致hbase报错
		https://www.cnblogs.com/yjt1993/p/11640458.html
			netstat -anp|grep 2181|grep -i '10.0.24.107'|grep ESTABLISHED|wc -l
			netstat -anl|grep 2181|grep -i '10.0.24.110'|grep ESTABLISHED|wc -l
			netstat -anl|grep 2181|grep -i '10.0.24.111'|grep ESTABLISHED|wc -l
			
	2、zookeeper 报错 ：
	2020-05-10 16:40:37,087 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@193] - Too many connections from /10.0.24.111 - max is 60

	3、 netstat -tunlp
		netstat -anp
		参考资料：https://www.cnblogs.com/xieshengsen/p/6618993.html
					 https://blog.csdn.net/u011552404/article/details/51130936/  （输出内容解析）
72 、 错误汇总
-、dbeaver 遇到hive
SQL 错误 [2] [08S01]: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 9, vertexId=vertex_1582689205045_0116_1_00, diagnostics=[Vertex vertex_1582689205045_0116_1_00 [Map 9] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: dwb_purchase_combine_mid_report_eas_distinct_new3 initializer failed, vertex=vertex_1582689205045_0116_1_00 [Map 9], java.lang.RuntimeException: serious problem
解决：在hive的命令行执行（会提示详细错误）
Caused by: java.util.concurrent.ExecutionException: java.io.FileNotFoundException: File hdfs://mycluster/apps/hive/warehouse/dwmiddle.db/dwb_purchase_combine_mid_report_eas_distinct_new3/day=2019-01-01 does not exist.
提示分区的文件不存在。因为hdfs文件删除了，分区却没删除。
	-- ALTER table dwmiddle.dwb_purchase_report_cd  drop PARTITION(day='2019-01-01')

-、 MR报错 （yarn log日志）

Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. 
(Nodes: current=[DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK], 
DatanodeInfoWithStorage[10.0.24.111:50010,DS-ac94f2f3-efe7-40ad-be78-8e70aa9bfe81,DISK]], 
original=[DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK], 
DatanodeInfoWithStorage[10.0.24.111:50010,DS-ac94f2f3-efe7-40ad-be78-8e70aa9bfe81,DISK]]).
The current failed datanode replacement policy is DEFAULT,
and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
解决：
原因：无法写入；我的环境中有3个datanode，备份数量设置的是3。在写操作时，它会在pipeline中写3个机器。默认replace-datanode-on-failure.policy是DEFAULT,如果系统中的datanode大于等于3，它会找另外一个datanode来拷贝。目前机器只有3台，因此只要一台datanode出问题，就一直无法写入成功。
解决办法：修改hdfs-site.xml文件，添加或者修改如下两项：
<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
  <value>true</value> 
</property>
<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
  <value>NEVER</value>
</property>
对于dfs.client.block.write.replace-datanode-on-failure.enable，客户端在写失败的时候，是否使用更换策略，默认是true没有问题。
对于，dfs.client.block.write.replace-datanode-on-failure.policy，default在3个或以上备份的时候，是会尝试更换结点尝试写入datanode。而在两个备份的时候，不更换datanode，直接开始写。对于3个datanode的集群，只要一个节点没响应写入就会出问题，所以可以关掉。

	-、2020-7-22 补充 
		2020-07-22 16:54:45,899 WARN  [ResponseProcessor for block BP-518231600-10.0.24.105-1567588688245:blk_1074387799_668752] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-518231600-10.0.24.105-1567588688245:blk_1074387799_668752
		java.io.IOException: Bad response ERROR for block BP-518231600-10.0.24.105-1567588688245:blk_1074387799_668752 from datanode DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK]
				at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:785)
		...
		2020-07-22 16:54:46,719 WARN  [ResponseProcessor for block BP-518231600-10.0.24.105-1567588688245:blk_1074387482_668433] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-518231600-10.0.24.105-1567588688245:blk_1074387482_668433
		java.io.IOException: Bad response ERROR for block BP-518231600-10.0.24.105-1567588688245:blk_1074387482_668433 from datanode DatanodeInfoWithStorage[10.0.24.107:50010,DS-904753da-9e15-43f6-8bb9-741df16bba75,DISK]
	
	解析: 提示 block在datanode上错误
	
	-、RS重启-> hbase hbase.ZNodeClearer: Can't write znode file
		解决：zookeeper异常。
	
-、MR  日志：Failed to connect to server: lymaster01/10.0.24.105:8030: retries get failed due to exceeded maximum allowed retries number: 0
解决:（HA模式下会出现的问题，貌似不影响）
当 MR ApplicationMaster在master机器上启动时，MR程序跑得很好。
当 MR ApplicationMaster在slave机器上启动时,MR程序僵住。
资料：https://bbs.csdn.net/topics/390787380
https://blog.csdn.net/zhouyuanlinli/article/details/81772100


-、 yarn tez分配资源异常，任务阻塞	
App total resource memory: 69632 cpu: 17 taskAllocations: 0
Session timed out, lastDAGCompletionTime=1583249346390 ms, sessionTimeoutInterval=600000 ms
73、
测试：
	-、启动ambari-server ，ambari-agent。大数据服务
	
	-、 https://blog.csdn.net/cp_panda_5/article/details/79993057
1、修改vim /etc/locale.conf
LANG="en_US.UTF-8"
LANGUAGE="en_US:en"

LANG="zh_CN.UTF-8" --> 	LANG="en_US.UTF-8"
	
2、ambari server 日志：
Received fatal alert: unknown_ca
Request https://10.0.24.110:8440/ca doesn't match any pattern.
This request is not allowed on this port: https://10.0.24.110:8440/ca

3、、ambari agent 日志 提示 ： SSLError: Failed to connect. Please check openssl library versions
参考资料：https://www.jianshu.com/p/ac3d045e1423  
 sslerror  -> 升级ssl
 
[root@lyzk02 ~]# rpm -qa | grep openssl
openssl-libs-1.0.2k-16.el7_6.1.x86_64
openssl-1.0.2k-16.el7_6.1.x86_64

[root@lymaster02 ~]# rpm -qa | grep openssl
openssl-libs-1.0.2k-19.el7.x86_64
openssl-1.0.2k-19.el7.x86_64

4、EOF occurred in violation of protocol

资料：https://community.cloudera.com/t5/Support-Questions/Ambari-automatic-registration-failed-Step-3-Confirm-Hosts/m-p/186334

uat：
	-、启动ambari-server ，ambari-agent。大数据服务 ，
	-、uatmaster02  开启crontab服务。
	
################################################################# >>>> 手动启动集群begin-start  <<<<<#########################################################
手动启动集群：
1、zookeeper启动
runuser -l zookeeper -c '/usr/hdp/2.5.3.0-37/zookeeper/bin/zkServer.sh start'
测试： 启动
/usr/hdp/2.5.3.0-37/zookeeper/bin/zkServer.sh start
2、namenode启动命令：(lymaster01 ,02)
runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start namenode'

2.1 启动zkfc
runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start zkfc'

3、DN和HN启动(lyzk01 ,02,03)
JN 启动：runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start journalnode' 
DN：	runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start datanode'

4、RS启动(lymaster01 ,02)
runuser -l yarn -c '/usr/hdp/2.5.3.0-37/hadoop-yarn/sbin/yarn-daemon.sh start resourcemanager'
4.1、启动yarn的历史服务器(lymaster02)
runuser -l mapred -c  '/usr/hdp/2.5.3.0-37/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh start historyserver'
【org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer】
【http://lymaster02:19888/jobhistory】 【貌似两个RS，主机只能启动一个History】

vim /usr/hdp/2.5.3.0-37/hadoop/conf/yarn-site.xml
手动修改NM的计算内存：yarn.nodemanager.resource.memory-mb=49152

4.2 启动 timelineserver

runuser -l yarn -c 'ulimit -c unlimited export HADOOP_LIBEXEC_DIR=/usr/hdp/current/hadoop-client/libexec && /usr/hdp/current/hadoop-yarn-timelineserver/sbin/yarn-daemon.sh --config /usr/hdp/current/hadoop-client/conf start timelineserver'
【org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer】
5、 NM启动：
 runuser -l yarn -c '/usr/hdp/2.5.3.0-37/hadoop-yarn/sbin/yarn-daemon.sh start nodemanager'
 
6、hivemetastore和hiveserver2启动

---- hivemetastore 启动
后台启动：
lymaster02: 
su hive
hive --service metastore -hiveconf hive.log.file=hivemetastore.log -hiveconf hive.log.dir=/opt/hive-manal &


---- hiveserver2 启动
后台启动：
su hive 
nohup hive --service hiveserver2 &

日志查看：/home/hive/nohup.out
hiveserver2操作日志：
/tmp/hive/hive.log

7、hbase启动 （lymaster02）
HMaster：
启动：
	runuser -l hbase -c '/usr/hdp/2.5.3.0-37/hbase/bin/hbase-daemon.sh start master'

RegionServer：
	runuser -l hbase -c '/usr/hdp/2.5.3.0-37/hbase/bin/hbase-daemon.sh start regionserver'
8、lyzk03  nginx启动
启动:/usr/local/nginx/sbin/nginx
 
	备注： 正式 是prodata01
 
 
 测试环境监控ui：
	Hbase监控：http://lymaster02:16010/master-status
	HDFS：http://lymaster01:50070/dfshealth.html#tab-overview
	YARN:http://lymaster01:8088/cluster/apps/RUNNING
 
 ################################################################# 手动启动集群 end#########################################################
 关闭顺序：
	1、先关闭Hbase ，  Yarn ， NameNode ,  Hive
 
 
dbeaner工具,question：
 -- jdbc驱动错误的原因，可能是  写入的时候，文件权限报错
 
 
 ==============================================================================================================
 ==============================================================================================================
《《《《《 2020-05-13 测试服务lyzk03运维备份重启， 错误排查日志》》》》》》》》》
1、lyzk02 日期异常 导致 zookeeper启动失败。 ->  发现ntpd被关闭。
yum install ntp
systemctl is-enabled ntpd
systemctl enable ntpd
systemctl start ntpd
2、启动namenode报错。
[root@lymaster02 ~]# runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start namenode'
mkdir: cannot create directory ‘/var/run/hadoop’: Permission denied
starting namenode, logging to /var/log/hadoop/hdfs/hadoop-hdfs-namenode-lymaster02.out
/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh: line 171: /var/run/hadoop/hdfs/hadoop-hdfs-namenode.pid: No such file or directory
解决： 需要手动启动 zkfc
 case $command in ( namenode|secondarynamenode|datanode|journalnode|dfs|dfsadmin|fsck|balancer|zkfc|portmap|nfs3)
  

 =========================================================================================================================================================================================================================================================================================
79、hive uat进去cli模式报错：Failed to connect to server: uatmaster01/10.0.24.204:8032: retries get failed due to exceeded maximum allowed retries number: 0
解决：Your standby RM (rm1) must be the first RM in the configured list of RMs. So its tried first and that results in exceptions.
-- 切换RS主备后，依然不能进入hiveCli,发现是因为spark2和spark的任务，在yarn中阻塞，删除任务，可以进入hiveCli且可以执行MR。
（yarn application -kill application_1583193148938_0008）
-- 见63、《uat大数据搭建》

80、 mapred-site.xml 配置解读（yarn任务报java heap space）
	见49、
	参考资料：https://blog.csdn.net/aijiudu/article/details/72353510

	-、设置MR
        set mapreduce.map.child.java.opts=\"-Xmx1638m\"

        set mapreduce.map.memory.mb=2048

        set mapreduce.reduce.child.java.opts=\"-Xmx3276m\"

        set mapreduce.reduce.memory.mb=4096

        set yarn.app.mapreduce.am.command-opts=\"-Xmx1638m\"

        set yarn.app.mapreduce.am.resource.mb=2048


	-、设置 hive Client heap size = 4096

mapreduce---Memory调优
(1)yarn.app.mapreduce.am.resource.mb
MR AppMaster需要的内存，默认是1536M
(2)yarn.app.mapreduce.am.command-opts
MR AppMaster的Java opts ，默认是-Xmx1024m
(3)mapreduce.map.memory.mb
每个map task所需要的内存，默认是1024M。应该是大于或者等于Container的最小内存
(4)mapreduce.reduce.memory.mb
每个reduce task所需要的内存，默认是1024M
(5)mapreduce.map.java.opts
map task进程的java.opts，默认是-Xmx200m
(6)mapreduce.reduce.java.opts
reduce task进程的java.opts，默认是-Xmx200m

特别注意:
mapreduce.map.memory.mb >mapreduce.map.java.opts
mapreduce.reduce.memory.mb >mapreduce.reduce.java.opts
mapreduce.map.java.opts / mapreduce.map.memory.mb
=0.70~0.80
mapreduce.reduce.java.opts / mapreduce.reduce.memory.mb
=0.70~0.80
在yarn container这种模式下，JVM进程跑在container中，mapreduce.{map|reduce}.java.opts 能够通过Xmx设置JVM最大的heap的使用，
一般设置为0.75倍的memory.mb，

则预留些空间会存储java,scala code等

作者：吃货大米饭
链接：https://www.jianshu.com/p/898582f88ca3
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

81、 手动启动测试环境服务后， lyzk01的hbase regionserver无法启动原因解读：

《lyzk01-regionserver.log》：
2020-03-03 12:45:33,351 WARN  [regionserver/lyzk01/10.0.24.107:16020] hbase.ZNodeClearer: Can't write znode file /var/run/hbase/hbase-hbase-regionserver.znode
java.io.FileNotFoundException: /var/run/hbase/hbase-hbase-regionserver.znode (No such file or directory)
...
2020-03-03 12:45:39,165 ERROR [RS_OPEN_REGION-lyzk01:16020-0] index.Indexer: Must be too early a version of HBase. Disabled coprocessor
java.lang.NoSuchMethodError: org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry.newCounter(Ljava/lang/String
Ljava/lang/String
J)Lorg/apache/hadoop/metrics2/lib/MutableCounterLong

...
2020-03-03 12:45:39,280 ERROR [RS_OPEN_REGION-lyzk01:16020-1] coprocessor.CoprocessorHost: The coprocessor org.apache.phoenix.hbase.index.Indexer threw org.apache.hadoop.metrics2.MetricsException: Metrics source RegionServer,sub=PhoenixIndexer already exists!
...
2020-03-03 12:45:39,281 FATAL [RS_OPEN_REGION-lyzk01:16020-1] regionserver.HRegionServer: ABORTING region server lyzk01,16020,1583210730822: The coprocessor org.apache.phoenix.hbase.index.Indexer threw org.apache.hadoop.metrics2.MetricsException: Metrics source RegionServer,sub=PhoenixIndexer already exists!
org.apache.hadoop.metrics2.MetricsException: Metrics source RegionServer,sub=PhoenixIndexer already exists!
...
2020-03-03 12:45:39,376 ERROR [RS_OPEN_REGION-lyzk01:16020-2] coprocessor.CoprocessorHost: The coprocessor org.apache.phoenix.hbase.index.Indexer threw org.apache.hadoop.metrics2.MetricsException: Metrics source RegionServer,sub=PhoenixIndexer already exists!
...
2020-03-03 12:45:39,843 ERROR [RS_OPEN_REGION-lyzk01:16020-0] handler.OpenRegionHandler: Failed open of region=DW:DWB_COMPANY,,1572329931792.ead2190f6ebe2ab707aaa6c5a31bfcc6., starting to roll back the global memstore size.
java.io.IOException: Cannot append
 log is closed
...
2020-03-03 12:45:40,252 ERROR [RS_OPEN_PRIORITY_REGION-lyzk01:16020-1] handler.OpenRegionHandler: Failed open of region=DW:INDEX_1_DWS_MANAGEMENT_OUTPUTVALUE_BY_PROJECT_DETAIL,2019-09\x0011\x005746086b0ec53c1ca74be5676873dbbeec45464,1582795086444.49a1952955a0ad7192c8707f5614012b., starting to roll back the global memstore size.
java.io.IOException: Cannot append
 log is closed

解决: 
	替换 hbase中的phoenix的jar包为4.9版本。
ln -s /opt/apache-phoenix-4.9.0-HBase-1.1-bin/phoenix-4.9.0-HBase-1.1-server.jar  phoenix-server.jar


---、修改hive表结构
alter table eas.ods_eas_new_eas75_ct_ls_match
set serde  'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'
 with  serdeproperties  ('field.delim'='^#')


-- 、了解HBase详解
https://www.cnblogs.com/jstarseven/p/11425071.html
81、hdfs数据迁移

	1、-- hdfs文件集群互迁
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_vw_km_salerpt_month
hdfs://10.0.24.205:8020/apps/hive/warehouse/dwbase.db/dwb_vw_km_salerpt_month

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwmiddle.db/dwm_moqie_supplier hdfs://10.0.24.205:8020/apps/hive/warehouse/dwmiddle.db/dwm_moqie_supplier
	2、如果目标表是分区表，需要先构建分区，在msck
		alter table dwbase.dwb_vw_km_salerpt_month add PARTITION(day='2020-03-15')
		alter table dwmiddle.dwm_dim_cd_by_other add PARTITION(day='2020-03-25',type='13')
	3、MSCK REPAIR TABLE 库名.表名
(不需要分区的一种）
	
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_purchase_by_weiqi    hdfs://10.0.24.204:8020/apps/hive/warehouse/dwbase.db/dwb_purchase_by_weiqi
	
hadoop distcp -skipcrccheck -update hdfs://mycluster/warehouse/eas/ods/ods_eas_new_eas75_t_pm_user/day=2020-04-22    hdfs://10.0.24.204:8020/apps/hive/warehouse/eas.db/ods_eas_new_eas75_t_pm_user/day=2020-05-13

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_vw_km_salerpt_month/day=2020-05-13    hdfs://10.0.24.204:8020/apps/hive/warehouse/dwbase.db/dwb_vw_km_salerpt_month/day=2020-05-13

hadoop distcp -skipcrccheck -update hdfs://mycluster/warehouse/eas/ods/ODS_EAS_NEW_EAS75_T_SM_SUPPLYINFO  hdfs://10.0.24.204:8020/apps/hive/warehouse/eas.db/ods_eas_new_eas75_t_sm_supplyinfo


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g  hdfs://10.0.24.204:8020/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g

MSCK REPAIR TABLE dwbase.dwb_purchase_report_by_5g


-- uat的cd表数据同步至测试环境。
1、hadoop distcp -skipcrccheck -update hdfs://uatmycluster/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd/day=2020-06-27   hdfs://10.0.24.105:8020/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd/day=2020-06-27
2、alter table dwmiddle.dwb_purchase_report_cd add PARTITION(day='2020-06-27')
3、MSCK REPAIR TABLE dwmiddle.dwb_purchase_report_cd 


-- uat的cd表数据同步至正式环境。
1、hadoop distcp -skipcrccheck -update hdfs://uatmycluster/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd/day=2020-06-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd/day=2020-06-27
2、alter table dwmiddle.dwb_purchase_report_cd add PARTITION(day='2020-06-27')
3、MSCK REPAIR TABLE dwmiddle.dwb_purchase_report_cd 


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwmiddle.db/dwm_dim_cd_by_other/day=2020-06-30   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwmiddle.db/dwm_dim_cd_by_other/day=2020-06-30
alter table dwmiddle.dwm_dim_cd_by_other add PARTITION(day='2020-06-27',type='12')
alter table dwmiddle.dwm_dim_cd_by_other add PARTITION(day='2020-06-27',type='13')
MSCK REPAIR TABLE dwmiddle.dwm_dim_cd_by_other


hadoop distcp -skipcrccheck -update hdfs://uatmycluster/apps/hive/warehouse/dwbase.db/dwb_sap_cicai_by_cd/day=2020-06-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwbase.db/dwb_sap_cicai_by_cd/day=2020-06-27
alter table dwbase.dwb_sap_cicai_by_cd add PARTITION(day='2020-06-27')
MSCK REPAIR TABLE  dwbase.dwb_sap_cicai_by_cd 

hadoop distcp -skipcrccheck -update hdfs://uatmycluster/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd/day=2020-06-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd/day=2020-06-27
alter table dwbase.dwb_sap_jiegoujian_by_cd add PARTITION(day='2020-06-27')
MSCK REPAIR TABLE  dwbase.dwb_sap_jiegoujian_by_cd 

hadoop distcp -skipcrccheck -update hdfs://uatmycluster/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g/day=2020-06-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g/day=2020-06-27
alter table dwbase.dwb_purchase_report_by_5g add PARTITION(day='2020-06-27')
MSCK REPAIR TABLE  dwbase.dwb_purchase_report_by_5g 

hadoop distcp -skipcrccheck -update hdfs://uatmycluster/apps/hive/warehouse/dwdetail.db/dwd_salcomp_cd_detail/day=2020-06-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwdetail.db/dwd_salcomp_cd_detail/day=2020-06-27
alter table dwdetail.dwd_salcomp_cd_detail add PARTITION(day='2020-06-27')
MSCK REPAIR TABLE dwdetail.dwd_salcomp_cd_detail

-- 测试的供应商对账，导入pro 
=================================================================================================================
==========================================测试同步正式 begin ====================================================
=================================================================================================================
=================================================================================================================
=================================================================================================================

-- 供应商对账原始表

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_recon_rate_month_update/day=2022-06-12_sb  hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_recon_rate_month_update/day=2022-06-12_sb

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_buyer_recon_rate_month_update/day=2022-06-12_sb hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_buyer_recon_rate_month_update/day=2022-06-12_sb

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_month_update/day=2022-06-12_sb   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_month_update/day=2022-06-12_sb   
  


-- 供应商达成率， BG排名二级详情

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_detail_month_update/day=2022-06-12_sb     hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_detail_month_update/day=2022-06-12_sb  

-- 供应商相关的采购员表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_buyer_update/day=2022-06-12_sb    hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_buyer_update/day=2022-06-12_sb


-- 模具系统 单独的二级详情表 ，达成率表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_muju_recon_rate_month_update/day=2021-05-18     hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_muju_recon_rate_month_update/day=2021-05-18

hadoop distcp -skipcrccheck -update  hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_muju_supplier_recon_rate_detail_month_update/day=2021-05-18    hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_muju_supplier_recon_rate_detail_month_update/day=2021-05-18



-- <<<<<< cd_deltete 表   ！！！！！！！！！！！！！！！>>>>>

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_delete/day=2021-04-27_sb hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_cd_delete/day=2021-04-27_sb

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_delete/day=2021-04-27    hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_cd_delete/day=2021-04-27




-- detail表 eas -update

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd_update/day=2021-06-21 hdfs://10.0.24.114:8020/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd_update/day=2021-06-21
msck REPAIR table dwmiddle.dwb_purchase_report_cd_update



hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g/day=2021-02-25 hdfs://10.0.24.113:8020/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g/day=2021-02-25 
msck REPAIR table   dwbase.dwb_purchase_report_by_5g


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwdetail.db/dwd_salcomp_cd_detail_update/day=2021-01-27     hdfs://10.0.24.113:8020/apps/hive/warehouse/dwdetail.db/dwd_salcomp_cd_detail_update/day=2021-01-27   
msck REPAIR table dwdetail.dwd_salcomp_cd_detail_update
	
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwdetail.db/dwd_huadongzidonghua_cd_detail_update/day=2021-02-24       hdfs://10.0.24.113:8020/apps/hive/warehouse/dwdetail.db/dwd_huadongzidonghua_cd_detail_update/day=2021-02-24 
msck REPAIR table dwdetail.dwd_huadongzidonghua_cd_detail_update

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_sap_cicai_by_cd_update/day=2021-02-24       hdfs://10.0.24.113:8020/apps/hive/warehouse/dwbase.db/dwb_sap_cicai_by_cd_update/day=2021-02-24 
msck REPAIR table dwbase.dwb_sap_cicai_by_cd_update

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd_update/day=2021-02-24    hdfs://10.0.24.113:8020/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd_update/day=2021-02-24 
msck REPAIR table dwbase.dwb_sap_jiegoujian_by_cd_update



:%s/10.0.24.114/g /10.0.24.111/g 
-- 维度表 -update  （参见 lymaster01(/root/shell/hive/purchase_bi_shell/distcp/zhengshi) -> ceshi_to_zhengshi_dimenson.sh）

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_material_cut_price_month_update/day=2020-10-03   hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_cut_price_month_update/day=2020-10-03
msck REPAIR table dwservice.dws_cd_by_material_cut_price_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_cut_price_month_update/day=2020-08-01


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_material_type_month_update/day=2020-10-03   hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_type_month_update/day=2020-10-03  
msck REPAIR table dwservice.dws_cd_by_material_type_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_type_month_update/day=2020-08-01 


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_material_month_update/day=2020-10-03   hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_month_update/day=2020-10-03  
msck REPAIR table dwservice.dws_cd_by_material_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_month_update/day=2020-08-01 


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_month_update/day=2020-10-03    hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_month_update/day=2020-10-03    
msck REPAIR table dwservice.dws_cd_by_supplier_month_update
hadoop fs -rm -r -skipTrash  hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_month_update/day=2020-08-01  


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_amount_month_update/day=2020-10-31 hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_amount_month_update/day=2020-10-31 
msck REPAIR table dwservice.dws_cd_by_amount_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_amount_month_update/day=2020-08-01 

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_area_day_month_update/day=2020-10-03    hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_area_day_month_update/day=2020-10-03   
msck REPAIR table dwservice.dws_cd_by_area_day_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_area_day_month_update/day=2020-10-03   


-- 额外维护表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_area_company_update/day=2021-04-27  hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_area_company_update/day=2021-04-27  
msck REPAIR table dwservice.dws_area_company_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_area_company_update/day=2020-08-01 

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_area_update/day=2021-04-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_area_update/day=2021-04-27 
msck REPAIR table dwservice.dws_area_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_area_update/day=2020-08-01  

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_material_name_update/day=2021-04-27   hdfs://10.0.24.113:8020/apps/hive/warehouse/dwservice.db/dws_material_name_update/day=2021-04-27  
msck REPAIR table dwservice.dws_material_name_update
hadoop fs -rm -r -skipTrash  hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_material_name_update/day=2020-08-01  


-- 维护物料未降价表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_no_price_reduce_material_update/day=2020-10-03  hdfs://10.0.24.114:8020/apps/hive/warehouse/dwservice.db/dws_no_price_reduce_material_update/day=2020-10-03
msck REPAIR table dwservice.dws_no_price_reduce_material






=================================================================================================================
=========================================测试同步正式 end =====================================================
========================================================================================>========================
========================================================================================>========================
========================================================================================>========================




****************************************************************************************************************
==========================================测试同步uat begin ====================================================
脚本目录：测试目录：lymaster01 (/root/shell/hive/purchase_bi_shell/distcp)
		   uat目录：uatdata01  (/root/shell/hive/purchase_bi_shell/distcp)
****************************************************************************************************************
****************************************************************************************************************


-- 供应商对账原始表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_recon_rate_month_update/day=2020-10-03_nomuju   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_recon_rate_month_update/day=2020-10-03_nomuju

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_buyer_recon_rate_month_update/day=2020-10-03_nomuju hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_buyer_recon_rate_month_update/day=2020-10-03_nomuju

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_month_update/day=2020-10-03_nomuju   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_month_update/day=2020-10-03_nomuju  
  


-- 供应商达成率， BG排名二级详情

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_detail_month_update/day=2020-10-03_nomuju    hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_recon_rate_detail_month_update/day=2020-10-03_nomuju 

-- 供应商相关的采购员表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_buyer_update/day=2020-10-03_nomuju   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_buyer_update/day=2020-10-03_nomuju

-- cd_deltete 表

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_delete/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_delete/day=2020-10-03 

-- detail表 eas -update  （先同步 2020-09-18 的数据，再同步2020-10-03的数据））

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd_update/day=2020-10-03 hdfs://10.0.24.205:8020/apps/hive/warehouse/dwmiddle.db/dwb_purchase_report_cd_update/day=2020-10-03
msck REPAIR table dwmiddle.dwb_purchase_report_cd_update


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g/day=2020-10-03 hdfs://10.0.24.205:8020/apps/hive/warehouse/dwbase.db/dwb_purchase_report_by_5g/day=2020-10-03
msck REPAIR table   dwbase.dwb_purchase_report_by_5g

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwdetail.db/dwd_salcomp_cd_detail_update/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwdetail.db/dwd_salcomp_cd_detail_update/day=2020-10-03 
msck REPAIR table dwdetail.dwd_salcomp_cd_detail_update
-- 2020-09-18
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwdetail.db/dwd_huadongzidonghua_cd_detail_update/day=2020-10-03    hdfs://10.0.24.205:8020/apps/hive/warehouse/dwdetail.db/dwd_huadongzidonghua_cd_detail_update/day=2020-10-03 
msck REPAIR table dwdetail.dwd_huadongzidonghua_cd_detail_update

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_sap_cicai_by_cd_update/day=2020-10-03    hdfs://10.0.24.205:8020/apps/hive/warehouse/dwbase.db/dwb_sap_cicai_by_cd_update/day=2020-10-03  
msck REPAIR table dwbase.dwb_sap_cicai_by_cd_update

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd_update/day=2020-10-03  hdfs://10.0.24.205:8020/apps/hive/warehouse/dwbase.db/dwb_sap_jiegoujian_by_cd_update/day=2020-10-03 
msck REPAIR table dwbase.dwb_sap_jiegoujian_by_cd_update



-- 维度表 -update

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_material_cut_price_month_update/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_cut_price_month_update/day=2020-10-03
msck REPAIR table dwservice.dws_cd_by_material_cut_price_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_cut_price_month_update/day=2020-08-01


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_material_type_month_update/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_type_month_update/day=2020-10-03  
msck REPAIR table dwservice.dws_cd_by_material_type_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_type_month_update/day=2020-08-01 


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_material_month_update/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_month_update/day=2020-10-03  
msck REPAIR table dwservice.dws_cd_by_material_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_material_month_update/day=2020-08-01 


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_month_update/day=2020-10-03    hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_month_update/day=2020-10-03    
msck REPAIR table dwservice.dws_cd_by_supplier_month_update
hadoop fs -rm -r -skipTrash  hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_supplier_month_update/day=2020-08-01  


hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_amount_month_update/day=2020-10-03 hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_amount_month_update/day=2020-10-03 
msck REPAIR table dwservice.dws_cd_by_amount_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_amount_month_update/day=2020-08-01 

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_cd_by_area_day_month_update/day=2020-10-03    hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_area_day_month_update/day=2020-10-03   
msck REPAIR table dwservice.dws_cd_by_area_day_month_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_cd_by_area_day_month_update/day=2020-10-03   


-- 额外维护表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_area_company_update/day=2020-10-03  hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_area_company_update/day=2020-10-03  
msck REPAIR table dwservice.dws_area_company_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_area_company_update/day=2020-08-01 

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_area_update/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_area_update/day=2020-10-03 
msck REPAIR table dwservice.dws_area_update
hadoop fs -rm -r -skipTrash hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_area_update/day=2020-08-01  

hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_material_name_update/day=2020-10-03   hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_material_name_update/day=2020-10-03  
msck REPAIR table dwservice.dws_material_name_update
hadoop fs -rm -r -skipTrash  hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_material_name_update/day=2020-08-01  


-- 维护物料未降价表
hadoop distcp -skipcrccheck -update hdfs://mycluster/apps/hive/warehouse/dwservice.db/dws_no_price_reduce_material_update/day=2020-09-18  hdfs://10.0.24.205:8020/apps/hive/warehouse/dwservice.db/dws_no_price_reduce_material_update/day=2020-09-18
msck REPAIR table dwservice.dws_no_price_reduce_material




****************************************************************************************************************
==========================================测试同步uat END ====================================================
****************************************************************************************************************
****************************************************************************************************************



-- SAP相关表复制覆盖至正式环境
hadoop distcp -skipcrccheck -update hdfs://mycluster/warehouse/sap/ods/ods_sap_zfmm_044_jishou  hdfs://10.0.24.113:8020/apps/hive/warehouse/sap.db/ods_sap_zfmm_044_jishou


MSCK REPAIR TABLE eas.ods_eas_new_eas75_t_pm_user

MSCK REPAIR TABLE dwbase.dwb_vw_km_salerpt_month

MSCK REPAIR TABLE dwbase.dwb_purchase_by_weiqi

MSCK REPAIR TABLE eas.ODS_EAS_NEW_EAS75_T_SM_SUPPLYINFO


select day,count(1) from eas.ods_eas_new_eas75_t_pm_user group by day
  -- 2020-04-22	7432
select day,count(1) from  dwbase.dwb_vw_km_salerpt_month group by day
 --  2020-05-13	223
select day,count(1) from  dwbase.dwb_purchase_by_weiqi  group by day
  --  2020-05-13	73
select day,count(1) from eas.ODS_EAS_NEW_EAS75_T_SM_SUPPLYINFO group by day
 -- 2020-05-10	465929

82、本地导入maven jar包拷贝到yarn的lib目录下
. 输入mvn命令（进入jar包所在地址）

? ??mvn install:install-file -Dfile=D:\ly\firefox\phoenix-4.9.0-HBase-1.1-client-CS.jar? -DgroupId=org.apache.phoenix -DartifactId=phoenix-client -Dversion=4.9.0-HBase-1.1?-Dpackaging=jar

命令解释：-Dfile=本地jar包路径? ?-DgroupId=查看的groupId? ? -DartifactId=查看的artifactId? ??-Dversion=查看的版本号?

-Dpackaging=jar

我们需要确认的是-Dfile，-DgroupId，-DartifactId，-Dversion，-Dpackaging都是准确无误的
――――――――――――――――
原文链接：https://blog.csdn.net/steven_sisi/java/article/details/81774632


mvn install:install-file  -Dfile=D:\ly\firefox\mvn_repo\mvn_repo\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar    -DgroupId=org.scala-lang -DartifactId=scala-library -Dversion=2.11.8?-Dpackaging=jar


83、执行jar包

nohup java -jar ly_dw_etl.jar > ly_dw_etl.log  2>&1 &

84、uat环境执行 spark on yarn  提示 ： unkonw hosts:mycluster
   日志详情：
   -、reson
   20/04/13 11:16:03 INFO Client: Uploading resource file:/opt/ly_spark/project_files/ly_dw_etl/spark_extrajars/phoenix-4.9.0-HBase-1.1-client.jar -> hdfs://uatmycluster/user/admin.ly/.sparkStaging/application_1585578186028_0249/phoenix-4.9.0-HBase-1.1-client.jar

	20/04/13 11:17:02 WARN TaskSetManager: Lost task 1.1 in stage 2.0 (TID 5, uatdata04): java.sql.SQLException: ERROR 726 (43M10):  Inconsistent namespace mapping properites.. Cannot initiate connection as SYSTEM:CATALOG is found but client does not have phoenix.schema.isNamespaceMappingEnabled enabled

	20/04/13 11:17:02 ERROR dwm_bg_to_dim$: com.ly.bigdata.phoenix.common.dwm_bg_to_dim 执行失败，原因：Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 9, uatdata04): java.sql.SQLException: ERROR 726 (43M10):  Inconsistent namespace mapping properites.. Cannot initiate connection as SYSTEM:CATALOG is found but client does not have phoenix.schema.isNamespaceMappingEnabled enabled
  -、因为替换了hbase-site.xml ，导致少了mapping设置（参见同文档61）
  
 85、spark2   测试提交任务 ，设置jdk版本
 su admin.ly
../../bin/spark-submit  --master yarn --deploy-mode client --conf "spark.executorEnv.JAVA_HOME=/home/admin.ly/jdk" --conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/admin.ly/jdk"  --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.0.0.2.5.3.0-37.jar

../../bin/spark-submit  --master yarn --deploy-mode client --conf "spark.executorEnv.JAVA_HOME=/opt/jdk1.8.0_221" --conf "spark.yarn.appMasterEnv.JAVA_HOME=/opt/jdk1.8.0_221"  --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.0.0.2.5.3.0-37.jar

../../bin/spark-submit  --master yarn --deploy-mode client --conf "spark.executorEnv.JAVA_HOME=/home/admin.ly" --conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/admin.ly"  --class org.apache.spark.examples.SparkPi spark-examples_2.11-2.0.0.2.5.3.0-37.jar

 -- 、工程里面测试， 使用
	--conf "spark.executorEnv.JAVA_HOME=/home/admin.ly/jdk" --conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/admin.ly/jdk"
	
	
--conf "spark.executorEnv.JAVA_HOME=/home/admin.ly/jdk" \
--conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/admin.ly/jdk" \


  ->  测试 修改 admin.ly  的环境变量 =jdk8
  
   cp -r /opt/jdk1.8.0_221  /home/admin.ly/
   mv /home/admin.ly/jdk1.8.0_221  jdk
   chown admin.ly:admin.ly -R /home/admin.ly/jdk
   chmod  755  /home/admin.ly
   
   
export JAVA_HOME=/home/admin.ly/jdk
export PATH=$JAVA_HOME/bin:$PATH

86、uat -> phoenix查询，插入异常总结。
1-、phoenix 查询报错 ：
org.apache.hadoop.hbase.DoNotRetryIOException: DW:INDEX_1_MATERIAL_CUT_PRICE,2019-08\x003\x00NON-STICK.LINER.O.D11\x00\xC0\x03HE\x002019\x0008\x006710246887b144c2c268a879cc37d1fcb14e2b8,1587985149804.3b2b490f692b86d72af218fb8384b71e.: null

原因：调用了round函数，但是字段中存在null值，故报错。

https://www.codeleading.com/article/4387227159

2-、大批量查询phoenix error: Unable to find cached index metadata
	解决：change hbase-site.xml
		phoenix.coprocessor.maxServerCacheTimeToLiveMs=180000
		phoenix.coprocessor.maxMetaDataCacheTimeToLiveMs=1800000
3-、hbase regionServer  报错：
	2020-05-27 13:11:10,685 WARN  [B.fifo.QRpcServer.handler=12,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"processingtimems":75514,"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","client":"10.0.24.205:48602","starttimems":1590556195171,"queuetimems":0,"class":"HRegionServer","responsesize":8765,"method":"Scan"}
2020-05-27 13:12:05,190 WARN  [Finalizer] memory.GlobalMemoryManager: Orphaned chunk of 494116 bytes found during finalize  (在完成过程中发现的孤立块494116字节)
  -- Orphaned 美[???rfnd] v.	使成为孤儿

  --  chunk美[t???k]n.	厚块
 厚片
 大块
 相当大的量
 话语组成部分
 组块


解决：ScanRequest -> responseTooSlow 由此可知是因为查询导致.
explain SELECT supplier_name,SUM(CD_AMOUNT) AS  cd_amount_all , SUM(cd_amount)/sum(total_amount) AS cd_proportion_all,sum(total_amount) AS  total_amount_all  FROM DW.DWS_CD_BY_SUPPLIER_MONTH WHERE "DATE"  BETWEEN '2020-01'  AND '2020-05'  AND dim='10'   AND TOTAL_AMOUNT IS NOT NULL AND TOTAL_AMOUNT !=0  GROUP BY supplier_name  ORDER BY cd_amount_all DESC       limit 10 

在uat执行查询解释，发现 
|CLIENT 1-CHUNK 0 ROWS 0 BYTES PARALLEL 1-WAY FULL SCAN OVER DW:DWS_CD_BY_SUP  |
|     SERVER FILTER BY (("DATE" >= '2020-01' AND "DATE" <= '2020-05') AND DIM  |
|     SERVER AGGREGATE INTO DISTINCT ROWS BY [SUPPLIER_NAME]                   |
| CLIENT MERGE SORT                                                            |
| CLIENT TOP 10 ROWS SORTED BY [SUM(CD_AMOUNT) DESC]
在测试查询解释发现，发现
 CLIENT 1-CHUNK 0 ROWS 0 BYTES PARALLEL 1-WAY SKIP SCAN ON 1 RANGE OVER DW:IN  |
|     SERVER FILTER BY (true AND "TOTAL_AMOUNT" IS NOT NULL AND "TOTAL_AMOUNT" |
|     SERVER AGGREGATE INTO DISTINCT ROWS BY ["SUPPLIER_NAME"]                 |
| CLIENT MERGE SORT                                                            |
| CLIENT TOP 10 ROWS SORTED BY [SUM("CD_AMOUNT") DESC]   

由此可知是因为uat没有supplier索引导致，full scan 导致查询超时。


最新结论: Orphaned [???rfnd] chunk of 309116 bytes found during finalize [?fa?n?la?z]   --> 孤立块 ... 定案期间
有可能是 hbase正在进行compaction，导致相应变慢。

番外知识点：
1、compaction相关
	1-、参数调优：
		hbase.hregion.majorcompaction=7  
			（一个region的所有HStoreFile进行major compact的时间周期，默认是604800000 毫秒（7天）；）
			hbase.hregion.majorcompaction.jitter=0.5  
				(major compaction的发生抖动范围，这么理解比较容易，就是说上一个参数不是一个严格周期，会有个抖动，这个参数就是这个抖动的比例，默认是0.5；)
			hbase.hstore.compactionThreshold=3
				(一个HStore存储HStoreFile的个数阈值，超过这个阈值则所有的HStoreFile会被写到一个新的HStore，需要平衡取舍，默认是3；)
		hbase.hstore.blockingStoreFiles=10
			(一个HStore存储HStoreFile阻塞update的阈值，超过这个阈值，HStore就进行compaction，直到做完才允许update，默认是10；)	
			hbase.hstore.blockingWaitTime：
				(一个更强力的配置，配合上一个参数，当HStore阻塞update时，超过这个时间限制，阻塞取消，就算compaction没有完成，update也不会再被阻塞，默认是90000毫秒；)
		hbase.hstore.compaction.max=3  
			(每个minor compaction的HStoreFile个数上限，默认是10；		)
		
	2-、日常使用：	
		-、使用以下命令手动触发major compaction
			major_compact "table_name"
		-、一般建议设置hbase.hregion.majorcompaction设为0来禁用该功能，
			由于执行期间会对整个集群的磁盘和带宽带来较大影响，并在夜间集群负载较低时通过定时任务脚本来执行。
			（大合并，就是将一个Region下的所有StoreFile合并成一个StoreFile文件，在大合并的过程中，之前删除的行和过期的版本都会被删除。大合并一般一周做一次）
	

参考资料：https://blog.csdn.net/co_zjw/article/details/106896674
https://blog.csdn.net/Post_Yuan/article/details/72865038?locationNum=4&fps=1
https://blog.csdn.net/hsg77/article/details/88734764			
https://www.jianshu.com/p/a296bbdc8d5f （脚本手动执行major compaction）
https://juejin.im/post/5c013a2051882516fa63a536 (详细介绍 Major Compaction 的核心作用原理)
https://juejin.im/post/5ed6fae9f265da76cd47f0ea （终于读懂了hbase）
http://hbasefly.com/2016/07/13/hbase-compaction-1/ （compaction 源码以及策略分析）
2、WAL更新 
	日志：
		2020-07-03 16:44:32,997 INFO  [HBase-Metrics2-1] impl.MetricsSystemImpl: HBase metrics system started
		2020-07-03 16:46:30,918 INFO  [regionserver/prodata04/10.0.24.119:16020.logRoller] wal.FSHLog: Rolled WAL /apps/hbase/data/WALs/prodata04,16020,1592803461998/prodata04%2C16020%2C1592803461998.default.1593762390505 with entries=3, filesize=1.04 KB
 new WAL /apps/hbase/data/WALs/prodata04,16020,1592803461998/prodata04%2C16020%2C1592803461998.default.1593765990832
		2020-07-03 16:46:30,919 INFO  [regionserver/prodata04/10.0.24.119:16020.logRoller] wal.FSHLog: Archiving hdfs://promycluster/apps/hbase/data/WALs/prodata04,16020,1592803461998/prodata04%2C16020%2C1592803461998.default.1593758790303 to hdfs://promycluster/apps/hbase/data/oldWALs/prodata04%2C16020%2C1592803461998.default.1593758790303
		2020-07-03 16:46:30,934 INFO  [regionserver/prodata04/10.0.24.119:16020.logRoller] wal.FSHLog: Archiving hdfs://promycluster/apps/hbase/data/WALs/prodata04,16020,1592803461998/prodata04%2C16020%2C1592803461998.default.1593762390505 to hdfs://promycluster/apps/hbase/data/oldWALs/prodata04%2C16020%2C1592803461998.default.1593762390505
		2020-07-03 16:49:21,826 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=19.86 MB, freeSize=7.82 GB, max=7.84 GB, blockCount=123, accesses=13432874, hits=13432874, hitRatio=100.00%, , cachingAccesses=13432744, cachingHits=13432744, cachingHitsRatio=100.00%, evictions=96267, evicted=325, evictedPerRun=0.0033760271035134792
		2020-07-03 16:49:22,656 INFO  [BucketCacheStatsExecutor] bucket.BucketCache: failedBlockAdditions=794, totalSize=2.00 GB, freeSize=1.22 GB, usedSize=803.40 MB, cacheSize=516.16 MB, accesses=6646766, hits=5715206, IOhitsPerSecond=0, IOTimePerHit=NaN, hitRatio=85.98%, cachingAccesses=5793530, cachingHits=5635061, cachingHitsRatio=97.26%, evictions=2, evicted=132338, evictedPerRun=66169.0
		2020-07-03 16:49:32,476 INFO  [HBase-Metrics2-1] impl.MetricsSystemImpl: Stopping HBase metrics system...
	解析：
	    每间隔hbase.regionserver.optionallogflushinterval(默认1s)， HBase会把操作从内存写入WAL。
	    WAL的检查间隔由hbase.regionserver.logroll.period定义，默认值为1小时（当前集群是40mins）。
		检查的内容是把当前WAL中的操作跟实际持久化到HDFS上的操作比较，看哪些操作已经被持久化
	    了，被持久化的操作就会被移动到.oldlogs文件夹内（这个文件夹也是在HDFS上的）。一个WAL实例包含有多个WAL文件。
			WAL文件的最大数量通过hbase.regionserver.maxlogs（默认  是32）参数来定义。
原文链接：https://blog.csdn.net/bitbitbyte/article/details/105559145

4-、phoenix, 查询supplier表，时快时慢的问题  [2020-06-01 处理]
	1-、观察hbase UI -> user Tables -> online Regions( uatdata01)。  
		查询缓慢时  online Regions = 1
	
	备注：查看2020-05-28 日志 ：
	2020-05-28 17:13:52,617 WARN  [ResponseProcessor for block BP-1176785730-10.0.24.204-1578048587874:blk_1073889851_149782] hdfs.DFSClient: Slow ReadProcessor read fields took 64430ms (threshold=30000ms)
 ack: seqno: -2 reply: SUCCESS reply: SUCCESS reply: ERROR downstreamAckTimeNanos: 0 flag: 0 flag: 0 flag: 1, targets: [DatanodeInfoWithStorage[10.0.24.200:50010,DS-1cc26f65-5725-451b-b870-2d0b2bbceda3,DISK], DatanodeInfoWithStorage[10.0.24.203:50010,DS-e2e600aa-749a-4b9f-adc9-33d737fad021,DISK], DatanodeInfoWithStorage[10.0.24.201:50010,DS-f69ba388-32d4-4511-be4a-e3c688a2ec58,DISK]]
	...
	2020-05-28 17:13:52,619 WARN  [ResponseProcessor for block BP-1176785730-10.0.24.204-1578048587874:blk_1073889851_149782] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1176785730-10.0.24.204-1578048587874:blk_1073889851_149782
java.io.IOException: Bad response ERROR for block BP-1176785730-10.0.24.204-1578048587874:blk_1073889851_149782 from datanode DatanodeInfoWithStorage[10.0.24.201:50010,DS-f69ba388-32d4-4511-be4a-e3c688a2ec58,DISK]
	2020-05-28 17:13:52,619 WARN  [DataStreamer for file /apps/hbase/data/WALs/uatdata01,16020,1590363046072/uatdata01%2C16020%2C1590363046072..meta.1590654678026.meta block BP-1176785730-10.0.24.204-1578048587874:blk_1073889851_149782] hdfs.DFSClient: Error Recovery for block BP-1176785730-10.0.24.204-1578048587874:blk_1073889851_149782 in pipeline DatanodeInfoWithStorage[10.0.24.200:50010,DS-1cc26f65-5725-451b-b870-2d0b2bbceda3,DISK], DatanodeInfoWithStorage[10.0.24.203:50010,DS-e2e600aa-749a-4b9f-adc9-33d737fad021,DISK], DatanodeInfoWithStorage[10.0.24.201:50010,DS-f69ba388-32d4-4511-be4a-e3c688a2ec58,DISK]: bad datanode DatanodeInfoWithStorage[10.0.24.201:50010,DS-f69ba388-32d4-4511-be4a-e3c688a2ec58,DISK]
	
	2-、观察hbase UI -> user Tables -> online Regions ( uatdata03)。    [2020-06-05]
		查询缓慢时  online Regions = 1
		
		分析：是因为region在uatdata01导致查询缓慢？切换至uatdata03查询正常？
		
		
5、phoenix插入表 eas2时，耗时严重。查看yarn日志，如下：
20/06/28 18:27:26 INFO MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
20/06/28 18:27:26 INFO MetricsSystemImpl: phoenix metrics system started
20/06/28 18:27:44 INFO AsyncProcess: #1, waiting for 10000  actions to finish
20/06/28 18:27:44 INFO AsyncProcess: #1, waiting for 10000  actions to finish
20/06/28 18:27:54 INFO AsyncProcess: #1, waiting for 6914  actions to finish
20/06/28 18:27:54 INFO AsyncProcess: #1, waiting for 6916  actions to finish
20/06/28 18:28:04 INFO AsyncProcess: #1, waiting for 3668  actions to finish
20/06/28 18:28:04 INFO AsyncProcess: #1, waiting for 3744  actions to finish
20/06/28 18:28:14 INFO AsyncProcess: #1, waiting for 3744  actions to finish

问题分析：出现这个问题是由于数据插入在服务端没有执行完成，客户端正在等待服务端插入完成。其实问题的本质在于服务端，可能插入太快，该表在spit 或者flush 或者gc stw 等问题引起，需要优化该表在服务端的存储，	 具体的表优化就看大家自己实际的业务情况了。

参考资料：https://blog.csdn.net/xiefu5hh/article/details/53056113?locationNum=4&fps=1

87、测试环境  -> reginserver 宕机 -> datanode -> datanode nio连接异常
lyzk02 ,lyzk01-> hregionserver
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK], DatanodeInfoWithStorage[10.0.24.107:50010,DS-904753da-9e15-43f6-8bb9-741df16bba75,DISK]], original=[DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK], DatanodeInfoWithStorage[10.0.24.107:50010,DS-904753da-9e15-43f6-8bb9-741df16bba75,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
Caused by:org.apache.hadoop.hbase.regionserver.wal.FailedSyncBeforeLogCloseException: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: On sync


lyzk02 datanode -> log
2020-05-27 11:17:28,891 INFO  web.DatanodeHttpServer (SimpleHttpProxyHandler.java:exceptionCaught(147)) - Proxy for / failed. cause:
java.io.IOException: Connection reset by peer
...
2020-05-27 11:31:40,688 ERROR datanode.DataNode (DataXceiver.java:run(278)) - lyzk02:50010:DataXceiver error processing WRITE_BLOCK operation  src: /10.0.24.111:58302 dst: /10.0.24.110:50010
java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.0.24.110:50010 remote=/10.0.24.111:58302]. 60000 millis timeout left.
...
2020-05-27 11:32:04,214 WARN  datanode.DataNode (BlockReceiver.java:run(1397)) - IOException in BlockReceiver.run():
java.io.IOException: Connection reset by peer
...
2020-05-27 11:32:11,276 ERROR datanode.DataNode (DataXceiver.java:run(278)) - lyzk02:50010:DataXceiver error processing WRITE_BLOCK operation  src: /10.0.24.107:50925 dst: /10.0.24.110:50010
java.io.IOException: Premature EOF from inputStream
...
2020-05-27 11:33:09,308 WARN  datanode.DataNode (BlockReceiver.java:run(1353)) - The downstream error might be due to congestion in upstream including this node. Propagating the error:
java.io.EOFException: Premature EOF: no length prefix available
...
2020-05-27 11:41:16,378 WARN  datanode.DataNode (BlockReceiver.java:run(1397)) - IOException in BlockReceiver.run():
java.io.IOException: Broken pipe

解决：怀疑是运维动了网线。导致pipe broke  / lyzk03莫名关机？
	   --> 参考 2926行《MR报错 （yarn log日志）》
88、2020-06-09 执行hive脚本 tez报错:
2020-06-09 01:33:34,454 [WARN] [main] |ipc.Client|: Failed to connect to server: uatmaster01/10.0.24.204:8020: try once and fail.
java.net.ConnectException: Connection refused
...
2020-06-09 01:33:37,527 [INFO] [main] |ipc.Client|: Retrying connect to server: uatmaster01/10.0.24.204:8020. Already tried 0 time(s)
 retry policy is RetryPolicy[MultipleLinearRandomRetry[500x2000ms], TryOnceThenFail]
2020-06-09 01:33:39,816 [INFO] [main] |ipc.Client|: Retrying connect to server: uatmaster01/10.0.24.204:8020. Already tried 1 time(s)
 retry policy is RetryPolicy[MultipleLinearRandomRetry[500x2000ms], TryOnceThenFail]

问题查找：
	1、发现，204主机，namenode日志,因为主备切换导致宕机。
2020-06-08 18:36:24,492 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(388)) - Remote journal 10.0.24.201:8485 failed to write txns 10076174-10076174. Will try to write to this JN again after the next log roll.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): IPC's epoch 304 is less than the last promised epoch 305
...
20-06-08 18:36:26,069 WARN  client.QuorumJournalManager (QuorumOutputStream.java:abort(72)) - Aborting QuorumOutputStream starting at txid 10075896
2020-06-08 18:36:26,073 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1
2020-06-08 18:36:26,080 INFO  namenode.NameNode (LogAdapter.java:info(47)) - SHUTDOWN_MSG:
/************************************************************SHUTDOWN_MSG: Shutting down NameNode at uatmaster01/10.0.24.204************************************************************/
2020-06-09 08:42:38,457 INFO  namenode.NameNode (LogAdapter.java:info(47)) - STARTUP_MSG:/************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG:   user = hdfs
STARTUP_MSG:   host = uatmaster01/10.0.24.204STARTUP_MSG:   args = []STARTUP_MSG:   version = 2.7.3.2.5.3.0-37	
	
如何解决？：可以在core-site.xml文件中修改ha.health-monitor.rpc-timeout.ms参数值，来扩大zkfc监控检查超时时间。
　　可以在core-site.xml文件中修改ha.health-monitor.rpc-timeout.ms参数值，来扩大zkfc监控检查超时时间。
<property>
<name>ha.health-monitor.rpc-timeout.ms</name>
<value>180000</value>
</property>
参考资料：https://www.cnblogs.com/lixiaolun/p/6485401.html
引申问题？ ： 为什么YARN提交的任务在其中一个NN宕机，不尝试连接另一台NN？
	
	2020-09-21 ，遇到同样的问题
	->MR任务执行
	-> 其中一个job（MAp,reduce）等待许久提交不了任务 
	->  查看lymaster02的yarn日志（提示timeline尝试重连） 
	->  手动启动timeline
	-> 提示连接不上NN1（- Retrying connect to server: lymaster01/10.0.24.105:8020）  
	-> 查看NN1日志
2020-09-21 13:55:04,989 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(388)) - Remote journal 10.0.24.111:8485 failedto write txns 17016511-17016511. Will try to write to this JN again after the next log roll.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): IPC's epoch 19 is less than the last promised epoch 20
		at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:428)
...
2020-09-21 13:55:04,990 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(406)) - Took 4929ms to send a batch of 1 edits(17 bytes) to remote journal 10.0.24.111:8485
2020-09-21 13:55:04,991 FATAL namenode.FSEditLog (JournalSet.java:mapJournalsAndReportErrors(398)) - Error: flush failed for required journal (JournalAndStream(mgr=QJM to [10.0.24.107:8485, 10.0.24.110:8485, 10.0.24.111:8485], stream=QuorumOutputStream starting at txid 17016511))
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:
10.0.24.111:8485: IPC's epoch 19 is less than the last promised epoch 20）
	   at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:428)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkWriteRequest(Journal.java:456)
        at org.apache.hadoop
	
88、附加：
	测试换将，跑MR，回收任务结果卡顿，后续发现，其中一个namenode挂掉，导致回收结果搁置， 发现错误也是 “ IPC's epoch ...”。
	
	
89、hive任务提交至yarn，运行失败：
TezSession has already shutdown. 	
2020-06-14 06:50:20,579 [INFO] [main] |ipc.Client|: Retrying connect to server: lymaster01/10.0.24.105:8020. Already tried 0 time(s)
 retry policy is RetryPolicy[MultipleLinearRandomRetry[500x2000ms], TryOnceThenFail]

解决：因为一台master挂掉，yarn不知道为什么一直提示连接挂掉的主机？
?????????????	


--、 hbase 物料降价查询缓慢   (18:57重启宕机namenode[standBY])

http://uatdata03:16030/dump(/var/log/hbase/hbase-hbase-regionserver-uatdata03.log:

2020-06-17 18:52:42,475 INFO  [B.fifo.QRpcServer.handler=0,queue=0,port=16020] hdfs.DFSClient: Access token was invalid when connecting to /10.0.24.202:50010 : org.apache.hadoop.security.token.SecretManager$InvalidToken: access control error while attempting to setup short-circuit access to /apps/hbase/data/data/DW/DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH/7390263d0987057ba27da7b0c97983a2/0/367acce9ec904db6a63ece57a7847dcb
2020-06-17 18:52:46,059 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3019msGC pool 'ParNew' had collection(s): count=1 time=3033ms
2020-06-17 18:52:50,266 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3706msGC pool 'ParNew' had collection(s): count=1 time=3710ms
...
2020-06-17 20:42:48,636 INFO  [split-log-closeStream-1] wal.WALSplitter: Rename hdfs://uatmycluster/apps/hbase/data/data/DW/INDEX_1_MATERIAL_CUT_PRICE/3b2b490f692b86d72af218fb8384b71e/recovered.edits/0000000000000135537.temp to hdfs://uatmycluster/apps/hbase/data/data/DW/INDEX_1_MATERIAL_CUT_PRICE/3b2b490f692b86d72af218fb8384b71e/recovered.edits/00000000000001380002020-06-17 20:42:48,639 INFO  [split-log-closeStream-2] wal.WALSplitter: Rename hdfs://uatmycluster/apps/hbase/data/data/DW/INDEX_1_MATERIAL_CUT_PRICE/6178a27ecb4fddf187ee5f6633f7e8bf/recovered.edits/0000000000000184841.temp to hdfs://uatmycluster/apps/hbase/data/data/DW/INDEX_1_MATERIAL_CUT_PRICE/6178a27ecb4fddf187ee5f6633f7e8bf/recovered.edits/0000000000000186475
2020-06-17 20:42:48,640 INFO  [split-log-closeStream-3] wal.WALSplitter: Rename hdfs://uatmycluster/apps/hbase/data/data/DW/DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH/bf002dd0a24a5db36111cb1cf9f1dbd3/recovered.edits/0000000000000063863.temp to hdfs://uatmycluster/apps/hbase/data/data/DW/ient.read.shortcircuit.streams.cache.size 0000000000000065477
.....
2020-06-17 18:52:42,474 INFO  [B.fifo.QRpcServer.handler=29,queue=2,port=16020] hdfs.DFSClient: Access token was invalid when connecting to /10.0.24.202:50010 : org.apache.hadoop.security.token.SecretManager$InvalidToken: access control error while attempting to set up short-circuit access to /apps/hbase/data/data/DW/DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH/7390263d0987057ba27da7b0c97983a2/0/367acce9ec904db6a63ece57a7847dcb
2020-06-17 18:52:42,474 INFO  [B.fifo.QRpcServer.handler=0,queue=0,port=16020] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x187c04a2): could not get 1073923373_BP-1176785730-10.0.24.204-1578048587874 due to InvalidToken exception.org.apache.hadoop.security.token.SecretManager$InvalidToken: access control error while attempting to set up short-circuit access to/apps/hbase/data/data/DW/DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH/7390263d0987057ba27da7b0c97983a2/0/367acce9ec904db6a63ece57a7847dcb


http://uatdata01:16030/dump(/var/log/hbase/hbase-hbase-regionserver-uatdata01.log) ： ---->  Found a failed index update!
2020-06-17 20:03:44,815 ERROR [IndexRpcServer.handler=8,queue=0,port=16020] parallel.BaseTaskRunner: Found a failed task because: org.apache.hadoop.hbase.ipc.CallerDisconnectedException: Aborting on region DW:DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH,54997527fa81227592477ddbad47df70ec69694,1587924110151.bf002dd0a24a5db36111cb1cf9f1dbd3., call org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl@5c71de38 after 4369 ms, since caller disconnected
...
Caused by: org.apache.hadoop.hbase.ipc.CallerDisconnectedException: Aborting on region DW:DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH,54997527fa81227592477ddbad47df70ec69694,1587924110151.bf002dd0a24a5db36111cb1cf9f1dbd3., call org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl@5c71de38 after 4369 ms, since caller disconnected
...
2020-06-17 20:03:44,818 ERROR [IndexRpcServer.handler=8,queue=0,port=16020] builder.IndexBuildManager: Found a failed index update!


--、hbase表 物料降价 表查询没数据（元数据，表数据都有）
2020-06-18 08:55:16,206 INFO  [B.fifo.QRpcServer.handler=3,queue=0,port=16020] hdfs.DFSClient: Access token was invalid when connecting to /10.0.24.201:50010 : org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error, status message , for OP_READ_BLOCK, self=/10.0.24.202:50803, remote=/10.0.24.201:50010, for file /apps/hbase/data/data/DW/DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH/bf002dd0a24a5db36111cb1cf9f1dbd3/0/3f3734d2725f4cc4a76b1688654969b5, for pool BP-1176785730-10.0.24.204-1578048587874 block 1073933067_193958



--、namenode宕机日记 （uatmaster01）
2020-06-17 11:53:23,563 WARN  client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 17013 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.0.24.202:8485]
2020-06-17 11:53:24,563 WARN  client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 18014 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.0.24.202:8485]
2020-06-17 11:53:25,565 WARN  client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 19015 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.0.24.202:8485]
2020-06-17 11:53:26,550 FATAL namenode.FSEditLog (JournalSet.java:mapJournalsAndReportErrors(398)) - Error: flush failed for required journal (JournalAndStream(mgr=QJM to [10.0.24.201:8485, 10.0.24.202:8485, 10.0.24.200:8485], stream=QuorumOutputStream starting at txid 10760685))
java.io.IOException: Timed out waiting 20000ms for a quorum of nodes to respond.

-- journalNode（uatdata03） 日志
2020-06-17 11:31:49,765 INFO  common.Storage (JNStorage.java:purgeMatching(170)) - Purging no-longer needed file 9728694
2020-06-17 11:31:49,770 INFO  common.Storage (JNStorage.java:purgeMatching(170)) - Purging no-longer needed file 9663739
2020-06-17 11:31:49,770 INFO  common.Storage (JNStorage.java:purgeMatching(170)) - Purging no-longer needed file 9663827
...
...
2020-06-17 11:31:49,905 INFO  common.Storage (JNStorage.java:purgeMatching(170)) - Purging no-longer needed file 9675677
2020-06-17 11:33:14,508 INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(142)) - Finalizing edits file /hadoop/hdfs/journal/uatmycluster/current/edits_inprogress_0000000000010759865 -> /hadoop/hdfs/journal/uatmycluster/current/edits_0000000000010759865-0000000000010759950


此步骤解决：
可选优化方法（我配置了第1项）：
1）调节journalnode 的写入超时时间 dfs.qjournal.write-txns.timeout.ms参数

其实在实际的生产环境中，也很容易发生类似的这种超时情况，所以我们需要把默认的20s超时改成更大的值，比如60或者90s。

我们可以在hadoop/etc/hadoop下的hdfs-site.xml中，加入一组配置:
<property>
 <name>dfs.qjournal.write-txns.timeout.ms</name>
 <value>60000</value>
</property>

从别人博客中看到的配置方法，神奇的是，hadoop的官网中的关于hdfs-site.xml介绍中，居然找不到关于这个配置的说明

http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml


2）调整namenode 的java参数，提前触发 full gc，这样full gc 的时间就会小一些。

3）默认namenode的fullgc方式是parallel gc，是stw模式的，更改为cms的格式。调整namenode的启动参数：
-XX:+UseCompressedOops
-XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled
-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0
-XX:+CMSParallelRemarkEnabled -XX:+DisableExplicitGC
-XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75
-XX:SoftRefLRUPolicyMSPerMB=0
――――――――――――――――
版权声明：本文为CSDN博主「levy_cui」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/levy_cui/article/details/51143214

90、新搭建服务器配置
登录用户
普通用户：bigdata/Tlg.data2020
超级用户：root/Tlg.data2020@

-、uat  主机间传递命令，
for i in `seq 2 9`
do ssh 10.0.24.11$i "date" 
done

for i in `seq 2 9`
do ssh 10.0.24.11$i "hostname" 
done


for i in `seq 2 9`
do ssh 10.0.24.11$i "systemctl status ntpd" 
done

for i in `seq 2 9`
do ssh 10.0.24.11$i "systemctl is-enabled ntpd" 
done

	-、ntpd重启，自动启动失效，因为chronyd冲突，disable此服务chronyd即可。
	for i in `seq 2 9`
do ssh 10.0.24.11$i "systemctl disable chronyd" 
done

	for i in `seq 2 9`
do ssh 10.0.24.11$i "systemctl is-enabled chronyd" 
done


for i in `seq 2 9`
do ssh 10.0.24.11$i "cat /etc/sysconfig/network" 
done

for i in `seq 2 9`
do ssh 10.0.24.11$i "systemctl status firewalld" 
done



for i in `seq 2 9`
do ssh 10.0.24.11$i "setenforce 0" 
done

for i in `seq 2 9`
do ssh 10.0.24.11$i "sestatus" 
done

for i in `seq 2 9`
do ssh 10.0.24.11$i "cat /etc/sysconfig/selinux" 
done

  
  
for i in `seq 2 9`
do ssh 10.0.24.11$i "cat /etc/sysctl.conf | grep ipv6" 
done

 
for i in `seq 2 9`
do ssh 10.0.24.11$i "echo $JAVA_HOME" 
done
 
/usr/local/java/jdk1.8.0_73


for i in `seq 2 9`
do ssh 10.0.24.11$i "cat /etc/yum.repos.d/" 
done
 
-、host配置
10.0.24.112     prodata01
10.0.24.113     promaster01
10.0.24.114     promaster02
10.0.24.115     proweb01
10.0.24.116     proweb02
10.0.24.117     prodata02       proambari
10.0.24.118     prodata03       promysql01
10.0.24.119     prodata04       promysql02	

-、全域名
prodata01
promaster01
promaster02
proweb01
proweb02
prodata02
prodata03
prodata04

-、mysql账户

root/lymysql123
lymysql/lymysql
ambari/Ambari-123	
hive/Hive-123


Accumulo root password ： Accumulo-123
Instance Secret：Instance-123
(Metrics )Grafana Admin Password：Metrics-123
(SmartSense)Password for user 'admin' : SmartSense-123
-、other 操作	
source /opt/Ambari-DDL-MySQL-CREATE.sql

grant all privileges on *.* to 'root'@'%' identified by 'lymysql123' with grant option


vim  /etc/ambari-agent/conf/ambari-agent.ini

-、修改 yum 仓库，主目录 -> /etc/yum.repos.d/
	-、修改 基础仓库 基础数据源  --->
			mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo-backup
			vim /etc/yum.repos.d/CentOS-Base.repo
			/etc/yum.repos.d/CentOS-Base.repo 文件内容如下：

# CentOS-Base.repo
#
# The mirror system uses the connecting IP address of the client and the
# update status of each mirror to pick mirrors that are updated to and
# geographically close to the client.  You should use this for CentOS updates
# unless you are manually picking other mirrors.
#
# If the mirrorlist= does not work for you, as a fall back you can try the
# remarked out baseurl= line instead.
#
#
[base]
name=CentOS-$releasever - Base - 163.com
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os
baseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/
gpgcheck=1
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7

#released updates
[updates]
name=CentOS-$releasever - Updates - 163.com
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates
baseurl=http://mirrors.163.com/centos/$releasever/updates/$basearch/
gpgcheck=1
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7

#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras - 163.com
#mirrorlist=http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras
baseurl=http://mirrors.163.com/centos/$releasever/extras/$basearch/
gpgcheck=1
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7

#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus - 163.com
baseurl=http://mirrors.163.com/centos/$releasever/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=http://mirrors.163.com/centos/RPM-GPG-KEY-CentOS-7
	
	-、修改ambari	
		[root@uatdata02 yum.repos.d]# cat ambari.repo
		#VERSION_NUMBER=2.4.2.0-136
		[Updates-ambari-2.4.2.0]
		name=ambari-2.4.2.0 - Updates
		baseurl=http://10.0.24.110/ambari/AMBARI-2.4.2.0/centos7/2.4.2.0-136/
		gpgcheck=1
		gpgkey=http://10.0.24.110/ambari/AMBARI-2.4.2.0/centos7/2.4.2.0-136/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
		enabled=1
		priority=1

		[root@uatdata02 yum.repos.d]# cat hdp.repo
		#VERSION_NUMBER=2.5.3.0-37
		[HDP-2.5.3.0]
		name=HDP Version - HDP-2.5.3.0
		baseurl=http://10.0.24.110/ambari/HDP/centos7/
		gpgcheck=1
		gpgkey=http://10.0.24.110/ambari/HDP/centos7/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
		enabled=1
		priority=1
		[HDP-UTILS-1.1.0.21]
		name=HDP-UTILS Version - HDP-UTILS-1.1.0.21
		baseurl=http://10.0.24.110/ambari/HDP-UTILS-1.1.0.21/
		gpgcheck=1
		gpgkey=http://10.0.24.110/ambari/HDP-UTILS-1.1.0.21/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins
		enabled=1
		priority=1
	
	-、刷新yum
	yum clean all
yum list update
yum makecache
yum repolist
yum list | grep ambari


备份：阿里云镜像：
# CentOS-Base.repo
#
# The mirror system uses the connecting IP address of the client and the
# update status of each mirror to pick mirrors that are updated to and
# geographically close to the client.  You should use this for CentOS updates
# unless you are manually picking other mirrors.
#
# If the mirrorlist= does not work for you, as a fall back you can try the 
# remarked out baseurl= line instead.
#
#
 
[base]
name=CentOS-$releasever - Base - mirrors.aliyun.com
failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/os/$basearch/
        http://mirrors.aliyuncs.com/centos/$releasever/os/$basearch/
        http://mirrors.cloud.aliyuncs.com/centos/$releasever/os/$basearch/
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7
 
#released updates 
[updates]
name=CentOS-$releasever - Updates - mirrors.aliyun.com
failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/
        http://mirrors.aliyuncs.com/centos/$releasever/updates/$basearch/
        http://mirrors.cloud.aliyuncs.com/centos/$releasever/updates/$basearch/
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7
 
#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras - mirrors.aliyun.com
failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/
        http://mirrors.aliyuncs.com/centos/$releasever/extras/$basearch/
        http://mirrors.cloud.aliyuncs.com/centos/$releasever/extras/$basearch/
gpgcheck=1
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7
 
#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus - mirrors.aliyun.com
failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/
        http://mirrors.aliyuncs.com/centos/$releasever/centosplus/$basearch/
        http://mirrors.cloud.aliyuncs.com/centos/$releasever/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7
 
#contrib - packages by Centos Users
[contrib]
name=CentOS-$releasever - Contrib - mirrors.aliyun.com
failovermethod=priority
baseurl=http://mirrors.aliyun.com/centos/$releasever/contrib/$basearch/
        http://mirrors.aliyuncs.com/centos/$releasever/contrib/$basearch/
        http://mirrors.cloud.aliyuncs.com/centos/$releasever/contrib/$basearch/
gpgcheck=1
enabled=0
gpgkey=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7



-、mysql 主从同步失败，  不能删除，创建等。
show slave status \G
	思路:	->stop slaves
			->跳过错误
				SET GLOBAL  SQL_SLAVE_SKIP_COUNTER=1
  SHOW GLOBAL VARIABLES LIKE 'SQL_SLAVE_SKIP_COUNTER'

			->start slave

			
https://www.cnblogs.com/tangpoets/p/9214275.html

-、mysql 备份所有库
mysqldump  -uroot -plymysql123 --all-databases > /root/mysql-backup/lymysql01-2021-07-19-backup.sql

-、hdfs提示->  Number of Under-Replicated Blocks

hadoop fs -setrep -R -w 3 /user/accumulo/
or
hdfs dfs -setrep [-R] [-w] 3 /user/accumulo/
上传文件时，指定副本语句：
Hadoop dfs -D dfs.replication=1 -put test.txt logs/
修改所有文件副本数为2
hadoop dfs -setrep -w 2 -R /

参考资料：
https://blog.csdn.net/xjping0794/article/details/77848001
https://blog.csdn.net/mn_kw/article/details/90017647
-、hbase修改对应参数 ，告警 
Some service configurations are not configured properly. We recommend you review and change the highlighted configuration values. Are you sure you want to proceed without correcting configurations?
Type	Service	Property	Value	Description
Warning	HBase	hbase.rpc.timeout	3600000	
Values greater than 3 Minutes are not recommended
This is for the RPC layer to define how long HBase client applications take for a remote call to time out. It uses pings to check connections but will eventually throw a TimeoutException.
Warning	HBase	phoenix.query.timeoutMs	3600000	
Values greater than 3 Minutes are not recommended
Number of milliseconds after which a Phoenix query will timeout on the client.

-、误删 journalNode元数据目录   /hadoop/hdfs/journal （测试）
方案：
情况一：只有部分journalnode报这个错，原因是这些journalnode的journal数据不同步
解决：将无错的journalnode下的journal文件夹拷贝覆盖之
情况二：由非HA转为HA
情况三：你的所有journalnode都报这个错，并且journal文件夹为空
解决：情况二和情况三一样，都是需要初始化 journalnode。那么，将所有journalnode守护进程启动后，在其中一台namenode下，执行 hdfs namenode -initializeSharedEdits
参考资料：http://www.mamicode.com/info-detail-2319099.html

-、hbase  master日志 出现 balance

2020-07-31 15:17:51,806 INFO  [lymaster02,16000,1591059159023_ChoreService_4] master.HMaster: balance hri=DW:DWS_MANAGEMENT_OUTPUTVALUE_BY_BU_DETAIL,,1585884123755.aa779aca3693b16a20e46fcd85204f76., src=lyzk03,16020,1596179424807, dest=lyzk01,16020,1596179415202
2020-07-31 15:17:51,806 INFO  [lymaster02,16000,1591059159023_ChoreService_4] master.AssignmentManager: Ignored moving region not assigned: {ENCODED => aa779aca3693b16a20e46fcd85204f76, NAME => 'DW:DWS_MANAGEMENT_OUTPUTVALUE_BY_BU_DETAIL,,1585884123755.aa779aca3693b16a20e46fcd85204f76.', STARTKEY => '', ENDKEY => ''}, {aa779aca3693b16a20e46fcd85204f76 state=OFFLINE, ts=1596179420082, server=lyzk03,16020,1596164409563}
2020-07-31 15:17:51,806 INFO  [lymaster02,16000,1591059159023_ChoreService_4] master.HMaster: balance hri=DW:DWS_NO_PRICE_REDUCE_MATERIAL,,1594955413900.2b05a6df5c7b436aa297f5e40a3ba959., src=lyzk03,16020,1596179424807, dest=lyzk02,16020,1596179420992
2020-07-31 15:17:51,806 INFO  [lymaster02,16000,1591059159023_ChoreService_4] master.AssignmentManager: Ignored moving region not assigned: {ENCODED => 2b05a6df5c7b436aa297f5e40a3ba959, NAME => 'DW:DWS_NO_PRICE_REDUCE_MATERIAL,,1594955413900.2b05a6df5c7b436aa297f5e40a3ba959.', STARTKEY => '', ENDKEY => ''}, {2b05a6df5c7b436aa297f5e40a3ba959 state=OFFLINE, ts=1596179420082, server=lyzk03,16020,1596164409563}	

-、hbase can 表，提示  "  not online regoionserver  lyzk03"

	-、 hbase hbck -details  DW:DWS_CD_BY_AREA_MONTH
		hbase hbck -details  DW:DWS_CD_BY_MATERIAL_MONTH
		
	-、查看hdfs文件块 状态（hbase 数据）
		blk_1074421627 /apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/61784bee78b34ec8967b284801261d09
		blk_1074421638 /apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/af436ddf75e34a4397b39f3bfb9dbe90

	hadoop fsck /apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/61784bee78b34ec8967b284801261d09
	hadoop fsck /apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/af436ddf75e34a4397b39f3bfb9dbe90
	
	/apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/61784bee78b34ec8967b284801261d09: CORRUPT blockpool BP-518231600-10.0.24.105-1567588688245 block blk_1074421627
	
	/apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/af436ddf75e34a4397b39f3bfb9dbe90: CORRUPT blockpool BP-518231600-10.0.24.105-1567588688245 block blk_1074421638

	参考资料：https://blog.csdn.net/u011291159/article/details/69384821



	Total size:    36846 B
	 Total dirs:    0
	 Total files:   1
	 Total symlinks:                0
	 Total blocks (validated):      1 (avg. block size 36846 B)
	  ********************************
	  UNDER MIN REPL'D BLOCKS:      1 (100.0 %)
	  dfs.namenode.replication.min: 1
	  CORRUPT FILES:        1
	  MISSING BLOCKS:       1
	  MISSING SIZE:         36846 B
	  CORRUPT BLOCKS:       1
	  ********************************
	 Minimally replicated blocks:   0 (0.0 %)
	 Over-replicated blocks:        0 (0.0 %)
	 Under-replicated blocks:       0 (0.0 %)
	 Mis-replicated blocks:         0 (0.0 %)
	 Default replication factor:    3
	 Average block replication:     0.0
	 Corrupt blocks:                1
	 Missing replicas:              0
	 Number of data-nodes:          3
	 Number of racks:               1
	FSCK ended at Fri Jul 31 17:10:19 CST 2020 in 2 milliseconds

	-- 查看文件状态
	hadoop fsck /warehouse/test/ods/a.txt

	-、列出位置
	 hdfs fsck /apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/61784bee78b34ec8967b284801261d09 -locations -blocks -files

	-、手动修复数据
	 hdfs debug recoverLease -path /apps/hbase/data/data/SYSTEM/STATS/a7897991dd165fa43971554a96477c9b/0/61784bee78b34ec8967b284801261d09 -retries 3


	参考资料：https://blog.csdn.net/lingbo229/article/details/81128316?utm_source=blogxgwz8
	https://blog.csdn.net/weixin_41158277/article/details/95041494 （手动修复数据）
	https://blog.csdn.net/qq_32641659/article/details/88243255（删除hdfs坏的文件）
	
	
-、hdfs Failing because I am unlikely to write too  ，
	could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.
解决：1、删除hdfs坏的快即可。
	  2、数据过多。
	我想我的原因可能是这个，看了下live datenode可用空间不足10%，删除了几个大的文件，空间空出来了，
	
	
	[hdfs@lymaster02 hdfs]$ hadoop fs -du -h /
578.6 M  /app-logs
150.4 G  /apps
2.4 G    /ats
0        /dolphinscheduler
0        /dolphinscheduler121
870.1 M  /hdp
0        /mapred
76.6 M   /mr-history
77.1 K   /spark-history
4.6 G    /spark2-history
151.6 M  /tmp
1.9 G    /user
147.1 G  /warehouse


	-、hbase 的master和RS启动失败
	2020-08-04 13:28:34,347 WARN  [lymaster02:16000.activeMasterManager] retry.RetryInvocationHandler: Exception while invoking ClientNamenodeProtocolTranslatorPB.delete over lymaster01/10.0.24.105:8020. Not retrying because try once and fail.
	org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.PathIsNotEmptyDirectoryException): `/apps/hbase/data/WALs/lyzk02,16020,1596416516156-splitting is non empty': Directory is not empty
	...
	2020-08-04 13:33:28,583 INFO  [509935096@qtp-749693202-2] client.RpcRetryingCaller: Call exception, tries=23, retries=350, started=309780 ms ago, cancelled=false, msg=
	2020-08-04 13:33:29,538 INFO  [1697857013@qtp-749693202-5] client.RpcRetryingCaller: Call exception, tries=11, retries=350, started=68505 ms ago, cancelled=false, msg=
	2020-08-04 13:33:32,824 INFO  [lymaster02,16000,1596518723317_splitLogManager__ChoreService_1] master.SplitLogManager: total tasks =1 unassigned = 0 tasks={/hbase-unsecure/splitWAL/WALs%2Flyzk03%2C16020%2C1595414289851-splitting%2Flyzk03%252C16020%252C1595414289851.default.1596152461439=last_update = 1596519195778 last_version = 126 cur_worker_name = lyzk03,16020,1596416564790 status = in_progress incarnation = 3 resubmits = 3 batch = installed = 1 done = 0 error = 0}
	2020-08-04 13:33:36,127 FATAL [lymaster02:16000.activeMasterManager] master.HMaster: Failed to become active master
	java.io.IOException: Timedout 300000ms waiting for namespace table to be assigned
			at org.apache.hadoop.hbase.master.TableNamespaceManager.start(TableNamespaceManager.java:104)
	2020-08-04 13:33:36,128 FATAL [lymaster02:16000.activeMasterManager] master.HMaster: Master server abort: loaded coprocessors are: [org.apache.hadoop.hbase.backup.master.BackupController]
	2020-08-04 13:33:36,128 FATAL [lymaster02:16000.activeMasterManager] master.HMaster: Unhandled exception. Starting shutdown.
	java.io.IOException: Timedout 300000ms waiting for namespace table to be assigned
	...
	2020-08-04 13:33:36,672 ERROR [lymaster02,16000,1596518723317_ChoreService_1] master.BackupLogCleaner: Failed to get hbase:backup table, therefore will keep all files
	java.io.InterruptedIOException
			at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:214)

	-、regionserver执行报错 （lyzk02）	
	
	020-08-03 09:04:36,417 WARN  [RS_LOG_REPLAY_OPS-lyzk02:16020-1] coordination.ZkSplitLogWorkerCoordination: transisition task /hbase-unsecure/splitWAL/WALs%2Flyzk03%2C16020%2C1595414289851-splitting%2Flyzk03%252C16020%252C1595414289851.default.1596152461439 to RESIGNED lyzk02,16020,1596416516156 failed because of version mismatch
	org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase-unsecure/splitWAL/WALs%2Flyzk03%2C16020%2C1595414289851-splitting%2Flyzk03%252C16020%252C1595414289851.default.1596152461439
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)	
	...
	2020-08-03 09:54:37,257 WARN  [RS_LOG_REPLAY_OPS-lyzk02:16020-1] hdfs.DFSClient: Last block locations not available. Datanodes mightnot have reported blocks completely. Will retry for 3 times
	2020-08-03 09:54:41,260 WARN  [RS_LOG_REPLAY_OPS-lyzk02:16020-1] hdfs.DFSClient: Last block locations not available. Datanodes mightnot have reported blocks completely. Will retry for 2 times
	2020-08-03 09:54:45,262 WARN  [RS_LOG_REPLAY_OPS-lyzk02:16020-1] hdfs.DFSClient: Last block locations not available. Datanodes mightnot have reported blocks completely. Will retry for 1 times
	2020-08-03 09:54:49,264 WARN  [RS_LOG_REPLAY_OPS-lyzk02:16020-1] wal.WALFactory: Lease should have recovered. This is not expected. Will retry
		java.io.IOException: Could not obtain the last block locations.
	...



	问题解析：
		1-、Last block locations not available.-> 即文件损坏或者异常（hdfs）
	解决：	
		1、删除错误的/apps/hbase/data/WALs/*  目录数据
参考资料：https://www.cnblogs.com/luckyRoyis/articles/7105808.html
	
	
	-、启动后，regionserver下线
	
		evicted files: 0
	2020-08-04 14:30:12,162 ERROR [regionserver/lyzk02/10.0.24.110:16020.logRoller] wal.FSHLog: Failed close of WAL writer hdfs://mycluster/apps/hbase/data/WALs/lyzk02,16020,1596519008023/lyzk02%2C16020%2C1596519008023.default.1596519011480, unflushedEntries=0
	java.io.FileNotFoundException: File does not exist: hdfs://mycluster/apps/hbase/data/WALs/lyzk02,16020,1596519008023/lyzk02%2C16020%2C1596519008023.default.1596519011480
			at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1427)
			at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1419)
			at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
			at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1419)
			at org.apache.hadoop.hbase.regionserver.wal.FSHLog.replaceWriter(FSHLog.java:1020)
			at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:749)
			at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:148)
			at java.lang.Thread.run(Thread.java:748)
	2020-08-04 14:30:12,164 FATAL [regionserver/lyzk02/10.0.24.110:16020.logRoller] regionserver.HRegionServer: ABORTING region server lyzk02,16020,1596519008023: Failed log close in log roller

		解决：遗留的服务，为重启，重启RS就OK
		
		
	-、待解决：The Load Balancer is not enabled which will eventually cause performance degradation in HBase as Regions will not be distributed across all RegionServers. The balancer is only expected to be disabled during rolling upgrade scenarios.
	
	
-、
用root登录
vi /etc/sysconfig/i18n
LANG="en_US.UTF-8"
保存
source /etc/sysconfig/i18n

echo $LANG

-、phoenix 创建二级索引报错

phoenix创建二级索引表 Error: java.lang.ArrayIndexOutOfBoundsException (state=08000,code=101)

：删除数据，建索引，插数据。
参考资料：http://www.hbase.group/question/12206 （）

- phoenix 
-- 插入数据，从其他表
UPSERT INTO TEST.DWS_MES_BY_MANUAL_RANK_
(PKID, CREATE_TIME, UPDATE_TIME, END_DATE, SEQ_NUM, EMP_NO, EMP_NAME, TEAM_ID, PROD_NO, YD_HOUR_QTY, "DAY")
SELECT * FROM DW.DWS_MES_BY_MANUAL_RANK 

-- 手动插入
UPSERT INTO DW.DWS_MES_BY_MQ_UDH (PKID,CREATE_TIME,UPDATE_TIME,END_DATE,EMP_NO,EMP_NAME,MACHINE_ID,MACHINE_NAME,PRO_NAME,AREA,T_ID,QTY,WORK_TIME,UPH) VALUES (
'111',NULL,NULL,'2021-07-08','10039187','邓胜华','GM11','模切机台','SZ-P3-平湖平湖4栋-4F',NULL,'2',96000.0,96000.0,9600.0);

-- 删除索引
DROP INDEX INDEX_2_MES_BY_MANUAL_RANK  ON  DW.DWS_MES_BY_MANUAL_RANK;


-、hive函数 ，sort_array() , 
	-、此种排序是字典书序， 1,12,13,2 故想实现数字排序，
	可以用左补足函数把cast(legcount as string) 改成cast(lpad(legcount,2,'0') as string)，这个时候就会按照01 02 03 ...09 10 11的顺序排序了
参考资料：https://blog.csdn.net/madaokuma/article/details/81544849


-、hive ，distribute by A,sort by B desc 

参考资料：https://blog.csdn.net/Z_Date/article/details/83987111
https://www.cnblogs.com/huxinga/p/7688376.html （图解）


-、phoenix 没有索引查询缓慢的原因（正式环境）
...
2020-08-12 15:52:35,547 INFO  [regionserver/prodata04/10.0.24.119:16020-shortCompactions-1593580391895] regionserver.HStore: Completed compaction of 3 (all) file(s) in 0 of DW:DWS_NO_PRICE_REDUCE_MATERIAL,,1594955434055.a3d202bcad50ed03c73f6173e319e7cc. into 5bbd71e0aaf64121aa3b34eaa9007f7e(size=25.4 M), total size for store is 25.4 M. This selection was in queue for 0sec, and took 5sec to execute.
2020-08-12 15:52:35,548 INFO  [regionserver/prodata04/10.0.24.119:16020-shortCompactions-1593580391895] regionserver.CompactSplitThread: Completed compaction: Request = regionName=DW:DWS_NO_PRICE_REDUCE_MATERIAL,,1594955434055.a3d202bcad50ed03c73f6173e319e7cc., storeName=0, fileCount=3, fileSize=47.5 M, priority=7, time=4492888235082479
 duration=5sec
..
<=================================   清理 lyzk02 根目录 磁盘满了===============================================>
-、清理lyzk02的根目录文件数据  du -sh /opt   
		 du -sh /opt     83G   2022-03-17
正式： 
	prodata03 ，mysql本地linux 根目录满了
	磁盘清理：
ls | xargs du -sh

1、-- 剔除根目录已挂载的目录
 ls / | grep -v 'opt\|root\|dev\|run\|sys\|home\|boot' 
2、
 ls / | grep -v 'opt\|root\|dev\|run\|sys\|home\|boot\|proc'  | awk '{ print  "/" $1$2 }' | xargs du -sh

正式环境:
	磁盘清理
	promaster02 ,根目录数据满了 /  [2021-11-29]
			20G     /var/log/hadoop
	
3、proweb02 
/usr/local/zookeeper  29G 
29G     data_dir
57M     zookeeper-3.4.5
16M     zookeeper-3.4.5.tar.gz


UAT：

 uatmater02 ，主机 /root 。20G,  特城 帆软软件
 ls /  | awk '{ print  "/" $1$2 }' | xargs du -sh

3、清理对应文件
ls | xargs du -sh 
ls | grep yarn-yarn-nodemanager-lyzk02.out.| xargs  rm -rf 


备注：
	正式环境->最近清理日期：2021-10-13
	正式环境->最近清理日期：2021-11-29
	
	测试环境->最近清理日期：2021-10-13

5.3G    hadoop
3.0G    hadoop-yarn
5.6G    hbase
6.1G    zookeeper

2021-05-01 发现是因为： 
mysql快照目录：lib的 目录满了
mysql /var/lib/mysql   39G


实际上就是清楚 /opt、/var 、/tmp下面的数据
	-、du -sh /var/log  3G   	-->可清理
		du -sh /var/lib 12G		-->不可清理
	-、du -sh /hadoop  	12G		-->不可清理（journal元数据）
	-、du -sh /usr		7.7G 	-->不可清理
	-、du -sh /opt   9G     	
	-、du -sh /hadoop/yarn/local/usercache/admin.ly/filecache  9G   --> yarn临时文件（不知可否删除）
			【yarn.nodemanager.local-dirs】

-- zookeeper 怎么产生这么多快照
dataDir=/hadoop/zookeeper




<=================================   清理 测试环境 HDFS  磁盘满了===============================================>

清理 数据
old: 
-- 删除脚本： (/opt/hdfs/generate_rm_hdfs.sh  [lymaster02])
new:
删除历史冗余数据  (脚本路径  
	-- (/home/hdfs/rm_hdfs   lymaster01) ==>  MainFunction
	
	备注： 最近清理-> 2021-10-13
			最近清理-> 2021-11-01
			最近清理-> 2021-12-07  EAS
						176.7 G  /warehouse/eas/ods (大文件)  88.43%(DFS use_rate)
						30G  /warehouse/eas/ods (大文件)  69.43%(DFS use_rate)
	备注2 ：清理 /spark2-history 日志；
		hadoop fs -rm -r -skipTrash  `hadoop fs -ls /spark2-history/* | egrep -v '*.inprogress' | awk  '{print $8}'`
		参考资料：https://blog.csdn.net/qq_42422698/article/details/118176405
	
	备注3：
		匹配“=”号前的内容：^([^=]*)=
		匹配“=”号后的内容：=.*$
		
2022-03-11 
	1、清理dwservice数据 ，102.4 G  /apps/hive/warehouse/dwservice.db
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
#!/bin/bash
##  执行方法:
##  nohup sh generate_rm_hdfs.sh ./muju.txt ./date_muju.txt  6 >> hdfs2.log 2>&1 &
##  1、使用hdfs用户操作。 su hdfs
##  2、参数1，hdfs待删除目录表，参数2，待删除日期，参数3，截取hdfs目录获取表名的下标数(以/分隔，首位为1)
##  2、生成删除hdfs文件的文本  $root_dir/delete_dir.txt
##  3、生成删除表分区语句 $root_dir/drop.sql  （数据每次都是重写）
##      -、drop_table.sql 为临时存储sql文件。
##  4、手动执行 hive -f ./drop.sql  删除对应表的分区。
table_file=$1
date_file=$2
root_dir=`pwd`
index=$3
echo $root_dir
# 清空drop.sql 内容
echo "" > $root_dir/drop.sql
# 清空 历史删除hdfs文件目录
echo "" > $root_dir/delete_dir.txt
cat $1 | while read line
do
    ##echo "inner1->line:"$line
    file=$line
    table_name="eas."`echo $line |awk -F '/' '{print $'${index}'}'`
    #echo "inner1->table_name:"$table_name
    drop_sql=""
    cat $2 | while read line
    do
        ##echo "inner2->line:"$line
        #hive -e "show databases
"
        echo "--start--删除文件 追加至 delete_dir.txt-->"$file/day=$line
        echo $file/day=$line >> $root_dir/delete_dir.txt
        #hadoop fs -rm -r  -skipTrash $file/day=$line
        echo "--end-->"$file/day=$line"--删除完毕"
        drop_sql=$drop_sql"alter table $table_name drop partition (day='$line')
"
        echo $drop_sql  > $root_dir/drop_table.sql
    done
    cat $root_dir/drop_table.sql >> $root_dir/drop.sql
done
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

-- 、2020-08-28 ： regionserver 宕机


2020-08-28 09:29:02,719 INFO  [B.fifo.QRpcServer.handler=28,queue=1,port=16020] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x6741deef): could not load 1074476485_BP-518231600-10.0.24.105-1567588688245 due to InvalidToken exception.org.apache.hadoop.security.token.SecretManager$InvalidToken: access control error while attempting to set up short-circuit access to/apps/hbase/data/data/DW/INDEX_2_MATERIAL_CUT_PRICE/fbcad0ec3f5e8d41d2581479ebd7370f/0/8eafd208e6c749c58e11ac9ffd934cde        at org.apache.hadoop.hdfs.BlockReaderFactory.requestFileDescriptors(BlockReaderFactory.java:589)
        at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:488
...
2020-08-28 09:34:42,036 WARN  [B.fifo.QRpcServer.handler=20,queue=2,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578438601,"responsesize":699,"method":"Scan","processingtimems":40675,"client":"10.0.40.114:38966","queuetimems":106,"class":"HRegionServer"}2020-08-28 09:34:42,036 WARN  [B.fifo.QRpcServer.handler=28,queue=1,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578439426,"responsesize":10503,"method":"Scan","processingtimems":42427,"client":"10.0.40.114:38966","queuetimems":1,"class":"HRegionServer"}
2020-08-28 09:34:42,091 WARN  [B.fifo.QRpcServer.handler=13,queue=1,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578440374,"responsesize":19,"method":"Scan","processingtimems":25434,"client":"10.0.40.113:34650","queuetimems":0,"class":"HRegionServer"}2020-08-28 09:34:42,351 WARN  [B.fifo.QRpcServer.handler=3,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578441832,"responsesize":107,"method":"Scan","processingtimems":37011,"client":"10.0.40.114:38966","queuetimems":0,"class":"HRegionServer"}
2020-08-28 09:34:42,733 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 13032ms
No GCs detected
2020-08-28 09:34:51,128 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 39847ms for sessionid 0x173fb4f438b0520, closing socket connection and attempting reconnect
2020-08-28 09:34:57,612 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26676ms for sessionid 0x2742464c6c300db, closing socket connection and attempting reconnect
2020-08-28 09:35:24,183 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26678ms for sessionid 0x2742464c6c30123, closing socket connection and attempting reconnect
...
2020-08-28 09:35:35,269 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26676ms for sessionid 0x2742464c6c3004c, closing socket connection and attempting reconnect
2020-08-28 09:35:36,218 WARN  [MetadataRpcServer.handler=20,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578488449,"responsesize":19,"method":"Scan","processingtimems":46820,"client":"10.0.40.114:56560","queuetimems":50,"class":"HRegionServer"}
...
2020-08-28 09:35:37,014 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk01/10.0.24.107:2181, initiating session
2020-08-28 09:35:37,014 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk01/10.0.24.107:2181, initiating session
2020-08-28 09:35:37,015 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2742464c6c3004c has expired, closing socket connection
2020-08-28 09:35:37,069 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:585)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:517)
......
2020-08-28 09:36:29,778 WARN  [ResponseProcessor for block BP-518231600-10.0.24.105-1567588688245:blk_1074479852_763240] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-518231600-10.0.24.105-1567588688245:blk_1074479852_763240
java.io.IOException: Bad response ERROR for block BP-518231600-10.0.24.105-1567588688245:blk_1074479852_763240 from datanode DatanodeInfoWithStorage[10.0.24.107:50010,DS-904753da-9e15-43f6-8bb9-741df16bba75,DISK]
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:785)
...
2020-08-28 09:36:29,781 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
...
2020-08-28 09:36:27,536 WARN  [B.priority.fifo.QRpcServer.handler=4,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578573635,"responsesize":16,"method":"Scan","processingtimems":13561,"client":"10.0.40.114:38966","queuetimems":740,"class":"HRegionServer"}
2020-08-28 09:36:27,536 WARN  [B.priority.fifo.QRpcServer.handler=2,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578572896,"responsesize":16,"method":"Scan","processingtimems":14299,"client":"10.0.40.114:38966","queuetimems":1,"class":"HRegionServer"}
2020-08-28 09:36:27,536 WARN  [B.priority.fifo.QRpcServer.handler=1,queue=1,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578572897,"responsesize":16,"method":"Scan","processingtimems":14298,"client":"10.0.40.114:38966","queuetimems":2,"class":"HRegionServer"}
2020-08-28 09:36:27,536 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk01/10.0.24.107:2181. Will not attempt to authenticate using SASL (unknown error)
2020-08-28 09:36:27,536 WARN  [B.priority.fifo.QRpcServer.handler=6,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1598578572896,"responsesize":16,"method":"Scan","processingtimems":14300,"client":"10.0.40.114:38966","queuetimems":1,"class":"HRegionServer"}
2020-08-28 09:36:51,688 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk01/10.0.24.107:2181, initiating session
2020-08-28 09:36:27,536 WARN  [B.fifo.QRpcServer.handler=13,queue=1,port=16020] ipc.RpcServer: B.fifo.QRpcServer.handler=13,queue=1,port=16020: caught: java.io.IOException: Broken pipe
        at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)
...
2020-08-28 09:36:55,383 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1598327489829-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
...
2020-08-28 09:36:55,720 FATAL [regionserver/lyzk02/10.0.24.110:16020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are:
...
2020-08-28 09:36:56,163 INFO  [regionserver/lyzk02/10.0.24.110:16020] regionserver.HRegionServer: STOPPED: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected
 currently processing lyzk02,16020,1597989859620 as dead server
        at org.apache.hadoop.hbase.master.ServerManager.checkIsDead(ServerManager.java:419)
        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:324)
        at org.apache.hadoop.hbase.master.MasterRpcServices.regionServerReport(MasterRpcServices.java:334)
        at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:8617)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2127)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:107)
        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108)
        at java.lang.Thread.run(Thread.java:748)

2020-08-28 09:36:56,163 INFO  [regionserver/lyzk02/10.0.24.110:16020] regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
...	
2020-08-28 09:37:14,416 WARN  [regionserver/lyzk02/10.0.24.110:16020] wal.ProtobufLogWriter: Failed to write trailer, non-fatal, continuing...
inuing...java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try.(Nodes: current=[DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK], DatanodeInfoWithStorage[10.0.24.111:50010,DS-ac94f2f3-efe7-40ad-be78-8e70aa9bfe81,DISK]], original=[DatanodeInfoWithStorage[10.0.24.110:50010,DS-a494c7f3-c03c-4f56-bca7-8ff90e7fcb36,DISK], DatanodeInfoWithStorage[10.0.24.111:50010,DS-ac94f2f3-efe7-40ad-be78-8e70aa9bfe81,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
     at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:947)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1021)
....
2020-08-28 09:39:21,502 WARN  [regionserver/lyzk02/10.0.24.110:16020] zookeeper.ZKUtil: regionserver:16020-0x2740e9bf78e0017, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181, baseZNode=/hbase-unsecure Unable to list children of znode /hbase-unsecure/replication/rs/lyzk02,16020,1597989859620
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/lyzk02,16020,1597989859620
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
		
		
--、2020-08-31  lymaster02  启动很多beeline

17796 org.apache.hadoop.util.RunJar /usr/hdp/2.5.3.0-37/hive/lib/hive-beeline-1.2.1000.2.5.3.0-37.jar org.apache.hive.beeline.BeeLine -u jdbc:hive2://lymaster02:10000/
transportMode=binary -e
14854 org.apache.hadoop.util.RunJar /usr/hdp/2.5.3.0-37/hive/lib/hive-beeline-1.2.1000.2.5.3.0-37.jar org.apache.hive.beeline.BeeLine -u jdbc:hive2://lymaster02:10000/
transportMode=binary -e
18183 sun.tools.jps.Jps -lm
24718 org.apache.hadoop.util.RunJar /usr/hdp/2.5.3.0-37/hive/lib/hive-beeline-1.2.1000.2.5.3.0-37.jar org.apache.hive.beeline.BeeLine -u jdbc:hive2://lymaster02:10000/
transportMode=binary -e
7827 org.apache.hadoop.util.RunJar /usr/hdp/2.5.3.0-37/hive/lib/hive-beeline-1.2.1000.2.5.3.0-37.jar org.apache.hive.beeline.BeeLine-u jdbc:hive2://lymaster02:10000/
transportMode=binary -e

进程详情；
[root@lymaster02 ~]# ps -def | grep 405
ambari-+   405   324 10 09:11 ?        00:00:02 /opt/jdk1.8.0_221/bin/java -Xmx1024m -Dhdp.version=2.5.3.0-37 -Djava.net.preferIPv4Stack=true -Dhdp.version=2.5.3.0-37 -Dhadoop.log.dir=/var/log/hadoop/ambari-qa -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.5.3.0-37/hadoop -Dhadoop.id.str=ambari-qa -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/2.5.3.0-37/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.5.3.0-37/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx1024m -Xmx1024m -Djava.util.logging.config.file=/usr/hdp/2.5.3.0-37/hive/conf/parquet-logging.properties -Dlog4j.configuration=beeline-log4j.properties -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/hdp/2.5.3.0-37/hive/lib/hive-beeline-1.2.1000.2.5.3.0-37.jar org.apache.hive.beeline.BeeLine -u jdbc:hive2://lymaster02:10000/
transportMode=binary -e


	-、HiveServer2异常，导致启动很多beeline
	jps -lm | grep beeline.BeeLine | awk -F ' ' '{print $1}'
	-、查看hiveserver2 日期， /home/hive/nohup.out
Exception in thread "HiveServer2-Handler-Pool: Thread-21492" java.lang.OutOfMemoryError: Java heap space
Exception in thread "ResponseProcessor for block BP-518231600-10.0.24.105-1567588688245:blk_1074553677_837302" java.lang.OutOfMemoryError: Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-21495" java.lang.OutOfMemoryError: Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-21508" java.lang.OutOfMemoryError: Java heap space
java.lang.OutOfMemoryError: Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-21491" java.lang.OutOfMemoryError: Java heap space
Exception in thread "LeaseRenewer:hdfs@mycluster" java.lang.OutOfMemoryError: Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-21504" java.lang.OutOfMemoryError: Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-21512" java.lang.OutOfMemoryError: Java heap space
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-44" java.lang.OutOfMemoryError: Java heap space

	1、jstat -gcutil 28858   5000 --每5秒打印一次jvm各个内存区的状态
   S0     S1     E      O      M     CCS    YGC     YGCT    FGC    FGCT     GCT
  0.00   0.00 100.00  99.98  98.01  95.88    613    7.911  7768 7247.433 7255.344
  0.00   0.00 100.00  99.98  98.01  95.88    613    7.911  7774 7252.416 7260.327
  0.00   0.00 100.00  99.98  98.01  95.88    613    7.911  7779 7256.833 7264.744
  0.00   0.00 100.00  99.98  98.01  95.88    613    7.911  7785 7262.153 7270.064
  0.00   0.00 100.00  99.98  98.01  95.88    613    7.911  7791 7267.492 7275.403
  0.00   0.00 100.00  99.99  98.01  95.88    613    7.911  7796 7271.753 7279.664
  0.00   0.00 100.00  99.99  98.01  95.88    613    7.911  7802 7277.022 7284.933
  0.00   0.00 100.00  99.99  98.01  95.88    613    7.911  7808 7282.181 7290.092
  0.00   0.00 100.00  99.96  98.01  95.88    613    7.911  7814 7287.475 7295.386
  0.00   0.00 100.00  99.96  98.01  95.88    613    7.911  7819 7291.626 7299.537
  0.00   0.00 100.00  99.96  98.01  95.88    613    7.911  7825 7296.642 7304.554
  0.00   0.00 100.00  99.96  98.01  95.88    613    7.911  7831 7301.885 7309.796
  0.00   0.00 100.00  99.97  98.01  95.88    613    7.911  7837 7307.024 7314.935
  0.00   0.00 100.00  99.97  98.01  95.88    613    7.911  7843 7311.874 7319.785
  
	-、namenode 日期提示：
2020-09-01 17:30:27,290 WARN  protocol.BlockStoragePolicy (BlockStoragePolicy.java:chooseStorageTypes(162)) - Failed to place enoughreplicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-09-01 17:30:27,290 WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(385)) - Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable: unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-09-01 17:30:27,290 INFO  BlockStateChange (BlockManager.java:computeReplicationWorkForBlocks(1588)) - BLOCK* neededReplications= 181, pendingReplications = 0.
2020-09-01 17:30:27,290 INFO  blockmanagement.BlockManager (BlockManager.java:computeReplicationWorkForBlocks(1595)) - Blocks chosenbut could not be replicated = 1
 of which 1 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.
2020-09-01 17:30:30,290 INFO  BlockStateChange (UnderReplicatedBlocks.java:chooseUnderReplicatedBlocks(395)) - chooseUnderReplicatedBlocks selected 6 blocks at priority level 1
  Total=6 Reset bookmarks? false

参考资料：https://www.cnblogs.com/ChoviWu/p/10069399.html	

	https://blog.csdn.net/gklifg/article/details/50418109
	怀疑连接没有关闭？
	https://blog.csdn.net/u011563666/article/details/79033226
	调大heapsize？  -> 调大 HADOOP_HEAPSIZE=4096
	
	-、查看hiveserver2 的连接情况
	
	[root@lymaster02 hive]# netstat -ant	 |grep 10000
tcp        0      0 0.0.0.0:10000           0.0.0.0:*               LISTEN      15507/java
tcp        0      0 10.0.24.106:50591       10.0.24.106:10000       TIME_WAIT   -
tcp        1      0 10.0.24.106:10000       10.0.24.106:49191       CLOSE_WAIT  15507/java
tcp        0      0 10.0.24.106:10000       10.20.88.11:1326        ESTABLISHED 15507/java
tcp        0      0 10.0.24.106:10000       10.20.88.11:1152        ESTABLISHED 15507/java
tcp        0      0 10.0.24.106:49766       10.0.24.106:10000       FIN_WAIT2   -
tcp        0      0 10.0.24.106:48974       10.0.24.106:10000       FIN_WAIT2   -
tcp        0      0 10.0.24.106:49191       10.0.24.106:10000       FIN_WAIT2   -
tcp        0      0 10.0.24.106:10000       10.20.89.28:8894        ESTABLISHED 15507/java
tcp        1      0 10.0.24.106:10000       10.0.24.106:49432       CLOSE_WAIT  15507/java
tcp        1      0 10.0.24.106:10000       10.0.24.106:49766       CLOSE_WAIT  15507/java
tcp        0      0 10.0.24.106:49432       10.0.24.106:10000       FIN_WAIT2   -
tcp        1      0 10.0.24.106:10000       10.0.24.106:48974       CLOSE_WAIT  15507/java


90、lymaster02 ，/var/log/hadoop/hdfs目录，审计日志突然暴增（hdfs-audit.log）  20M -> 5GB

91、执行MR ，卡顿 
-、查看 yarn日志：
2020-10-13 17:02:24,761 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(464)) - Memory usage of ProcessTree 6254for container-id container_e248_1601346079991_1192_01_000020: 777.9 MB of 2 GB physical memory used
 3.5 GB of 4.2 GB virtual memoryused
2020-10-13 17:02:26,791 WARN  nodemanager.DirectoryCollection (DirectoryCollection.java:checkDirs(311)) - Directory /hadoop/yarn/local error, used space above threshold of 90.0%, removing from list of valid directories
2020-10-13 17:02:26,792 WARN  nodemanager.DirectoryCollection (DirectoryCollection.java:checkDirs(311)) - Directory /hadoop/yarn/logerror, used space above threshold of 90.0%, removing from list of valid directories

	推测原因： lyzk02，lyzk03 磁盘满了
92、hive cli命令行 没进去
2020-10-16 15:44:07,319 INFO  [main]: ipc.Client (Client.java:handleConnectionFailure(904)) - Retrying connect to server: lymaster01/10.0.24.105:8020. Already tried 0 time(s)
 retry policy is RetryPolicy[MultipleLinearRandomRetry[500x2000ms], TryOnceThenFail]
解决：发现是连接不上 205的NN

93、lyzk02 负载10%
	-磁盘io瓶颈 
解决：因为zk挂掉，导致许多连接未关闭。
sar -b 1 10

94、查看CPU核数
# 总核数 = 物理CPU个数 X 每颗物理CPU的核数 
# 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数

# 查看物理CPU个数
cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l


# 查看每个物理CPU中core的个数(即核数)
cat /proc/cpuinfo| grep "cpu cores"| uniq


# 查看逻辑CPU的个数
cat /proc/cpuinfo| grep "processor"| wc -l

94、 2020-11-10  测试 regionserver宕机，lyzk02日志
2020-11-10 13:17:35,207 INFO  [IndexRpcServer.handler=10,queue=0,port=16020] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x4a3fc7ac): could not load 1074787156_BP-518231600-10.0.24.105-1567588688245 due to InvalidToken exception.
org.apache.hadoop.security.token.SecretManager$InvalidToken: access control error while attempting to set up short-circuit access to /apps/hbase/data/data/DW/DWS_PURCHASE_REPORT_BY_JIEG/bb0eec2df67c8e9eef5e464624d22e36/0/6ac4f12b5698404e9e327decc05ec972      
		at org.apache.hadoop.hdfs.BlockReaderFactory.requestFileDescriptors(BlockReaderFactory.java:589)
        at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:488)
		at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.create(ShortCircuitCache.java:784)
...	
2020-11-10 13:17:45,943 WARN  [IndexRpcServer.handler=13,queue=0,port=16020] ipc.RpcServer: (responseTooSlow): {"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","starttimems":1604985455244,"responsesize":14,"method":"Scan","processingtimems":10081,"client":"10.0.40.116:34602","queuetimems":181,"class":"HRegionServer"}
......

2020-11-10 14:46:19,004 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=696.49 MB, freeSize=2.46 GB, max=3.14 GB, blockCount=37207, accesses=4216400, hits=4031462, hitRatio=95.61%, , cachingAccesses=4068677, cachingHits=4030882, cachingHitsRatio=99.07%, evictions=35849, evicted=583, evictedPerRun=0.016262657940387726
2020-11-10 14:51:19,004 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=696.49 MB, freeSize=2.46 GB, max=3.14 GB, blockCount=37207, accesses=4223007, hits=4038069, hitRatio=95.62%, , cachingAccesses=4075284, cachingHits=4037489, cachingHitsRatio=99.07%, evictions=35879, evicted=583, evictedPerRun=0.01624905876815319
2020-11-10 14:56:19,004 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=696.49 MB, freeSize=2.46 GB, max=3.14 GB, blockCount=37207, accesses=4228137, hits=4043199, hitRatio=95.63%, , cachingAccesses=4080414, cachingHits=4042619, cachingHitsRatio=99.07%, evictions=35909, evicted=583, evictedPerRun=0.016235483810305595
2020-11-10 14:57:00,455 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4236ms
No GCs detected
2020-11-10 14:57:52,930 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 21533ms
No GCs detected
...
2020-11-10 14:57:53,994 INFO  [RS_OPEN_REGION-lyzk02:16020-2-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 33139ms for sessionid 0x1758d5d2fcf02f5, closing socket connection and attempting reconnect
2020-11-10 14:57:54,039 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1604772294459-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26730ms for sessionid 0x2759e6abcaa022a, closing socket connection and attempting reconnect
2020-11-10 14:57:54,039 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1604772294459-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 33395ms for sessionid 0x3758d5cb1b204a7, closing socket connection and attempting reconnect
...
2020-11-10 14:57:58,356 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1604772294459-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk01/10.0.24.107:2181, initiating session
...
2020-11-10 14:58:14,676 FATAL [main-EventThread] regionserver.HRegionServer: ABORTING region server lyzk02,16020,1604632278759: regionserver:16020-0x1758d5d2fcf01be, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181, baseZNode=/hbase-unsecure regionserver:16020-0x1758d5d2fcf01be received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:585)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:517)
        at org.apache.hadoop.hbase.zookeeper.PendingWatcher.process(PendingWatcher.java:40)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:534)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)
2020-11-10 14:58:14,676 FATAL [main-EventThread] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.phoenix.coprocessor.MetaDataEndpointImpl, org.apache.phoenix.coprocessor.SequenceRegionObserver, org.apache.phoenix.coprocessor.ScanRegionObserver, org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver, org.apache.phoenix.hbase.index.Indexer, org.apache.phoenix.coprocessor.MetaDataRegionObserver, org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver, org.apache.phoenix.coprocessor.ServerCachingEndpointImpl, org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint]
...
2020-11-10 14:58:31,490 WARN  [DataStreamer for file /apps/hbase/data/WALs/lyzk02,16020,1604632278759/lyzk02%2C16020%2C1604632278759.default.1604988713708 block BP-518231600-10.0.24.105-1567588688245:blk_1074791294_1078513] hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): Lease mismatch on /apps/hbase/data/WALs/lyzk02,16020,1604632278759-splitting/lyzk02%2C16020%2C1604632278759.default.1604988713708 (inode 5748567) owned by DFSClient_NONMAPREDUCE_-1765257182_1 but is accessed by DFSClient_NONMAPREDUCE_1995389629_1
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3549)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:3436)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:877)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:523)
...
2020-11-10 14:59:41,785 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1604772294459-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:585)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:517)
        at org.apache.hadoop.hbase.zookeeper.PendingWatcher.process(PendingWatcher.java:40)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:534)
......
2020-11-10 14:59:44,099 ERROR [regionserver/lyzk02/10.0.24.110:16020] regionserver.HRegionServer: Shutdown / close of WAL failed: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on /apps/hbase/data/WALs/lyzk02,16020,1604632278759-splitting/lyzk02%2C16020%2C1604632278759.default.1604988713708 (inode 5748567) owned by DFSClient_NONMAPREDUCE_-1765257182_1 but is accessed by DFSClient_NONMAPREDUCE_1995389629_1
		问题原因： 
			推测1:是因为 hdfs写密集，导致RS的租约失效，进而导致session失效。
			推测2:GC导致，导致RS的租约失效，进而导致session失效。 《参考资料：https://blog.csdn.net/mtj66/article/details/78392523》
		解决：	
			1、脚本重启RS
			2、调大zookeeper.session.timeout参数（测试为90s）
			
95、2020-11-17 10:41:51,463 INFO  blockmanagement.BlockManager (BlockManager.java:computeReplicationWorkForBlocks(1595)) - Blocks chosen but could not be replicated = 6
 of which 6 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.  ？？


当出现大量无法同步的under-replicated block时，在检查了replication设置小于datanode的情况下，查看namenode日志如果发现类似情况：

Blocks chosen but could not be replicated = 10
 of which 10 have no target, 0 have no source, 0 are UC, 0 are abandoned, 0 already have enough replicas.

这意味着replication无处可以复制了，由于种种原因，系统可能出现单份block复制数超过replication设置的情况，这时候就无法继续做同步了。

需要更改参数dfs.replication.max<=datanode数量。

另外可通过：

首先设置 dfs.replication=1进行block删除放弃，只保留一个副本，

再通过：

hdfs dfs Csettrep CR 3 /
重新进行平衡

再将dfs.replication=3

参考资料：http://www.icefish.cc/?p=556


96、hive重启任务
lymater02 
目录：/opt/shell/cluster_monitor
功能：重启hivemeta

lymater01：
功能：重启namenode1


NameNode服务异常:Timed out waiting 20000ms for a quorum of nodes to respond
解决：
cp /usr/hdp/2.5.3.0-37/hadoop/conf/hdfs-site.xml  /usr/hdp/2.5.3.0-37/hadoop/conf/hdfs-site.xml-backup
<property>
        <name>dfs.qjournal.write-txns.timeout.ms</name>
        <value>90000</value>
</property>
<property>
        <name>dfs.qjournal.start-segment.timeout.ms</name>
        <value>90000</value>
</property>
<property>
        <name>dfs.qjournal.select-input-streams.timeout.ms</name>
        <value>90000</value>
</property>

cp /usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml  /usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml-backup
/usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml

<property>
        <name>ipc.client.connect.timeout</name>
        <value>90000</value>
</property>

scp /usr/hdp/2.5.3.0-37/hadoop/conf/hdfs-site.xml  root@lymaster02:/usr/hdp/2.5.3.0-37/hadoop/conf/hdfs-site.xml
scp /usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml  root@lymaster02:/usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml

scp -r current/  root@lyzk02:/hadoop/hdfs/journal/mycluster/current/ 

参考资料：https://blog.csdn.net/wuleidaren/article/details/106395551
https://my.oschina.net/u/2277929/blog/760847

根本原因： 
lyzk03，lyzk01 ： Sync of transaction range 19114869-19114869 took 1761ms
lyzk02 journalnode Can't scan a pre-transactional edit log异常处理
解决：删除lyzk02的journal  ,/hadoop/hdfs/journal/mycluster/current/  ,copy 正确的文件（lyzk01或lykz03）至lyzk02
https://blog.csdn.net/odailidong/article/details/79628425

scp -r /hadoop/hdfs/journal/mycluster/current/ root@lyzk02:/hadoop/hdfs/journal/mycluster/


scp -r /hadoop/hdfs/journal/promycluster/current/ root@prodata03:/hadoop/hdfs/journal/promycluster/

chown hdfs:hadoop -R ./current

97、设置文件打开数 
ulimit -n 65536

https://blog.csdn.net/nchu2020/article/details/100608624


/etc/security/limits.conf




# Add by MquanZeng on 20200928
 * soft nofile 65536
 * hard nofile 131072
 * soft nproc 2048
 * hard nproc 4096
 * - nofile 65536



-- 问题： hive 设置参数后， 导致 java heap space 			
set hive.execution.engine=mr;
set hive.exec.reducers.max=20;
set mapreduce.map.java.opts=-Xmx2048m;
set mapreduce.map.memory.mb=2304;
set mapreduce.reduce.java.opts=-Xmx2048m;
set mapreduce.reduce.memory.mb=2304;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapredfiles = true;


set hive.execution.engine=mr;
set hive.exec.reducers.max=20;
set mapreduce.map.java.opts=-Xmx5120m;
set mapreduce.map.memory.mb=5000;
set mapreduce.reduce.java.opts=-Xmx9300m;
set mapreduce.reduce.memory.mb=9216;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapredfiles = true;


set hive.execution.engine=mr;
set hive.exec.reducers.max=20;
set mapreduce.map.java.opts=-Xmx2048m;
set mapreduce.map.memory.mb=2304;
set mapreduce.reduce.java.opts=-Xmx4048m;
set mapreduce.reduce.memory.mb=4304;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapredfiles = true;


/opt/ly_project/06-IdeaProjects/ly_dw_dataload/ashell/production/hive/cncprocommon/merge/dwb_t_mes_prodnosetting.sh direct_start

1、调整。hive-env.sh 
	export HADOOP_HEAPSIZE=8033 # Setting for HiveServer2 and Client
		
2、换衣，auxlib下少了测试有的几个jar？


3、
orgrg.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: mapResourceRequest:<memory:4608, vCores:1>
 org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: reduceResourceRequest:<memory:9216, vCores:1>
 设置系统默认的参数后，  
 set mapreduce.map.java.opts=-Xmx5120m;
set mapreduce.map.memory.mb=5000;
set mapreduce.reduce.java.opts=-Xmx9300m;
set mapreduce.reduce.memory.mb=9216;  
可以跑成功
5、  重启  hdfs, yarn
没效果。。。

98、
2020-11-26 16:21:32,864 ERROR metrics.SystemMetricsPublisher (SystemMetricsPublisher.java:putEntity(517)) - Error when publishing entity [YARN_APPLIC
ATION,application_1601346079991_1218], server side error code: 7
2020-11-26 16:21:32,865 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
        at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:915)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$5.run(ZKRMStateStore.java:998)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$5.run(ZKRMStateStore.java:995)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1174)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1207)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:995)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.access$500(ZKRMStateStore.java:92)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$VerifyActiveStatusThread.run(ZKRMStateStore.java:1142)
2020-11-26 16:21:32,865 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 5
2020-11-26 16:21:32,954 ERROR metrics.SystemMetricsPublisher (SystemMetricsPublisher.java:putEntity(517)) - Error when publishing entity [YARN_APPLIC
ATION,application_1601346079991_1415], server side error code: 72020-11-26 16:21:32,971 ERROR metrics.SystemMetricsPublisher (SystemMetricsPublisher.java:putEntity(517)) - Error when publishing entity [YARN_APPLIC
ATION,application_1601346079991_1315], server side error code: 7

参考资料：https://blog.csdn.net/github_32521685/article/details/89953788


99、观察 
[root@lymaster02 yarn]# df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/centos-root   50G   38G   13G  75% /

[root@lymaster02 yarn]# du -sh /var/log/hadoop-yarn/yarn
1.7G    /var/log/hadoop-yarn/yarn

100、hive tez  Error evaluating  NullPointerException
https://www.jianshu.com/p/6e57d24d2636?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation

100、 
[(]*[)]
恪守(ke),陈演恪（que）：方言读音,假道伐虢(guo),
毋(wu)乃有阙,伟懋(mao),玺绶(shou),擢(zhuo)用名流,蔡邕(yong),大恸(tong),茕(qiong)速,鸩(zhen)酒,连轸(zhen)还都,
欲伺(si,[ci])便杀卓,暗忖(cun),急掣(che),惶遽(ju),鞍辔(pei),谯(qiao)郡,成皋(gao,[hao]：姓),
夏侯(dun),旗幡(fan),檄(xi)文,韩馥(fu),孔(zhou, 同“胄),兖(yan)州,张邈(miao),白旄(mao)黄钺(yue),牦(mao)牛,
俾(bi)坠(zhui)其命,歃(sha)血,汜(si)水,赵岑(cen),搦(nuo)战,无垠(yin)[边际],赤帻(ze)[头巾],弁(bian)[冠],
巾(kui)[是在不用簪子的情况下固定冠的发带],绡(xiao)[生丝],i(he)冠,锭(ding)刀,屯(zha),折(she)了,
酾(shi)酒,斟(zhen)酌,酌(zhuo)酒,銮(luan)铃[系在马身上的响铃],李(jue,[que]:姓),郭汜(si,又说(fan)),
勒(le)甲,行李没有捆紧，再勒(lei)一勒,挥槊(shuo)[杆儿比较长的矛],抖擞(sou),酣(han)战,雕栏玉砌(qi),簪(zan)子,
绒绦(tao),瓦砾(li),崤(xiao)山,作揖(yi),周毖(bi),家赀(zi),坟冢(zhong),荥(xing)阳,荥(ying)经,胳膊(bo),踅(xue)过[来回走],
镌(juan),卞(bian)和,S(huan)辕,滂(pang)沱,孔昱(yu),檀(tan)敷,岑y(zhi),蒯(kuai)良,垓(gai)下,
磐(pan)河,荀谌(chen,shen[作姓名时]),祢(mi)衡,譬(pi)如,沮(ju)授,[沮丧],麴(qu)义,重颐(yi)[双下巴],搴(qian)旗[拔取],
兜鍪(mao),马日(di),曩(nang)日[往日],叵(po)耐,翊(yi),叔弼(bi),进泊(bo),水泊(po)梁山,岘(xian)山,遽(ju)[匆忙]尔罢兵,
趱(zan)程,贮(zhu),僭(jian)天子仪仗[ 超越本分],董F(min),(hu[户])县,董璜(huang),d(mei)坞 (wu),府邸(di),
闻雷失箸(zhu),荼(tu)蘼(mi),长吁(xu),呼吁(yu),嘉肴美馔(zhuan),迎迓(ya),钧(jun)意[阁下],帏(wei)幔(man),
捧觞(shang),帘栊(long),笙(sheng)簧(huang)缭绕,榆(xu)钱[像小铜钱的果实],低讴(ou),绛(jiang)唇,
q(zhun[纯粹])钢剑,擎(qing)杯,毡(zhan)车,妆奁(lian),蹙(cu)双眉,怏(yang)怏而出,居心叵(po)测,生噬(shi),
肉颤(chan),颤(zhan)栗,谒(ye)[进见],偈(ji)语,通衢(qu),绮(qi)罗,叱咤(zha)风云,黥(qiong)首刖(yue)足,
佞(ning)臣,讪(shan)议,贾诩(xu),鸿胪(lu),车辄(zhe),弥天亘(gen)地,魁(kui)首,
T(zhou)(zhi),忻(xin)喜,搠(shuo[刺])个空,隘(ai)口,半阑(lan[同栏]),荀(yu),荀绲(gun),
刘晔(ye),吕虔(qian),毛d(jie),纯笃(du), 鄄(juan)城,朐(qu)县,李膺(ying),阍(hun)人[守门人],
亟(ji)待,修葺(qi),陂(po)陀,葛陂(bei)/～池,黄陂(pi),掳(lu)掠,贽(zhi)见之礼,_(chen)目,暹(xian)罗,
水獭(ta),暴殄(tian)天物,饥馑(jin),金瓯(ou),黍(shu)离[庄稼一行行],
稻,黍(shu)[黄米],稷(ji)[高粱/粟/不粘的黍],麦,菽(shu)[豆子总称],
撄(ying)其锋,郦(zhi)县,郦(li)[姓],有恃(shi)无恐,虎贲(ben),骅(hua)骝(liu),驻跸(bi),粗粝(li),
饿莩(piao),饿殍(piao)遍野,簸箕(ji),具茨(ci)山[始祖山],轵(zhi)道,茼蒿(hao),樵(qiao)采,芒砀(dang),
骓(zhui)[毛色青白相杂的马],赍(ji)诏,清淡(dan),颔(han)之,屦(ju),履(lv),辄(zhe),盱(xu)眙(yi),巨觥(gong)[兽角酒器],
阔绰(chuo),绰(chao)丈八蛇矛,张(hong),牛渚(zhu),笮(ze)融,笮(zuo)桥,兜鍪(mao),秣(mo)陵,安辑(ji),
阊(chang)门,裨(pi)将,无裨(bi)于事,昕(xin)[日出],鏖(ao)战,查渎(du),海隅(yu),葳(wei)蕤(rui)[枝叶繁茂],桥蕤,
年已及笄(ji)[15岁],俟(si)备[等待准备],万俟l[mò qí Xiè],U(yu)水[南阳白河],饱则r(yang)去,
谶(chen)云,一语成谶,东临碣(jie)石,下邳[pi今睢[sui]宁],浚(jun)山,浚(xun)县,临沂(yi),吝(lin)啬(se),
刈(yi)麦,自戕(qiang),惊悚(song),貔貅(pi,xiu),锹(qiao)(jue),觑(qu)得亲切,啖(dan)之,昌g(xi)[g:大野猪],
盍(he)早为计,佞(ning)贼,眭(sui)固,小憩(qi),棺椁(guo),丹墀(chi),金t(pi),夤(yin)夜,
玉磬(qiu)[美玉],薄言往(su)[诉说],(shuo)[恐惧]而再拜,仓廪(lin),踌(chou)躇(chu),臻(zhen),
饕餮(tao,tie),入赘(zhui),折衄(nv),诘(ji)屈聱(ao)牙[文章晦涩难懂],诘(jie)外奸,勖(xu),豕(shi)[猪],
俾V(bi,yi)[使治理], 日仄(ze)[太阳偏西],编纂(zuan),鹗(e)[鱼鹰],坌(ben)涌,擢(zhuo)[提拔]拜,衢(qu)[四通八达路],
[(yao)袅,褐(he)衣[粗布衣服],挝(zhua)鼓,蜾(guo)虫,俦(chou)[辈],公廨(xie),解豸(xie,zhi),簪(zan)缨,
三缄(jian)其口,笔楮(chu)难穷,卞(bian),虬(qiu)龙,瞻谒(ye),偕(xie)同,歆(xin)慕,鹤氅(chang)[鹤毛做的大衣],
藜(li)杖,缧(lei)绁(xie),斫(zhuo)[砍]之,禳(rang)[向鬼神祈祷消除灾]之,(di)n(dao),
设醮(jiao)[ 1.古代结婚时用酒祭神的礼节。2.指女子嫁人。3.指僧道设坛祭神。],禄(lu)祚(zuo),
两(jun),谮(zen)[诬陷]曰,诬(wu)人,眭(sui)元进,韩莒(ju)子,啜(chuo)饮,啜(chuai)[姓],
跣(xian)足,僭(jian)言,嗔(chen)[生气],衔(xian)枚,迤(yi)逦(li)[曲折走],箪(dan)食,自隳(hui)其志,征鼙(pi),
髡(kun)发,毗(pi)邻,觊觎,(yi)觎,夔(kui),簸(bo)箕(ji),琰(yan)[雕饰的美玉],珩(heng)[美玉],诡谲(jue),
命运多蹇(jian),饮馔(zhuan),火欲殂(cu),泌(bi)阳,内分泌(mi),螟(ming)蛉(ling),缧(lei)X(xie),劬(qu)劳,
樗(chu)栎(li)庸材,趱(zan)程,鼎镬(huo)[大锅],倥(kong)偬(zong),鼎鼐(nai),悒(yi)怏,揲(she)蓍(shi),
愧赧(nan),沔(mian),妫(gui)览,汉祚(zuo)[福],艨艟[蒙冲],鄱(po)阳湖,浼(mei),隽(juan)永,.隽(jun)[俊],簿(bu),
G(li)牛尾,飙(biao),郗(chi,xi)虑,狎(xia)侮,殛(ji)汝,讣(fu)告,王璨(can),齑(ji)粉,谄(chan)佞(ning),〔[欷〕(xi,xu)也作嘘唏,
青G(gong),(ke)Q(da),(yǐ)觎,蕲(qi)州,龙骧(xiang)虎视,步骘(zhi),怀桔遗亲[陆绩-孝顺],耿m(yan),
程德枢(shu),芟(shan)除,哂(shen)笑,罹(li)兵,矍(jue)然,飞棹(zhao),阚(kan)泽,浅戆(gang),穰(rang)苴(ju),
榜佥(jian),纛(dao),袒(tan)臂一呼,锋镝(di),指(qun)相赠,R(ju)(ji),孱(chan)陵,孱(cna)头,髭(zi)髯(ran),
诌(zhou)佞(ning),市井闾(lv)阎(yan),箧(qie),帙(zhi),郢(ying)州,阆(lang)中,Z(zhuan)[专]候,鹪(jiao)鹩(liao),
操以谲(jue),涪(fu)城,刘Y(gui),泠(ling)苞(bao),葭(jia)萌关,盘诘(jie),秣(mo)陵,濡(ru)须,舄(xi)履(足下),
(ju)鬯(chang)[黑黍和郁金酿造的酒],圭(gui)瓒(zan)[玉制酒器],卣(you)[古代盛酒的器具，口小腹大],
荀恽(yun),豕(shi),豚(tun),彘(zhi),雒(luo)县,犍为（Qiánwéi）,s(zhan),冕十二旒(liu),玉(tian),
眇(miao)一目,跛(bo)一足,懒(lan)衣,e(xun),倏(shu)忽,管辂(lu),(wei)而谈,L(bi)疾,觳(hu)觫(su),鹿脯(fu),
M(chen),曩(nang)[过去],殪(yi),骘(zhi),廨，榇(chen),趱(zan)军,疥(jie)癞(lai),鼙(pi),棹(zhao)[.划船的一种工具，形状和桨差不多],
舟(gou),蓍(shi)草,揲(she)[古代数蓍草以占卜吉],
骠骑(piao,qi),殁  (mo),丁M(yi),郯(tan)城,宄(gui)[坏人],颤(zhan)栗,丹墀(chi),V(xiao)亭,恂(xun),
番(fan)王,番(pan)禺,e(xun)血,雠(chou),擐(huan)[穿],夥(huo),醢之[肉酱],槛(jian)车,纡(yu),
包原隰(xi),[低湿的地方],槎[cha,ya],郦食[yì]其[jī],秦宓(mi),子(chi),鹤鸣九皋(gao),耆[ [qí]：1.年老，六十岁以上的人。 2.强横。 [shì]：古同“嗜”，爱好。]帅,t(yi), 
雍][yōng kǎi],越Q[xi]郡,伉(kang)俪,愠[yun]色,薤(xie)叶,大纛[dao]南征,盗跖(zhi)下惠,罪愆[qian],虿[chai],廪[lin, 1.粮仓。 2.指粮食。]禄,
崩殂[cu],S[xi]正,都亭侯袁D[chen],爨[cuan]习,龙骧[xiang]将军关兴,刘琰[yan，美玉],夏侯[mao],上[gui]
,高阜[fu]处,畿(ji),箕(ji)谷,麈[zhu]尾[驱虫p掸尘的一种工具],[yong],殛鲧用禹[jí gǔn yòng yǔ ](指称一人有罪，不可株连家人),
 周鲂[fang],鄱(po)阳,全琮[cong],诸葛恪[ke],而君引愆[qian],馘[guo]斩,毕星廛[chan]于太阴之分,
 千里馈[kui]粮，士有饥色；樵[qiao]苏后爨[cuàn]，师不宿饱{从千里之外运送军粮，士兵就会忍饥挨饿；临时打柴做饭，士兵常常不能吃饱},
 簪[zan]冠鹤氅[chang],张隽V[yi],谪[zhe]为,曷[he]此,大龛[kan],槎[cha]山,金t[pi]箭,
 宫闼[ta],甑[zeng],邓r[yang],畋[tian]猎,豚[tun]犊[du],兄毓[yu],洮[tao]河,傅嘏[ [gǔ] ,jiǎ],王昶[chang],
 酹[lei]酒，蹶[jue]然,@[yin]水,慎[shen]县,⒑薄fú hǎn】 (bao),狄[di]道城，陈骞[qian],全t[yi],全怿[yi],
 成济[nian]戟，王[guan],S[xi]正,愀[qiao]然,刘[shi],撺[cuan],石碣[jie],偈[ji]语,涪[fu]城,三[dong]鼓,刘谌[chen],
 面缚舆榇[miàn fù yú chèn],华[he],蠹[zhu]国,荀[yi],羊祜【hu】,鸩[zhen]人,斑鸠[jiu],
 荀[yi]、冯[dan],王F[jun],棋枰[ping],司马[zhou],牛渚[zhu],沅[yuan]、湘,誊[teng]录,
 
101、解决ambari-server挂机
	问题：prodata02(10.0.24.117),proweb01(10.0.24.115),两台主机突然挂机。
	prodata01的agent日志断连接详情：
		WARNING 2021-04-05 19:19:40,373 NetUtil.py:93 - Failed to connect to https://prodata02:8440/ca due to [Errno 111] 拒绝连接
		WARNING 2021-04-05 19:19:40,375 NetUtil.py:116 - Server at https://prodata02:8440 is not reachable, sleeping for 10 seconds...
		INFO 2021-04-05 19:19:50,376 NetUtil.py:62 - Connecting to https://prodata02:8440/ca
		WARNING 2021-04-05 19:19:50,392 NetUtil.py:93 - Failed to connect to https://prodata02:8440/ca due to [Errno 111] 拒绝连接
		WARNING 2021-04-05 19:19:50,393 NetUtil.py:116 - Server at https://prodata02:8440 is not reachable, sleeping for 10 seconds...
		INFO 2021-04-05 19:20:00,394 NetUtil.py:62 - Connecting to https://prodata02:8440/ca
		WARNING 2021-04-05 19:20:00,411 NetUtil.py:93 - Failed to connect to https://prodata02:8440/ca due to [Errno 111] 拒绝连接
		WARNING 2021-04-05 19:20:00,412 NetUtil.py:116 - Server at https://prodata02:8440 is not reachable, sleeping for 10 seconds...
		INFO 2021-04-05 19:20:10,413 NetUtil.py:62 - Connecting to https://prodata02:8440/ca
		ERROR 2021-04-05 19:20:10,497 NetUtil.py:88 - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:618)
		ERROR 2021-04-05 19:20:10,497 NetUtil.py:89 - SSLError: Failed to connect. Please check openssl library versions.
		Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.
		WARNING 2021-04-05 19:20:10,500 NetUtil.py:116 - Server at https://prodata02:8440 is not reachable, sleeping for 10 seconds...
		INFO 2021-04-05 19:20:20,501 NetUtil.py:62 - Connecting to https://prodata02:8440/ca
		ERROR 2021-04-05 19:20:20,585 NetUtil.py:88 - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:618)
		ERROR 2021-04-05 19:20:20,586 NetUtil.py:89 - SSLError: Failed to connect. Please check openssl library versions.
		Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.
		WARNING 2021-04-05 19:20:20,589 NetUtil.py:116 - Server at https://prodata02:8440 is not reachable, sleeping for 10 seconds...
		INFO 2021-04-05 19:20:30,589 NetUtil.py:62 - Connecting to https://prodata02:8440/ca
		ERROR 2021-04-05 19:20:30,666 NetUtil.py:88 - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:618)
		ERROR 2021-04-05 19:20:30,666 NetUtil.py:89 - SSLError: Failed to connect. Please check openssl library versions.
		Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.
		WARNING 2021-04-05 19:20:30,669 NetUtil.py:116 - Server at https://prodata02:8440 is not reachable, sleeping for 10 seconds...
			
	解决：
		1、vim /etc/python/cert-verification.cfg
				-> verify=disable
			重启ambari-agent restart  ，失败
		
		2、cat /etc/ambari-agent/conf/ambari-agent.ini
				-> 在[security]添加下面内容 force_https_protocol=PROTOCOL_TLSv1_2
			重启ambari-agent restart  ，失败
			
		3、查看 /var/lib/ambari-agent/keys ，相关keys文件
		
	操作之后 prodata01日志：
		
		INFO 2021-04-05 19:23:01,876 HeartbeatHandlers.py:115 - Stop event received
		INFO 2021-04-05 19:23:01,876 NetUtil.py:122 - Stop event received
		INFO 2021-04-05 19:23:01,877 ExitHelper.py:53 - Performing cleanup before exiting...
		INFO 2021-04-05 19:23:01,877 ExitHelper.py:67 - Cleanup finished, exiting with code:0
		INFO 2021-04-05 19:23:04,212 main.py:223 - Agent died gracefully, exiting.
		INFO 2021-04-05 19:23:04,213 ExitHelper.py:53 - Performing cleanup before exiting...
		INFO 2021-04-05 19:23:04,614 main.py:90 - loglevel=logging.INFO
		INFO 2021-04-05 19:23:04,614 main.py:90 - loglevel=logging.INFO
		INFO 2021-04-05 19:23:04,614 main.py:90 - loglevel=logging.INFO
		INFO 2021-04-05 19:23:04,616 DataCleaner.py:39 - Data cleanup thread started
		INFO 2021-04-05 19:23:04,617 DataCleaner.py:120 - Data cleanup started
		INFO 2021-04-05 19:23:04,619 DataCleaner.py:122 - Data cleanup finished
		INFO 2021-04-05 19:23:04,661 PingPortListener.py:50 - Ping port listener started on port: 8670
		INFO 2021-04-05 19:23:04,665 main.py:349 - Connecting to Ambari server at https://prodata02:8440 (10.0.24.117)
		INFO 2021-04-05 19:23:04,665 NetUtil.py:62 - Connecting to https://prodata02:8440/ca
		INFO 2021-04-05 19:23:04,738 main.py:359 - Connected to Ambari server prodata02
	WARNING 2021-04-05 19:23:04,866 ClusterConfiguration.py:71 - Unable to load configurations from /var/lib/ambari-agent/cache/cluster_configuration/configurations.json. This file will be regenerated on registration
		INFO 2021-04-05 19:23:04,876 threadpool.py:52 - Started thread pool with 3 core threads and 20 maximum threads
		INFO 2021-04-05 19:23:04,881 AlertSchedulerHandler.py:271 - [AlertScheduler] Caching cluster mycluster with alert hash 698bebecfd269f9d48a5aeaf09c5dfc1
		INFO 2021-04-05 19:23:04,887 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,887 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling datanode_heap_usage with UUID 08fab47a-2bb9-4f04-91f7-fb63ee95a62d
		INFO 2021-04-05 19:23:04,887 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling storm_supervisor_process with UUID d324d429-0fad-4650-963f-faeb0a15eddc
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling datanode_unmounted_data_dir with UUID 99635b03-2e56-4bbe-b460-6696a9f79844
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling datanode_process with UUID 53ebaaad-668f-42c8-ac66-31c2521ab7bf
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling yarn_nodemanager_health with UUID 2b39b8da-72c9-4378-9ea9-20befaa1670b
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling datanode_webui with UUID 5b23b3a0-f2ab-460e-a133-736fa4715fec
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling datanode_storage with UUID f201b9e6-73d3-4c56-9862-8777d55cda59
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling yarn_nodemanager_webui with UUID 44f27634-a6d9-4704-a4ae-19b97ae75854
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling nfsgateway_process with UUID 27aafebe-4782-4046-9b29-dd91caa82a03
		INFO 2021-04-05 19:23:04,888 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,888 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling ams_metrics_monitor_process with UUID 61256f5a-cee3-425d-864a-dd732fd7fbf3
		INFO 2021-04-05 19:23:04,889 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts
		INFO 2021-04-05 19:23:04,889 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling journalnode_process with UUID f7b2eb33-eeb0-4bb0-9770-8b0598e0a7c6
		INFO 2021-04-05 19:23:04,889 scheduler.py:287 - Adding job tentatively -- it will be properly scheduled when the scheduler starts		
		INFO 2021-04-05 19:23:04,889 AlertSchedulerHandler.py:356 - [AlertScheduler] Scheduling hbase_regionserver_process with UUID 45efebee-b442-455b-b5eb-20100dc285ca
		INFO 2021-04-05 19:23:04,889 AlertSchedulerHandler.py:156 - [AlertScheduler] Starting <ambari_agent.apscheduler.scheduler.Scheduler object at 0x7f4e228c8f90>
 currently running: False
		INFO 2021-04-05 19:23:06,895 hostname.py:98 - Read public hostname 'prodata01' using socket.getfqdn()
		INFO 2021-04-05 19:23:06,910 logger.py:71 - call[['test', '-w', '\xe6\x8c\x82\xe8\xbd\xbd\xe7\x82\xb9']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,921 logger.py:71 - call returned (1, '')
		INFO 2021-04-05 19:23:06,922 logger.py:71 - call[['test', '-w', '/']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,932 logger.py:71 - call returned (0, '')
		INFO 2021-04-05 19:23:06,932 logger.py:71 - call[['test', '-w', '/dev']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,942 logger.py:71 - call returned (0, '')
		INFO 2021-04-05 19:23:06,943 logger.py:71 - call[['test', '-w', '/dev/shm']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,952 logger.py:71 - call returned (0, '')
		INFO 2021-04-05 19:23:06,952 logger.py:71 - call[['test', '-w', '/run']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,962 logger.py:71 - call returned (0, '')
		INFO 2021-04-05 19:23:06,962 logger.py:71 - call[['test', '-w', '/sys/fs/cgroup']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,972 logger.py:71 - call returned (1, '')
		INFO 2021-04-05 19:23:06,972 logger.py:71 - call[['test', '-w', '/home']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,982 logger.py:71 - call returned (0, '')
		INFO 2021-04-05 19:23:06,982 logger.py:71 - call[['test', '-w', '/opt']] {'sudo': True, 'timeout': 5}
		INFO 2021-04-05 19:23:06,991 logger.py:71 - call returned (0, '')
		INFO 2021-04-05 19:23:07,023 logger.py:71 - call returned (0, '')
		WARNING 2021-04-05 19:23:07,100 Facter.py:454 - Can't get the IP address for eno16780
		INFO 2021-04-05 19:23:07,102 Facter.py:194 - Directory: '/etc/resource_overrides' does not exist - it won't be used for gathering system resources.
		INFO 2021-04-05 19:23:07,263 Controller.py:160 - Registering with prodata01 (10.0.24.112) (agent='{"hardwareProfile": {"kernel": "Linux", "domain": "", "physicalprocessorcount": 16, "kernelrelease": "3.10.0-327.el7.x86_64", "uptime_days": "1", "memorytotal": 131851520, "swapfree": "7.87 GB", "memorysize": 131851520, "osfamily": "redhat", "swapsize": "7.87 GB", "processorcount": 16, "netmask": null, "timezone": "CST", "hardwareisa": "x86_64", "memoryfree": 130047132, "operatingsystem": "centos", "kernelmajversion": "3.10", "kernelversion": "3.10.0", "macaddress": "00:50:56:B8:70:06"
		.......
		INFO 2021-04-05 19:23:07,263 NetUtil.py:62 - Connecting to https://prodata02:8440/connection_info
		INFO 2021-04-05 19:23:07,335 security.py:100 - SSL Connect being called.. connecting to the server
		INFO 2021-04-05 19:23:07,339 security.py:67 - Insecure connection to https://prodata02:8441/ failed. Reconnecting using two-way SSL authentication..
		INFO 2021-04-05 19:23:07,340 security.py:186 - Server certicate not exists, downloading
		INFO 2021-04-05 19:23:07,340 security.py:209 - Downloading server cert from https://prodata02:8440/cert/ca/
		INFO 2021-04-05 19:23:07,413 security.py:194 - Agent key not exists, generating request
		INFO 2021-04-05 19:23:07,414 security.py:263 - openssl req -new -newkey rsa:1024 -nodes -keyout "/var/lib/ambari-agent/keys/prodata01.key" -subj /OU=prodata01/ -out "/var/lib/ambari-agent/keys/prodata01.csr"
		INFO 2021-04-05 19:23:07,451 security.py:202 - Agent certificate not exists, sending sign request
		INFO 2021-04-05 19:23:07,605 security.py:100 - SSL Connect being called.. connecting to the server
		ERROR 2021-04-05 19:23:07,610 security.py:87 - Two-way SSL authentication failed. Ensure that server and agent certificates were signed by the same CA and restart the agent.In order to receive a new agent certificate, remove existing certificate file from keys directory. As a workaround you can turn off two-way SSL authentication in server configuration(ambari.properties)Exiting..
		ERROR 2021-04-05 19:23:07,610 Controller.py:212 - Unable to connect to: https://prodata02:8441/agent/v1/register/prodata01
		Traceback (most recent call last):
		  File "/usr/lib/python2.6/site-packages/ambari_agent/Controller.py", line 165, in registerWithServer  
			ret = self.sendRequest(self.registerUrl, data)  File "/usr/lib/python2.6/site-packages/ambari_agent/Controller.py", line 496, in sendRequest 
		  raise IOError('Request to {0} failed due to {1}'.format(url, str(exception)))
		  IOError: Request to https://prodata02:8441/agent/v1/register/prodata01 failed due to EOF occurred in violation of protocol (_ssl.c:618)
	
	暂时解决：
		1、手动重启对应服务。prodata02 （proweb01 未启动，因为暂时未用到。）
			DataNode
			JournalNode
			RegionServer
			NodeManager
				Spark2 Thrift Server
				Spark Thrift Server
				Supervisor
			ZooKeeper Server

	正常的agent启动日志(以UAT为例 ，uatadata01[ambari-server] -> jdk = 1.7 )：
	...
	INFO 2021-04-06 16:41:27,434 NetUtil.py:62 - Connecting to https://10.0.24.200:8440/connection_info
	INFO 2021-04-06 16:41:27,490 security.py:100 - SSL Connect being called.. connecting to the server
	INFO 2021-04-06 16:41:27,549 security.py:61 - SSL connection established. Two-way SSL authentication is turned off on the server.
	INFO 2021-04-06 16:41:28,672 Controller.py:186 - Registration Successful (response id = 0)
	INFO 2021-04-06 16:41:28,672 ClusterConfiguration.py:119 - Updating cached configurations for cluster uatmycluster
	...
		-、可见SSL connection 连接成功，并不会通过Two-way SSL 连接。
	


备注：
10.0.24.112     prodata01
10.0.24.113     promaster01
10.0.24.114     promaster02
10.0.24.115     proweb01
10.0.24.116     proweb02
10.0.24.117     prodata02       proambari       prozk01
10.0.24.118     prodata03       promysql01      prozk02
10.0.24.119     prodata04       promysql02      prozk03
		
参考资料：https://blog.csdn.net/devalone/article/details/81356617
https://community.cloudera.com/t5/Support-Questions/How-to-turn-off-2-way-SSL-authentication-for-smartsense/td-p/125241 【修改jdk安全配置】

111、
1、订单详情页接口
https://trade.nicetuan.net/customer/join/detail
请求参数：
{
 "orderId": "472485430693352789"
}

2、获取附近商家列表
https://near-api.nicetuan.net/partner/near
请求参数：
{
 "lat": 31.312124252319336,
 "lng": 121.5020523071289
}

3、获取手机

{
	"grouponId":120474,
	"partnerId":94631
}



120、
10.0.24.112     prodata01
10.0.24.113     promaster01
10.0.24.114     promaster02
10.0.24.115     proweb01
10.0.24.116     proweb02
10.0.24.117     prodata02       proambari       prozk01
10.0.24.118     prodata03       promysql01      prozk02
10.0.24.119     prodata04       promysql02      prozk03

正式 ambari失去连接的主机 ： prodata02（10.0.24.117）、proweb01（10.0.24.115）

root/Tlg


############## >>>> 手动启动 正式的集群begin-start  <<<<<################################
手动启动集群：
1、zookeeper启动 【prodata02、prodata03、prodata04】

/usr/hdp/2.5.3.0-37/zookeeper/bin/zkServer.sh start
2、namenode启动命令：(promaster01 ,02)
runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start namenode'

2.1 启动zkfc
runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start zkfc'

3、DN和HN启动(prodata01 ,02,03,04)
JN 启动：runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start journalnode' 
DN：	runuser -l hdfs -c '/usr/hdp/2.5.3.0-37/hadoop/sbin/hadoop-daemon.sh start datanode'

4、RS启动(promaster01 ,02)
runuser -l yarn -c '/usr/hdp/2.5.3.0-37/hadoop-yarn/sbin/yarn-daemon.sh start resourcemanager'
4.1、启动yarn的历史服务器(promaster01)
runuser -l mapred -c  '/usr/hdp/2.5.3.0-37/hadoop-mapreduce/sbin/mr-jobhistory-daemon.sh start historyserver'
【http://lymaster02:19888/jobhistory】

vim /usr/hdp/2.5.3.0-37/hadoop/conf/yarn-site.xml
手动修改NM的计算内存：yarn.nodemanager.resource.memory-mb=49152

4.2 启动 timelineserver

runuser -l yarn -c 'ulimit -c unlimited
 export HADOOP_LIBEXEC_DIR=/usr/hdp/current/hadoop-client/libexec && /usr/hdp/current/hadoop-yarn-timelineserver/sbin/yarn-daemon.sh --config /usr/hdp/current/hadoop-client/conf start timelineserver'
・
5、 NM启动：
 runuser -l yarn -c '/usr/hdp/2.5.3.0-37/hadoop-yarn/sbin/yarn-daemon.sh start nodemanager'
 
6、hivemetastore和hiveserver2启动

---- hivemetastore 启动
后台启动：
proymaster02: 
su hive
hive --service metastore -hiveconf hive.log.file=hivemetastore.log -hiveconf hive.log.dir=/opt/hive-manal &

【1、更新远程权限：
	grant all privileges on *.* to 'hive'@'%' identified by 'Hive-123' with grant option

  flush privileges

】
【 2、查看是否因为密码没有填：
 promater02: 主机添加 ，手动更新 hive-site.xml  hive的密码：
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>Hive-123</value>
      <hidden>HIVE_CLIENT,WEBHCAT_SERVER,HCAT,CONFIG_DOWNLOAD</hidden>
    </property>
】

--select dim,`date`,sum(A.amount),sum(A.cd_amount) from (
--) A  where A.dim = '1'   group by A.dim,A.`date` order by A.`date`,A.dim


---- hiveserver2 启动
后台启动：
su hive 
nohup hive --service hiveserver2 &
日志查看：/home/hive/nohup.out

7、hbase启动 （promaster02）
HMaster：
runuser -l hbase -c '/usr/hdp/2.5.3.0-37/hbase/bin/hbase-daemon.sh start master'

RegionServer：
 runuser -l hbase -c '/usr/hdp/2.5.3.0-37/hbase/bin/hbase-daemon.sh start regionserver'
8、prodata01  nginx启动
启动:/usr/local/nginx/sbin/nginx
 

121、mysql 日志文件多， /var/lib/mysql 清理
	1、vi /etc/my.cnf
		
		expire_logs_days = 10
	参考资料：https://www.cnblogs.com/xxoome/p/9802684.html
	
	systemctl stop mysqld
	systemctl start mysqld
	systemctl status mysqld
	
	2、备份mysql数据库
	-- mysqldump --all-databases -h127.0.0.1 -uroot -ppass > /opt/mysql-backup/mysql-backup-20210521.sql
	mysqldump --all-databases  -uroot -p  > /opt/mysql-backup/mysql-backup-20210521.sql
	
	参考资料：https://www.cnblogs.com/xxoome/p/9802684.html
	https://www.cnblogs.com/fander/p/11635925.html
	122、
	
  
  111、  完整月报：  date > 15 , 传入当前月份
					date <=15  ,传入当前月份-1
		累计月报：  date >16 ,传入当前月份+1 
					date <=16 ,传入当前月份
					
	累计月份周期示例： 2020-12-16~2021-05-06  ，
起始日期: 目前写死为2020-12-16 ，,年=当前年-1， 月=12（写死），日=16 （写死）
截止日期：默认昨天。

112、hdfs读取  resource文件

https://blog.csdn.net/zhuyu19911016520/article/details/98071242



sap.ods_sap_zffi_009  SAP汇率表day='2021-02-17'
tcurr=本位币， fcurr=原币


-- SAP汇率表
select day,count(1) from sap.ods_sap_zffi_009 group by day

select * from  sap.ods_sap_zffi_009 where day='2021-02-17' and tcurr = 'CNY' and fcurr = 'USD'


113、测试 ，hMater宕机， session超时  2021-06-02
Socket connection established to lyzk01 （0x176beef98bc00f5,--> 2021-01-02 23:32:25,580）

lymaster02 相关 lyzk01日志：
2021-05-19 16:17:06,890 INFO  [lymaster02,16000,1621411589981_ChoreService_2-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk01/10.0.24.107:2181. Will not attempt to authenticate using SASL (unknown error)
2021-05-19 16:17:06,890 INFO  [lymaster02,16000,1621411589981_ChoreService_2-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk01/10.0.24.107:2181, 
initiating session
2021-05-19 16:17:36,912 INFO  [lymaster02,16000,1621411589981_ChoreService_2-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 30022ms for sessionid 0x0, closing socket connection and attempting reconnect2021-05-19 16:17:37,012 
WARN  [lymaster02,16000,1621411589981_ChoreService_2] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/hbaseid
2021-05-19 16:17:37,754 INFO  [lymaster02,16000,1621411589981_ChoreService_2-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk02/10.0.24.110:2181. Will not attempt to authenticate using SASL (unknown error)
2021-05-19 16:17:37,755 INFO  [lymaster02,16000,1621411589981_ChoreService_2-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk02/10.0.24.110:2181, initiating session
2021-05-19 16:17:37,759 INFO  [lymaster02,16000,1621411589981_ChoreService_2-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Session establishment complete on server lyzk02/10.0.24.110:2181, sessionid = 0x2798337f193003f, negotiated timeout = 40000
2021-05-19 16:17:38,028 INFO  [lymaster02,16000,1621411589981_ChoreService_2] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x2798337f193003f
2021-05-19 16:17:38,032 INFO  [lymaster02,16000,1621411589981_ChoreService_2] zookeeper.ZooKeeper: Session: 0x2798337f193003f closed
2021-05-19 16:17:38,032 INFO  [lymaster02,16000,1621411589981_ChoreService_2-EventThread] zookeeper.ClientCnxn: EventThread shut down


---->>>>>>>>lymater02 日志：

2021-06-01 01:36:19,561 ERROR [B.priority.fifo.QRpcServer.handler=16,queue=0,port=16000] master.MasterRpcServices: Region server lyzk02,16020,1621838106821 reported a fatal error:
ABORTING region server lyzk02,16020,1621838106821: regionserver:16020-0x3799d0c1dfb0017, quorum=lyzk01:2181,lyzk02:2181,lyzk03:2181,baseZNode=/hbase-unsecure regionserver:16020-0x3799d0c1dfb0017 received expired from ZooKeeper, aborting
Cause:
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:585)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:517)
        at org.apache.hadoop.hbase.zookeeper.PendingWatcher.process(PendingWatcher.java:40)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:534)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)
2021-06-01 01:36:19,728 ERROR [B.priority.fifo.QRpcServer.handler=19,queue=1,port=16000] master.MasterRpcServices: Region server lyzk02,16020,1621838106821 reported a fatal error:

---->>>>>>>>lyzk02日志：

2021-06-01 01:32:21,438 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Session establishment complete on server lyzk03/10.0.24.111:2181, sessionid = 0x2799d0c1eb2032d, negotiated timeout = 40000
2021-06-01 01:32:21,505 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26682ms for sessionid 0x3799d0c1dfb0541, closing socket connection and attempting reconnect
2021-06-01 01:32:21,510 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26679ms for sessionid 0x2799d0c1eb20572, closing socket connection and attempting reconnect
2021-06-01 01:32:21,645 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26671ms for sessionid 0x2799d0c1eb201bb, closing socket connection and attempting reconnect
2021-06-01 01:32:21,689 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk01/10.0.24.107:2181. Will not attempt to authenticate using SASL (unknown error)
2021-06-01 01:32:21,689 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Session 0x3799d0c1dfb0541 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)
........
2021-06-01 01:35:07,182 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Session 0x2799d0c1eb20393 for server null, unexpected error, closing socket connection and attempting reconnectjava.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)2021-06-01 01:35:07,185 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk03/10.0.24.111:2181. Will not attempt to authenticate using SASL (unknown error)
021-06-01 01:35:06,756 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x3799d0c1dfb0419, likely server has closed socket, closing socket connection and attempting reconnect
2021-06-01 01:35:06,757 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 14417ms for sessionid 0x2799d0c1eb20393, closing socket connection and attempting reconnect
2021-06-01 01:35:06,758 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 14597ms for sessionid 0x3799d0c1dfb0498, closing socket connection and attempting reconnect
.......
2021-06-01 01:36:15,562 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Session 0x3799d0c1dfb0161 for server null, unexpected error, closing socket connection and attempting reconnectjava.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)        
		at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)   
		at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)
2021-06-01 01:36:15,562 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Session 0x3799d0c1dfb0030 for server null, unexpected error, closing socket connection and attempting reconnectjava.net.ConnectException: Connection refused
	    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)        
		at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)   
		at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)
.......
2021-06-01 01:36:19,277 INFO  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x2799d0c1eb2032d has expired, closing socket connection2021-06-01 01:36:19,278 WARN  [regionserver/lyzk02/10.0.24.110:16020-shortCompactions-1621838121707-EventThread] client.ConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs itorg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired


------>>>>>  lyzk01 日志；
2021-06-01 01:00:23,693 INFO  [HBase-Metrics2-1] impl.MetricsSinkAdapter: Sink timeline started
2021-06-01 01:00:23,694 INFO  [HBase-Metrics2-1] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2021-06-01 01:00:23,694 INFO  [HBase-Metrics2-1] impl.MetricsSystemImpl: HBase metrics system started
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)
2021-06-01 01:34:51,984 INFO  [regionserver/lyzk01/10.0.24.107:16020-shortCompactions-1621868126475-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk02/10.0.24.110:2181. Will not attempt to authenticate using SASL (unknown error)
2021-06-01 01:34:51,985 INFO  [regionserver/lyzk01/10.0.24.107:16020-shortCompactions-1621868126475-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk02/10.0.24.110:2181, initiating session
2021-06-01 01:34:51,988 INFO  [regionserver/lyzk01/10.0.24.107:16020-shortCompactions-1621868126475-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x2799d0c1eb20206, likely server has closed socket, closing socket connection and attempting reconnect
2021-06-01 01:34:52,004 INFO  [RS_OPEN_REGION-lyzk01:16020-0-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk03/10.0.24.111:2181. Will not attempt to authenticate using SASL (unknown error)
2021-06-01 01:34:52,004 INFO  [RS_OPEN_REGION-lyzk01:16020-0-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Socket connection established to lyzk03/10.0.24.111:2181, initiating session
2021-06-01 01:34:52,005 INFO  [RS_OPEN_REGION-lyzk01:16020-0-SendThread(lyzk03:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x3799d0c1dfb001a, likely server has closed socket, closing socket connection and attempting reconnect
2021-06-01 01:34:52,007 INFO  [regionserver/lyzk01/10.0.24.107:16020-shortCompactions-1621868126475-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 13597ms for sessionid 0x2799d0c1eb201e0, closing socket connection and attempting reconnect
2021-06-01 01:34:52,290 WARN  [regionserver/lyzk01/10.0.24.107:16020-shortCompactions-1621868126475-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Session 0x3799d0c1dfb01ea for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)
		
		
--->>>  lyzk03 日志：
2021-06-01 01:32:20,635 INFO  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26686ms for sessionid 0x2799d0c1eb200d2, closing socket connection and attempting reconnect
2021-06-01 01:32:20,699 INFO  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26686ms for sessionid 0x2799d0c1eb20488, closing socket connection and attempting reconnect
2021-06-01 01:32:20,709 INFO  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26684ms for sessionid 0x2799d0c1eb204a0, closing socket connection and attempting reconnect
2021-06-01 01:32:20,765 INFO  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Opening socket connection to server lyzk01/10.0.24.107:2181. Will not attempt to authenticate using SASL (unknown error)
2021-06-01 01:32:20,773 INFO  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session ti
2021-06-01 01:32:20,773 INFO  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk02:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26681ms for sessionid 0x2799d0c1eb20503, closing socket connection and attempting reconnect
2021-06-01 01:32:20,773 WARN  [regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk01:2181)] zookeeper.ClientCnxn: Session 0x2799d0c1eb200d2 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
        at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)
		
结论：
	1、regionserver/lyzk01/10.0.24.107:16020-shortCompactions-1621868126475-SendThread(lyzk01:2181)  、
	[regionserver/lyzk03/10.0.24.111:16020-shortCompactions-1621838121254-SendThread(lyzk01:2181)]
	  解决： lyzk01 的zk宕机，导致 zk和rs通信失败   ZK 连接数
		lyzk01 ：zookeeper 2021-05-19 就宕机了， 
			-、 连接数少了，  netstat -nap |grep 2181  | wc -l 【https://blog.csdn.net/weixin_30832983/article/details/99778204 ，http://www.openskill.cn/question/361】
				以前已修改连接数 ，/usr/hdp/2.5.3.0-37/zookeeper/conf/zoo.cfg
				设置为500还是少？
				【
				maxClientCnxns  --> 解释
					Limits the number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. This is used to prevent certain classes of DoS attacks, including file descriptor exhaustion. The default is 60. Setting this to 0 entirely removes the limit on concurrent connections.
					 单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制。请注意这个限制的使用范围，仅仅是单台客户端机器与单台ZK服务器之间的连接数限制，不是针对指定客户端IP，也不是ZK集群的连接数限制，也不是单台ZK对所有客户端的连接数限制。
					 
					 
					所以最后：你这里想解决把 maxClientCnxns =1000设置为1000就好。
				】
			-、	java.nio.channels.CancelledKeyException？  【https://blog.51cto.com/u_11429886/2444426】
			原因：  
				ZooKeeper Server发送回复时，Socket连接已经被关闭。	
				CancelledKeyException错误，是因为session失效后，socket已关闭，但服务端仍往该session发送回复信号，引起该错误，该错误并不致命，影响不大。
				是zookeeper版本（3.4.5）bug所致，已在ZK新版本中优化掉。
			备注： 查看连接的进程命令：   netstat -nap |grep 2181 |awk -F " " '{print $7}' | awk -F "/" '{print $1}'| sort -u	
		 netstat -nap |grep 2181 |awk -F " " '{print $7}' | awk -F "/" '{print $1}'| sort| uniq -c | sort -k 1 -nr 
		 
				netstat -nap |grep 2181 | grep 107545 | wc -l
					netstat -nap |grep 2181 | grep 111403 | wc -l
						netstat -nap |grep 2181 | grep 19368 | wc -l
							netstat -nap |grep 2181 | grep 2720 | wc -l
								netstat -nap |grep 2181 | grep 5409 | wc -l
								netstat -nap |grep 2181 | grep 55379 | wc -l
								netstat -nap |grep 2181 | grep 55475 | wc -l
								netstat -nap |grep 2181 | grep 55635 | wc -l
								
								netstat -nap |grep 3306 |awk -F " " '{print $7}' | awk -F "/" '{print $1}'| sort -u	
				
			【lyzk01  zookeeper日志】
			2021-05-19 01:28:20,306 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@193] - Too many connections from /10.0.24.111 - max is 500
			2021-05-19 01:28:20,306 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@193] - Too many connections from /10.0.24.111 - max is 500
			2021-05-19 01:28:20,306 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@193] - Too many connections from /10.0.24.111 - max is 500
			.......
			2021-05-19 09:09:51,316 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@617] - Established session 0x3798085182d0065 with negotiated timeout 40000 for client /10.0.24.107:38717
			2021-05-19 09:09:51,359 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.24.107:38723
			2021-05-19 09:09:51,359 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client/10.0.24.107:36857 which had sessionid 0x3798085182d0060
			2021-05-19 09:09:51,360 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@617] - Established session 0x3798085182d0060 with negotiated timeout 40000 for client /10.0.24.107:38723
			2021-05-19 09:09:51,373 - WARN  [SyncThread:1:FileTxnLog@334] - fsync-ing the write ahead log in SyncThread:1 took 1237ms which willadversely effect operation latency. See the ZooKeeper troubleshooting guide
			2021-05-19 09:09:51,373 - ERROR [CommitProcessor:1:NIOServerCnxn@178] - Unexpected Exception:
			java.nio.channels.CancelledKeyException
					at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
					at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
					at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:151)
					at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1081)
					at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:170)
					at org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:644)
					at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)
			......
			2021-05-19 09:09:52,859 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@357] - caught end of stream exception
			EndOfStreamException: Unable to read additional data from client sessionid 0x178495823f3001d, likely client has closed socket
					at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
					at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
					at java.lang.Thread.run(Thread.java:748)
			2021-05-19 09:09:52,859 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client/10.0.24.111:42322 which had sessionid 0x178495823f3001d
			......
			2021-05-19 09:09:52,860 - INFO  [ProcessThread(sid:1 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x178e0dade642096 type:getData cxid:0x130 zxid:0xfffffffffffffffe txntype:unknown reqpath:/dolphinscheduler136/nodesError Path:null Error:KeeperErrorCode = Session moved
			2021-05-19 09:09:52,860 - INFO  [ProcessThread(sid:1 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x278495853000024 type:getChildren cxid:0x869f5 zxid:0xfffffffffffffffe txntype:unknown reqpath:/hbase-unsecure/backup-masters Error Path:null Error:KeeperErrorCode = Session moved
			2021-05-19 09:09:52,860 - ERROR [CommitProcessor:1:NIOServerCnxn@178] - Unexpected Exception:
			java.nio.channels.CancelledKeyException
					at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
					at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
			.....
			2021-05-19 09:09:53,553 - INFO  [LearnerHandler-/10.0.24.110:55776:LearnerHandler@330] - Follower sid: 2 : info : 		org.apache.zookeeper.server.quorum.QuorumPeer$QuorumServer@ca59818
			2021-05-19 09:09:53,557 - ERROR [CommitProcessor:1:NIOServerCnxn@178] - Unexpected Exception:
			java.nio.channels.CancelledKeyException
					at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
					at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
					at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:151)
					at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1081)
					at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:170)
					at org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:644)
					at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:74)
			2021-05-19 09:09:53,557 - ERROR [CommitProcessor:1:NIOServerCnxn@178] - Unexpected Exception:
			java.nio.channels.CancelledKeyException
					at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
					at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77)
					at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:151)
					at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1081)
			.......
			2021-05-19 09:09:53,941 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@617] - Established session 0x2792e1a96c0047a with negotiated timeout 40000 for client /10.0.24.111:44035
			2021-05-19 09:09:54,021 - INFO  [ProcessThread(sid:1 cport:-1)::PrepRequestProcessor@592] - Got user-level KeeperException when processing sessionid:0x3798085182d005f type:multi cxid:0x219 zxid:0x2140061fa8f txntype:-1 reqpath:n/a aborting remaining multi ops. Error Path:/dolphinscheduler136/nodes/master/10.0.24.107:5678 Error:KeeperErrorCode = NoNode for /dolphinscheduler136/nodes/master/10.0.24.107:5678
			2021-05-19 09:09:54,034 - ERROR [CommitProcessor:1:NIOServerCnxn@178] - Unexpected Exception:
			java.nio.channels.CancelledKeyException
			....
			021-05-19 13:35:49,347 - WARN  [SyncThread:1:FileTxnLog@334] - fsync-ing the write ahead log in SyncThread:1 took 4206ms which willadversely effect operation latency. See the ZooKeeper troubleshooting guide
			2021-05-19 13:36:46,583 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.24.107:59920
			2021-05-19 13:36:46,584 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /10.0.24.107:59920
			2021-05-19 13:36:46,584 - INFO  [Thread-265329:NIOServerCnxn@1007] - Closed socket connection for client /10.0.24.107:59920 (no session established for client)
			2021-05-19 13:37:22,430 - WARN  [SyncThread:1:FileTxnLog@334] - fsync-ing the write ahead log in SyncThread:1 took 78489ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide
			
	
		2021-07-09后续研究：
			1-、发现prodata03 ，2181连接数 ，4000+ ，主要由  zk和rs连接。
				-、重启prodata03， Regionserver ， 连接数立刻由 4000+ ->  3000+ -> 2000+  ->  1300+
					（重启后的RS, 连接数只有5个。）
				-、对应的zk日志，暴增许多，processed session termination 
				2021-07-09 13:59:31,838 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@357] - caught end of stream exception
							EndOfStreamException: Unable to read additional data from client sessionid 0x27a5ba7d67c0011, likely client has closed socket
							at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)
							at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
							at java.lang.Thread.run(Thread.java:745)
				2021-07-09 13:59:31,838 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client/10.0.24.118:52537 which had sessionid 0x27a5ba7d67c0011
				.....
				2021-07-09 14:00:12,055 - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x37a86bbeeed04fe
				2021-07-09 14:00:12,055 - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x17a5b9045d204dc
				2021-07-09 14:00:12,055 - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x27a5c4933dd04e1
				......
				2021-07-09 14:08:08,000 - INFO  [SessionTracker:ZooKeeperServer@347] - Expiring session 0x17a86baeb41064f, timeout of 40000ms exceeded
				2021-07-09 14:08:08,000 - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x17a86baeb41064f
				2021-07-09 14:08:12,000 - INFO  [SessionTracker:ZooKeeperServer@347] - Expiring session 0x27a712f7c3d1ec7, timeout of 40000ms exceeded
				2021-07-09 14:08:12,001 - INFO  [ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x27a712f7c3d1ec7
			
		2022-10-11  
			 Too many connections from /10.0.24.111 - max is 1000
				http://t.zoukankan.com/nhz-M-p-8135229.html
			1、重启zk   ， 连接数并未减少
			2、重启RS， 连接数减为个位数。     为什么那么多连接数？还是会超了2000+
			
124、  日期format

if( int(SUBSTRING(stock_in_audit_time,9,2)) >= 16 ,
  if(SUBSTRING(stock_in_audit_time,6,2)='12',concat(int(SUBSTRING(stock_in_audit_time,0,4))+1 ,'-','01') ,
  concat(int(SUBSTRING(stock_in_audit_time,0,4)) ,'-', if( (int(SUBSTRING(stock_in_audit_time,6,2))+1) < 10 ,
  concat('0',(int(SUBSTRING(stock_in_audit_time,6,2))+1)), int(SUBSTRING(stock_in_audit_time,6,2))+1 ))),
  concat(SUBSTRING(stock_in_audit_time,0,4),'-' ,SUBSTRING(stock_in_audit_time,6,2)))


大数据对外端口：
50070：HDFS (NameNode)WEB UI端口 【http://promaster01:50070/dfshealth.html#tab-overview】
50075：HDFS (DataNode)WEB UI端口  【http://prodata01:50075/datanode.html】
19888：JobHistory WEB UI端口   【http://promaster01:19888/jobhistory】
8088 ：Yarn(RS) 的WEB UI 接口  【http://promaster01:8088/cluster/apps/RUNNING】
8042 ：Yarn(NM) 的WEB UI 接口  【http://prodata02:8042/node/allApplications】
10000： HiveServer2 
2181 ：客户端连接zookeeper的端口 
16010：Hbase (HMaster)的WEB UI端口  【http://promaster02:16010/master-status】
16030：Hbase (RegionServer) 的WEB UI端口 【http://prodata02:16030/rs-status】
18080: Spark 的WEB UI端口 【http://promaster02:18080/】
18081: Spark2 的WEB UI端口 【http://promaster02:18081/】
11000 ：Oozie 【http://uatdata01:11000/oozie/?user.name=admin】
4040 ： spark Driver的WEB UI 端口  任务调度
4041 ： Spark2 Driver的WEB UI 端口  任务调度
8080 ： ambari UI地址 【10.0.24.117:8080】
8886 ： ambari Infra UI  【http://promaster02:8886/】
3000 ： ambari Metrics  UI  【http://promaster02:3000/】
16000: sqoop默认开放端口
80 ： nginx端口  【10.0.24.112:80】
22 ： sshd    
3306 ： mysql
18933 : prosto 连接以及UI 【jdbc:presto://lyzk01:18933/hive】
12345 : dolphinscheduler 连接以及UI 【http://lyzk01:12345/dolphinscheduler】
9990~9999 : dolphinscheduler 
8848:   nacos
8082:   sop后台管理系统
8083:   sop接口平台
8443:  Azkaban 0.1.0
8084:  InfluxDB1.2.0
3000:  Grafana7.1.5
9200:  Elasticsearch 7.9.1
5601:  Kibanna 7.9.1


5601,9200,3000,8084,8443,8083,8082,8848,12345,18933,3306,22,80,16000,8886,8080,4041,4040,11000,18081,18080,16030,16010,2181,10000,8042,8088,19888,50075,50070,9990~9999

大数据UAT集群（集群之间端口通信需全开放）
10.0.24.204
10.0.24.205
10.0.24.200
10.0.24.201
10.0.24.202
10.0.24.203
大数据正式环境集群（集群之间端口通信需全开放）
10.0.24.112
10.0.24.113
10.0.24.114
10.0.24.115
10.0.24.116
10.0.24.117
10.0.24.118
10.0.24.119
大数据测试集群（集群之间端口通信需全开放）
10.0.24.105
10.0.24.106
10.0.24.107
10.0.24.110
10.0.24.111

teblous 358971483@qq.com/Nan520ruili.

桂花路庙-公司路口北方向

125、
promaster01宕机：
Retrying connect to server: prodata03/10.0.24.118:8485. Already tried 21 time(s)

解决：发现是prodata03，DN存活，但是端口8485没有。
	promaster01 ，8020也没有。 两者无法相互访问 
	: 发现是 prodata03 的journalNode没起
	
	
promaster02 主机NN宕机：
2021-07-07 19:17:45,96	Waited 91063 ms (timeout=120000 ms) for aresponse for finalizeLogSegment(62190773-62190784)
	Exceptions so far: [10.0.24.112:8485: IPC's epoch 54 is less than the last promised epoch 55
2021-07-07 19:17:49,873	java.io.IOException: Timed out waiting 120000ms for a quorum of nodes to respond.
promaster02 下线

最新一次的：
2021-07-09 23:10:49,335 INFO  client.QuorumJournalManager (QuorumCall.java:waitFor(136)) - Waited 8003 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.0.24.117:8485]
2021-07-09 23:10:50,337 INFO  client.QuorumJournalManager (QuorumCall.java:waitFor(136)) - Waited 9004 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.0.24.117:8485]
2021-07-09 23:10:51,337 INFO  client.QuorumJournalManager (QuorumCall.java:waitFor(136)) - Waited 10005 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.0.24.117:8485]
2021-07-09 23:10:51,728 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(406)) - Took 10395ms to send a batch of 1 edits (314 bytes) to remote journal 10.0.24.112:8485
2021-07-09 23:10:51,742 INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1279)) - Ending log segment 62773820
2021-07-09 23:10:51,798 WARN  client.QuorumJournalManager (IPCLoggerChannel.java:call(388)) - Remote journal 10.0.24.117:8485 failedto write txns 62773830-62773830. Will try to write to this JN again after the next log roll.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): IPC's epoch 70 is less than the last promised epoch 71
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:428)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkWriteRequest(Journal.java:456)

	https://www.136.la/tech/show-280014.html
	-、【HDFS机制】：该问题属于hdfs对于脑列的异常保护，属于正常行为，不影响业务。
	　　1）ZKFC1对NameNode1(Active)进行健康检查，因为长时间监控不到NN1的回复，认为该NameNode1不健康
			，主动释放zk中的ActiveStandbyElectorLock，此时NN1还是active（因为zkfc与NameNode1连接异常，不能将其shutdown）。
		2）ZKFC2在zk中竞争到ActiveStandbyElectorLock，将NameNode2（原来的Standby）变成Active，同时会更新JN中的epoch使其+1。
	　　3）NameNode1（原先的Active）再次去操作JournalNode的editlog时发现自己的epoch比JN的epoch小1，促使自己重启，成为Standby NameNode。
	三.解决方案
		可以在core-site.xml文件中修改ha.health-monitor.rpc-timeout.ms参数值，来扩大zkfc监控检查超时时间。
<property>
	<name>ha.health-monitor.rpc-timeout.ms</name>
	<value>180000</value>
</property>

scp /usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml  root@promaster02:/usr/hdp/2.5.3.0-37/hadoop/conf/

	场景重现：
	promaster01 -NN的log：
		2021-10-27 22:25:20,000 INFO  client.QuorumJournalManager (QuorumCall.java:waitFor(136)) - Waited 36001 ms (timeout=120000 ms) for a response for finalizeLogSegment(125372206-125372807). 
			Exceptions so far: [10.0.24.117:8485: IPC's epoch 492 is less than the last promised epoch 493
				 at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:428)      
				 at org.apache.hadoop.hdfs.qjournal.server.Journal.finalizeLogSegment(Journal.java:569)
				 at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.finalizeLogSegment(JournalNodeRpcServer.java:172)
			 
	promaster01 -zkfc的log：	
		2021-10-27 22:24:38,143 WARN  ha.ActiveStandbyElector (ActiveStandbyElector.java:becomeActive(863)) - 
			Exception handling the winning of election
				org.apache.zookeeper.KeeperException$BadVersionException: 
				KeeperErrorCode = BadVersion for /hadoop-ha/promycluster/ActiveBreadCrumb
		...
		2021-10-27 22:24:39,468 INFO  ha.ZKFailoverController (ZKFailoverController.java:becomeStandby(484)) - ZK Election indicated that 	NameNode at promaster01/10.0.24.113:8020 should become standby
		2021-10-27 22:24:44,481 ERROR ha.ZKFailoverController (ZKFailoverController.java:becomeStandby(492)) - Couldn't transition NameNode at promaster01/10.0.24.113:8020 to standby state
		java.net.SocketTimeoutException: Call From promaster01/10.0.24.113 to promaster01:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.0.24.113:36041 remote=promaster01/10.0.24.113:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	
	原因定位； zkfc日志：5000 millis timeout while waiting for channel to be ready for read. 
	最终解决：参考《三.解决方案》。修改ha.health-monitor.rpc-timeout.ms ， 从5000ms设置为18000ms
	
	
	其他解释：https://www.jianshu.com/p/603c4ae37c7c
	原因：
		也就是说，在切主时，会调用createNewUniqueEpoch方法，增加lastPromises的值。	那也就好解释了，由于SNN切主没成功，原ANN先是transitionToStandby。然后再transitionToActive。这样lastPromisedEpoch又+1。然后SNN在状态变为Active的那短暂时间往journal node写edit log失败，遂挂掉。

	解决：切主不能restart zkfc，要用stop zkfc，然后观察状态，切成功之后再启动刚才关闭的ZKFC。
	
		
126、mysql
 show variables like 'max_allowed_packet'

修改方法1(配置文件持久化修改)：
vim /etc/my.cnf
[mysqld]
max_allowed_packet = 100M

方法2：
set global max_allowed_packet = 10 * 1024 * 1024

但是重启Mysql还是会恢复到默认值,所以需要写在配置文件当中

注意：

1.命令行修改时，不能用M、G，只能这算成字节数设置。配置文件修改才允许设置M、G单位。

2.命令行修改之后，需要退出当前回话(关闭当前mysql server链接)，然后重新登录才能查看修改后的值。通过命令行修改只能临时生效，下次数据库重启后又复原了。

3.max_allowed_packet 最大值是1G(1073741824)，如果设置超过1G，查看最终生效结果也只有1G。
――――――――――――――――
版权声明：本文为CSDN博主「y41992910」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/y41992910/article/details/88788778


127、
1-、红本商品房
2-、商业房贷款年限不能超过10年
3-、不能使用住房公积金进行贷款
4-、商业房的土地出让年限只有40年，而住宅房的土地出让年限是70年【住宅到期自动续签， 商品房的目前不收费，以后不确定】
5-、商用房的水、电、物业费都是按商业标准收取，高于住宅房的收费标准 【电费和水费和居民比，会鬼50%以上。】
6-、商业房即使用于居住时也是不能办理落户手续的，而住宅房可以办理落户手续

DBeaver:
regexp_replace('[1,2]','\\[|\\]','') as badnum,al.badkey

hive -e "
regexp_replace(E.FSimpleName,'\\t|\\\\^|\"|\'|\\n|#','') 
"


127.1 、
	1-、每个图表的维度（年月周日等）
	2-、存储过程需要标明，
		-、参数（类型、含义、取值范围）
		-、返回字段->需要标注中文注释
	3-、sql语句返回字段->需要标注中文注释
	4-、每个图表，需要用到哪些DB，需要标注。


128、2021-07-26 study 
	0-、前中后
	参考资料：https://blog.csdn.net/u013834525/article/details/80421684
	1-、二叉查找树
	参考资料：https://blog.csdn.net/qq_40693171/article/details/99699862
		findMax->递归实现
		insert，iscontains， -> while实现
			http://c.biancheng.net/view/3431.html
	-、平衡二叉树	https://blog.csdn.net/qq_25940921/article/details/82183093?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-2.control

	2-、红黑数
	参考资料：https://www.jianshu.com/p/e136ec79235c
	
	
3、网易公开课  - 算法导论
https://open.163.com/newview/movie/courseintro?newurl=%2Fspecial%2Fopencourse%2Falgorithms.html
	
129、tebleau账号：358971483@qq.com/Nan520ruili.
龙腾烈火
表面抗原->阴性
e抗原->阴性
e抗体->阳性

1、关于用户使用tebleau desktop生成的dashboard如何publish到server的流程？有如下问题：
	1-、需要我们后台开发安装tebleau的什么服务？
	2-、需要如何操作？
2、麻烦发一下tebleau server linux版本的安装包以及安装教程。

TCX5-8AD5-A390-FCB8-822F


型号：美的KF-23GW/WDAD3@
能效：3级
输入功率（W）：725
制冷量（W）：2390
生产日期:2017-03

美的冰箱服务流程:（电话预约）――（上门检查）――（制定维修方案）――（确定维修价格）――（ 开始维修）――（维修后开保修）――（单建立维修档案）――（电话回访）
美的冰箱售后维修中心全市统一售后维修服务网点。 24小时免费热线400-8788-945
 400-8899-315

报装报修，可以关注“美的商城”公众号，点击右下方菜单栏“我的服务--报装报修”，
进入界面后填写您的详细信息，提交后24小时内会有工作人员电话联系您预约服务时间，请保持手机畅通哦~~


130、生产智造3.8看板

对应库：
			新厂-- CNCPROMQ
			中炎-CNCPROZhongYan
			硅胶--CNCPROGJ
			直供-- CNCPROZG
			印刷 --CNCPROPRINT
			壳厂 --CNCPROXM


10.16.3.50-CNCPROXM
10.16.3.50
CNCPROXM
cncpro
cncpro

/*
1 -- pcg达成率 PCG达成率趋势图   (每周，每月累计达成率>= 95% 即为达成数量 ,除以 对应的总数)   【显示计算最近半年的数据】
	
	生成表的列名：
		EntDate 日期
		AchievementRate 手工PCC达成率
	参数：
			@BegDate  C开始时间
			@EndDate  C结束时间
*/
EXEC dbo.T_HandCompletion_Kan  '2021-05-21','2021-07-22'
/*
2 -- 不良数据明细   【显示计算昨天的数据】
	字段名： 

		料号:  c_ProductName
		机台号: c_EquipmentNum
		检测时间：c_Provingtime
		不良原因:c_Unphenomenon
		处理方式:c_Processmode
	参数：
			@BegDate  C开始时间
			@EndDate  C结束时间
*/
EXEC dbo.T_BadInfoList_Kan  '2021-05-21','2021-07-22'

/*
3 -- 机器生产进度      【显示计算最近一个月的数据】  ->   改成默认显示昨天?

	字段：
	MachID 机台名
	Schedule  机器生产进度
	参数：
	@BegDate  C开始时间
	@EndDate  C结束时间

*/
EXEC dbo.T_MachineProduction_Kan '2021-05-21','2021-05-22'

/*
4 -- 每月员工约定产能   【显示计算最近一个月的数据】

	字段名：
							序号                      
		EmpNo    --工号 
	EmpName  --姓名
	YDQty    --产量(K)  
	YDHourQty (PCS/H)   -- 约当效率(K/H)  /1000 
		
	参数：
	@BegDate  C开始时间
	@EndDate  C结束时间


*/
EXEC dbo.T_HandDaliyRecord_Kan '2021-05-21','2021-05-22'
/*
5-模切 UDH  【显示计算最近一个月的数据】

	字段名:
		机台编号：MachID
		数量：Qty

	参数：
		@BegDate  C开始时间
		@EndDate  C结束时间
*/
EXEC dbo.ModuleUPH_Kan  '2021-05-21','2021-05-22'

/*
6-设备状态分析-模切报数机台数量/机台表总数量   【取昨天的数据】	  
	
	字段：
		a 百分比
	参数：
		@BegDate  C开始时间
		@EndDate  C结束时间

*/
EXEC dbo.T_MachineStatus_Kan '2021-05-21','2021-05-22'


/*
7-手工UPH   【显示计算最近一个月的数据】
	
	字段名：
	Line 线别
	MUPH 手工UPH(PCS)

	参数：
	@BegDate  C开始时间
	@EndDate  C结束时间


*/
EXEC dbo.T_HandfInfoUPH_Kan '2021-05-21','2021-05-22'

/*
8-手工效率排行榜    【显示计算最近一个月的数据】  改为 【默认显示昨天】

	参数：
		@BegDate  开始时间
		@EndDate  结束时间
		@EmpNo    工号
		@TeamID   组别ID
		@ProdNo   料号

	字段名：
		EmpNo   工号
		EmpName 姓名
		TeamID  组别
		ProdNo  品名
		YDHourQty --约当产能(K/H)    10* a.Qty / FmanualEfficiency* (a.WorkTime- a.DeductTime)

*/

EXEC dbo.T_HandEfficiencyRank_Kan '2021-05-21','2021-05-22'

修改：
1、字段：EndDate,EmpNo,EmpName,TeamID,YDQty(产量),WorkTime(工时H)，LabYueDProduc （K/H）
2、EXEC dbo.T_HandEfficiencyRank_Kan '2021-11-08,'2021-11-08' 【按天抽取，】
3、计算每个月约当产能逻辑
	1、EmpNo,EmpName,TeamID分组
	2、YDQty聚合，WorkTime聚合，LabYueDProduc=YDQty聚合/WorkTime聚合 * 1000
	
Qestion:、
	1、页面展示，添加日期字段，显示每一天的约当值。 还是现有不显示日期字段，显示最近一个月的汇总约当值。


2021-11-09 修改；
	1、物料库存排行， 料号只取中间6位
	2、手工效率排行榜， 【默认显示昨天】


Q:

1、PCG达成率
	-、添加模切存储过程
	-、维度是按月维度，
	
2、不良数据明细， 
	-、新增字段 date=xxxx-xx-xx

3、每月员工约定产能
	-、新增字段 date=xxxx-xx-xx
4、机器生产进度：
	-、新增字段 date=xxxx-xx-xx
5、模切UPH ：
	-、新增字段 date=xxxx-xx-xx
6、设备状态分析-模切报数机台数量/机台表总数量
	-、每个DB库是否合并?
	
	-、若合并，需要返回字段， 分子，分母， date
7、手工UPH,模切UPH,  
	-、当前图标，显示时间维度是什么？
		-、按天

8、EXEC dbo.T_HandEfficiencyRank_Kan '2021-05-21','2021-05-22'
提示没有返回结果集，有可能是返回字段有问题。


new mession - 2021-10-22
	
1、不良数据明细 图表 ->  改为 《设备利用率》,《设备稼动率及良率》
http://10.0.8.11/TYAOI/UtilizationIndex

2、机器生产进度      【显示计算最近一个月的数据】  ->   改成默认显示昨天?


新增需求：

1、 库存周转率
抽数：10.16.3.50， CNCPROMQ,CNCPROXM
      Wms_Pda_OutStorage_Goods   出库表  -> 按照ScanTime增量抽取
    根据日期，当天库存/当天出库数

question：
	1、计算库存数，是否需要去重？  -> 不需要
	2、只计算当天的库存周转率？   -> 可以
	3、库存周转率逻辑当天库存/当天出库数 ，有疑问，不应该是 当天出库数/ 当天库存？
	4、出库数据，只取CNCPROMQ,CNCPROXM这两个库？
	5、当天库存，和当天出库数，是指的StorageTime ，ScanTIme,取出来的条数是吧 ？
	表字段：
		日期 ，当天库存，当天出库数，当天周转率。
	
	更新周期：	-、按天更新

2、物料库存量排行
抽数：10.16.3.50， CNCPROMQ,CNCPROXM
  Profession_MaterialsTock   库存表   --> 每天抽取全量
  按数量排序
  
  question：
	1、按照Qty，排序后， 显示那几个字段 ->  物料id，数量
		
	更新周期：	-、按天更新
	
3、料件占比
抽数：10.16.3.50， CNCPROMQ,CNCPROXM
  Profession_MaterialsTock   库存表
过期料        ValidTime<当前日期的数据量 /当前物料库存表数据量 
呆滞料（90）  当前日期 > StorageTime+ 90天的数据量 /当前库存表数据量

	字段：  计算时间，过期料数量，呆滞料数量，库存总量，过期占比，呆滞占比，
	更新周期：-- 、按天更新

4、华南出货数据统计 
	Select  Sum(c.Weight) 重量总数,Sum(c.NumberOfPlates) 板数总数,Sum(IFNULL(c.NumberOfPCS,0)+IFNULL(c.SNumberOfPCS,0)) 箱数总数,Sum(c.Volume) 体积总数,Count(DISTINCT d.DispatchNo) 出车次数,Count(DISTINCT b.CustomerName) 客户总数,Sum(Case when f.RtMileage is null Then 0 else IFNULL(f.RtMileage,0)-IFNULL(f.GoMileage,0) end) 里程总数
from tms_bss_launch a 
left join tms_bss_launchsub b on a.LaunchNo=b.LaunchNo
Left join tms_bss_launchdetail c on b.GuidNo=c.LaunchSubNo
Left join tms_bss_dispatchlaunch d on b.GuidNo=d.LaunchSubNo
Left join tms_bss_dispatch e on d.DispatchNo=e.DispatchNo
left join tms_bss_mileage f on e.DispatchNo=f.DispatchNo
where d.LaunchSubNo is not null and a.FactoryName like '%平湖%' and e.StarTime>='2021-9-12 00:00:00' and e.StarTime<='2021-10-12 23:59:59'


5、稼动率，利率，良率

-- F运行总时间[RunTime]（单位s），基台个数[Cnt]，吃饭时间[Bad4]，良率[yield]。
-- 计算逻辑：AOI稼动率计算逻辑：
	
--  runtime /( 获取当前时间 - 吃饭时间) * 100 

-- runtime / (（当前时间-上班时间）*开机数量-吃饭时间 )  * 100 

-- 	
平湖一厂
平湖二厂
平湖三厂
平湖六厂

EXEC dbo.GetAOIYieldAndUti '2021-10-13', '平湖一厂' 

连接信息：
	数据源：AOI稼动率及良率  【ODS_GET_AOIYIELD_AND_UTI】
	数据库：10.16.3.50
	存储过程：GetAOIYieldAndUti
		厂区：平湖一厂
		库名：CNCPROMQ
	
		厂区：平湖二厂,平湖六厂
		库名：CNCPROZhongYan
	
		厂区：平湖三厂
		库名：CNCPROXM
	
	
逻辑：	开机数量/AOI数量  
	
	数据源：AOI利用率
	数据库：192.168.3.52
	库名：CNCPROCommon
	存储过程：GetT_MES_AOIUtilizationByTime


/**


-- 当天出库数
select count(1) from Wms_Pda_OutStorage_Goods
where ScanTime BETWEEN '2021-10-11 00:00:00' and '2021-10-11 23:59:59' 

-- 当天库存
select count(1) from Profession_MaterialsTock where StorageTime between '2021-10-11' and '2021-10-11'



select count(1) from (select distinct MaterieId  from Wms_Pda_OutStorage_Goods
where ScanTime BETWEEN '2021-10-11 00:00:00' and '2021-10-11 23:59:59') A 

select top 10 * from Profession_MaterialsTock 



select * from Profession_MaterialsTock where StorageTime between '2021-10-11' and '2021-10-11'
select * from Wms_Pda_OutStorage_Goods
where ScanTime BETWEEN '2021-10-11 00:00:00' and '2021-10-11 23:59:59'

**/
-、kettle，onefloor新增报表抽取时间整理：
	-、 默认每天凌晨00:30 左右抽取，昨天数据
		华南出货-> :
			ods_onefloor_mysql_job.sh  
		物料库存量排行、物料占比 报表
			ods_onefloor_material_stock_cncproxm_job.sh
			ods_onefloor_material_stock_cncpromq_job.sh 
	-、 默认每天早上08:10左右抽取，昨天数据
		-- 【默认抽取昨天数据，因夜班数据截止到第二条早上8点，故抽取时间定在每天早上08:10】
		AOI稼动率及良率  -> :
			ods_onefloor_cncpromq_pinghu_onefactory_job.sh 
			ods_onefloor_cncprozhongyan_pinghu_two_or_six_factory_job.sh  
			ods_onefloor_cncproxm_pinghu_three_factory_job.sh   
			ods_onefloor_cncprozhongyan_pinghu_six_factory_job.sh  
		利用率报表 -> :
			ods_onefloor_cncprocommon_job.sh 
-、基础表-> dwservice
	-、默认每天早上08:10左右执行，昨天数据 
		onefloor_report_add.sh  [上级目录：\data_warehouse\branches\06-IdeaProjects\test_lydwetl\shell\hive\dwservice\onefloor\]
		
-、dwservice -> phoenix
dws_mes_by_material_propor.sh -> 物料占比   [ 写入service之后，每天凌晨00:35 左右]
dws_mes_by_material_stock_rank.sh  -> 出货排行  [写入service之后，每天凌晨00:35 左右]
dws_mes_by_stock_turnover_rate.sh  -> 周转率   [写入service之后，每天凌晨00:35 左右]
dws_mes_by_utilization_yield_aoi.sh -> 稼动率，良率，利用率   [写入service之后，每天凌晨 08:20 左右]
dws_tms_out_stock_statis.sh -> 华南出货  [写入service之后，每天凌晨00:35 左右]





1 -- pcg达成率 PCG达成率趋势图    【显示计算最近半年的数据】

2 -- 机器生产进度    【  改成默认显示昨天】

3 -- 每月员工约定产能   【显示计算最近一个月的数据】

4 -- 模切 UDH  【显示计算最近一个月的数据】

5 -- 设备状态分析-模切报数机台数量/机台表总数量   【取昨天的数据】	  

6 -- 手工UPH   【显示计算最近一个月的数据】

7 -- 手工效率排行榜    【显示计算最近一个月的数据】

8 -- 设备利用率/稼动率/良率   【取昨天的数据】	

9 -- 库存周转率    【计算每天的周转率，页面只显示昨天周转率】

10 -- 物料库存量排行 【历史库存所有数据】

11 -- 料件占比 【历史库存所有数据 】 
	（
		过期料        ValidTime<当前日期的数据量 /当前物料库存表数据量 
		呆滞料（90）  当前日期 > StorageTime + 90天的数据量 /当前库存表数据量
	 ）

12 -- 华南出货数据统计  【显示计算最近一个月的数据】

133、
	mes模切需求：
		1、每小时抽取一次，按照创建日期抽取
		2、多个库数据合并
		
对应库：
			新厂-- CNCPROMQ
			中炎-CNCPROZhongYan
			硅胶--CNCPROGJ   [没有后两个表  【Wms_Pda_InStorage_Goods（入库记录表） 和 Wms_Pda_OutStorage_Goods（出库记录表）】]
			直供-- CNCPROZG   [没有后两个表  【Wms_Pda_InStorage_Goods（入库记录表） 和 Wms_Pda_OutStorage_Goods（出库记录表）】]
			印刷 --CNCPROPRINT
			壳厂 --CNCPROXM

对应表：		
		
t_mes_labelprintrcd（标签打印记录表）：【T_MES_LABELPRINTRCD】
ProdNo(内部料号)，CustProdNo（客户料号），ProdDesc（物料描述），LotNo（批次），RevNo（版本），Qty（数量），Fnumber（唯一码），BoxQty（外箱数量），LimitDate（有效期），PrintDate（生产日期），FManuOrder（工单号）

		select '${fixed_curtime}'  data_loadtime,ProdNo,CustProdNo,ProdDesc,LotNo,RevNo,Qty,Fnumber,BoxQty,
		LimitDate,PrintDate,FManuOrder from dbo.t_mes_labelprintrcd ,CreateTime
		where CreateTime between '2021-08-06 22:16:51' and  '2021-08-06 22:16:51'

Wms_Pda_InStorage_Goods（入库记录表）: 【WMS_PDA_INSTORAGE_GOODS】
MaterieId（内部料号）,Spec（规格）  LotNumber（批次）  StorageTime（生产日期）  ValidTime（有效日期）  Fnumber（唯一码）  Qty（数量） DeliveryNo（入库单据号）,InStorageType（入库类型）,MaterialCode（物料标签码）,RepertoryId（仓库ID）,ScanTime

	select '${fixed_curtime}'  data_loadtime,MaterieId,Spec,LotNumber,StorageTime,ValidTime,Fnumber,Qty,DeliveryNo ,InStorageType,MaterialCode,RepertoryId,ScanTime
	from dbo.Wms_Pda_InStorage_Goods
	where ScanTime between '2021-08-06 22:16:51' and  '2021-08-06 22:16:51'

Wms_Pda_OutStorage_Goods（出库记录表）： 【WMS_PDA_OUTSTORAGE_GOODS】
MaterieId（内部料号）  Spec（规格）  LotNumber（批次）  StorageTime（生产日期）  ValidTime（有效日期）  Fnumber（唯一码）  Qty（数量）  DeliveryNo（出库单据号），OutStorageType（出库类型），MaterialCode（物料标签码）  RepertoryId（仓库ID） ,ScanTime

	select '${fixed_curtime}'  data_loadtime, MaterieId,Spec,LotNumber,StorageTime,ValidTime,Fnumber,Qty,DeliveryNo ,OutStorageType,MaterialCode,RepertoryId,ScanTime
	from dbo.Wms_Pda_OutStorage_Goods
	where ScanTime between '2021-08-06 22:16:51' and  '2021-08-06 22:16:51'

Kettle参数列表：
	fixed_curtime	[2021-09-01 09:16:51]
	sys_day_hour	[2021-09-01_09]
	db_name			[CNCPROMQ]



/**
select ProdNo,CustProdNo,ProdDesc,LotNo,RevNo,Qty,Fnumber,BoxQty,LimitDate,PrintDate,FManuOrder from dbo.t_mes_labelprintrcd 
where CreateTime > '2021-08-06 22:16:51'


select top 10 * from dbo.t_mes_labelprintrcd 
select top 10 * from  dbo.Wms_Pda_InStorage_Goods 
select top 10 * from  dbo.Wms_Pda_OutStorage_Goods 

select top 10 * from dbo.t_mes_labelprintrcd 
select top 10 * from  dbo.Wms_Pda_InStorage_Goods 
select top 10 * from  dbo.Wms_Pda_OutStorage_Goods 


select MaterieId,Spec,LotNumber,StorageTime,ValidTime,Fnumber,Qty,DeliveryNo ,InStorageType,MaterialCode,RepertoryId,ScanTime
from dbo.Wms_Pda_InStorage_Goods 

**/

130、

-- 特城nginx.conf
修改了/usr/local/nginx/conf/nginx.conf文件：
1、93行，去掉了proxy_conect_timeout 10;
2、119行，去掉了proxy_conect_timeout 10;


131、爬爬垫选择
	1-、材质：XPE
	2-、品牌：选择大品牌
		曼波鱼屋，费雪，Parklon帕克伦，伊彩华特-彩田
	3-、厚度：选择1.5CM
	4-、透气性：脚微汗踩，粘脚则透气性差
	5-、颜色:清晰，不鲜艳，不反光
	6-、形态：折叠式[宝宝容易扣连接处损坏]或一体成型式[不利于收纳]。
	7-、尺寸：1.8*2
	王慕涵
		1、老爸爱慕老妈
		2、做一个有涵养的人
		3、有草吃，有水喝的牛=衣食无忧

132、
		hive.auto.convert.join=true


		https://www.cnblogs.com/tijun/p/7605430.html

		基于CDH5.x的Hive权限配置

		1、打开权限控制，默认是没有限制的

		set hive.security.authorization.enabled=true;
		2、配置默认权限

		1 hive.security.authorization.createtable.owner.grants = ALL  
		2 hive.security.authorization.createtable.role.grants = admin_role:ALL  
		3 hive.security.authorization.createtable.user.grants = user1,user2:select;user3:create
		在Cloudera Manager中配置

		hive-site.xml 的 Hive 客户端高级配置代码段（安全阀）
		hive-site.xml 的 HiveServer2 高级配置代码段（安全阀）

		复制代码

		  
			hive.security.authorization.enabled  
			true  
		  
			
			hive.security.authorization.createtable.owner.grants    
			ALL  
		  
			
			hive.security.authorization.task.factory    
			org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl  
		复制代码

		 

		3、创建角色

		CREATE ROLE test_role;
		 

		分配用户到角色

		GRANT ROLE test_role TO USER mllib;
		 

		如出现FAILED: SemanticException The current builtin authorization in Hive is incomplete and disabled.这个异常。

		解决方案:

		配置

		 

		set hive.security.authorization.task.factory = org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl;
		 

		4、分配权限

		基于角色:

		 

		GRANT CREATE ON DATABASE default TO group test_role;  
		GRANT SELECT on table authorization_test to group test_role;  
		GRANT DROP on table authorization_test to group test_role;  
		GRANT ALL on table authorization_test to group test_role;
		 

		基于用户:

		GRANT CREATE ON DATABASE default TO user mllib;  
		GRANT SELECT on table authorization_test to user mllib;  
		GRANT DROP on table authorization_test to user mllib;  
		GRANT ALL on table authorization_test to user mllib;
		 

		分配创建数据库的权限

		GRANT CREATE  TO user root;
		 

		5、查看权限分配

		 

		SHOW GRANT user mllib ON DATABASE default;     
		SHOW GRANT group test_role ON DATABASE default;
		 

		6、删除权限

		revoke all on database spark from user mllib;
		 

		7、HIVE支持以下权限：
		权限名称 含义
		ALL      :  所有权限
		ALTER  :  允许修改元数据（modify metadata data of object）---表信息数据
		UPDATE  :  允许修改物理数据（modify physical data of object）---实际数据
		CREATE  :  允许进行Create操作
		DROP  :  允许进行DROP操作
		INDEX  :  允许建索引（目前还没有实现）
		LOCK  :  当出现并发的使用允许用户进行LOCK和UNLOCK操作
		SELECT  :  允许用户进行SELECT操作
		SHOW_DATABASE : 允许用户查看可用的数据库


		8、登录hive元数据库，可以发现以下表:
		Db_privs:记录了User/Role在DB上的权限
		Tbl_privs:记录了User/Role在table上的权限
		Tbl_col_privs：记录了User/Role在table column上的权限
		Roles：记录了所有创建的role
		Role_map：记录了User与Role的对应关系


134、 
Size of hash cache (104857670 bytes) exceeds the maximum allowed size (104857600 bytes)
调整 phoenix.query.maxServerCacheBytes大小，在CDH Hbase-site.xml 客户端设置即可，默认100MB

注意事项：必须在客户端添加才生效，服务器添加不生效
<property>
      <name>phoenix.query.maxServerCacheBytes</name>
      <value>1572864000</value>
</property>

scp /usr/hdp/2.5.3.0-37/hbase/conf/hbase-site.xml  root@lymaster01:/usr/hdp/2.5.3.0-37/hbase/conf/



135、hbase  java heap space

修改 hbase-env.sh
export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmn1632m -XX:CMSInitiatingOccupancyFraction=70  -Xms8192m -Xmx8192m $JDK_DEPENDED_OPTS"

export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xmn1632m -XX:CMSInitiatingOccupancyFraction=70  -Xms20480m -Xmx20480m $JDK_DEPENDED_OPTS"

vim /usr/hdp/2.5.3.0-37/hbase/conf/hbase-env.sh
:%s/8192m/20480m/g 

:%s/20480m/30720m/g
:%s/1024m/30720m/g 

/**
		日期格式，中间有空格的，需要加双引号，在sh脚本中
#begin_time  2021-09-06 11:00:00
#end_time  2021-09-10 13:59:59
#db_name  CNCPROMQ
#sys_day  2021-09-10
#sys_hour  13

if [[ x"$2" = x && x"$3" = x && x"$4" = x && x"$5" = x ]] ||  [[ x"$2" != x && x"$3" != x && x"$4" != x && x"$5" != x ]] 

**/
## 校验年月日，时分秒格式日期
function isValidDateByHourSec(){
        date -d "$1" "+%Y-%m-%d %H:%M:%S" |grep -q "$1" 2>/dev/null
        if  [ $? = 1 ]; then
                  date -d "$1" "+%Y%m%d %H:%M:%S" |grep -q "$1" 2>/dev/null
                  if [ $? = 1 ]; then
                       return 1
                  else
                       return 0
                  fi
        else
            return 0
        fi
}
echo "输入的第3个参数："$3
## 校验参数3，begin_time
if [ $# -gt 2 ];then
    # 有输入日期参数，检出参数是否合法
    isValidDateByHourSec "$3"
    if [ $? = 1 ]; then
        echo "invalid begin_time format is yyyy-MM-dd HH:MM:ss or yyyyMMdd HH:MM:ss"
        exit 1
    else
        begin_time=`date -d "$3" +"%Y-%m-%d %H:%M:%S"`
    fi
else
    # 若无输入日期参数，默认为当前日期前一小时，，临界开始日期
    begin_time=`date -d "1 hour ago" +"%Y-%m-%d %H:00:00"`
fi
echo "begin_time:"$begin_time

echo "输入的第4个参数："$4
## 校验参数4，end_time
if [ $# -gt 3 ];then
    # 有输入日期参数，检出参数是否合法
    isValidDateByHourSec "$4"
    if [ $? = 1 ]; then
        echo "invalid end_time format is yyyy-MM-dd HH:MM:ss or yyyyMMdd HH:MM:ss "
        exit 1
    else
        end_time=`date -d "$4" +"%Y-%m-%d %H:%M:%S"`
    fi
else
    # 若无输入日期参数，默认为当前日期前一小时，临界结束日期
    end_time=`date -d "1 hour ago" +"%Y-%m-%d %H:59:59"`
fi
echo "end_time:"$end_time

echo "输入的第5个参数："$5
## 参数5，hour数值校验，
if echo $5 | grep -q '[^0-9]'
then
        echo "this is not a num,please input num"
else
        if [ $5 -ge 0 ] && [ $5 -le 23 ]
        then
            sys_hour=$5
        else
             echo "this is not a hour num,please input hour num"
        fi
fi
echo "sys_hour:"$sys_hour

/**
end
**/

136、
vfs2.FileSystemException: Could not create folder "hdfs://mycluster:8020/
背景：hadoop集群配置了HA，使用命名空间来连接操作hadoop
Hadoop clusters 配置的Hadoop，HDFS- Houstname为mycluster，Port配置为空【如果不为空，前面的Houstname就不是表示HA的命名空间，而是表示域名或IP地址，就没使用到Hadoop HA连接了。这一点非常重要！！】

作业执行到"EAS_VOUCHERTYPES 采集到HDFS.0"转换【该转换需要连接hadoop创建目录和文件】时报错
2019/01/19 10:17:22 - EAS_VOUCHERTYPES采集到HDFS.0 - ERROR (version 8.2.0.0-342, build 8.2.0.0-342 from 2018-11-14 10.30.55 by buildguy) : org.pentaho.di.core.exception.KettleException:
2019/01/199 10:17:22 - EAS_VOUCHERTYPES 采集到HDFS.0 - Error opening new file : org.apache.commons.vfs2.FileSystemException: Could not create folder "hdfs://mycluster:8020/".
2019/01/19 10:17:22 - EAS_NEW_EAS75_T_BD_VOUCHERTYPES 采集到HDFS.0 -
2019/01/19 10:17:22 - EAS_VOUCHERTYPES采集到HDFS.0 -    at org.pentaho.di.trans.steps.textfileoutput.TextFileOutput.initFileStreamWriter(TextFileOutput.java:235)
2019/01/19 10:17:22 - EAS_VOUCHERTYPES采集到HDFS.0 -    at org.pentaho.di.trans.steps.textfileoutput.TextFileOutput.initOutput(TextFileOutput.java:866)
2019/01/19 10:17:22 - EAS_VOUCHERTYPES采集到HDFS.0 -    at org.pentaho.di.trans.steps.textfileoutput.TextFileOutput.init(TextFileOutput.java:837)
2019/01/19 10:17:22 - EAS_VOUCHERTYPES采集到HDFS.0 -    at org.pentaho.di.trans.step.StepInitThread.run(StepInitThread.java:69)

原因分析：
$KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp26 下的core-site.xml替换成hadoop集群的文件，其它都是默认文件。
后面发现hadoop集群中的hdfs-site.xml文件里也包含了Hadoop HA 命名空间的配置信息，所以将hadoop集群中的hdfs-site.xml文件拷贝到$KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp26目录下，问题得以解决。
解决：
kettle 连接Hadoop HA，需要拷贝hadoop集群中的hdfs-site.xml、core-site.xml文件到
$KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp26目录下


本机电脑错误原因分析：
	1、plugins下已有对应的hdfs-site.xml,core-site.xml,但不是测试环境的。
	

137、
1、替换已打好的jar包中的依赖lib，jar
	-、修改BOOT-INF/下，jar的启动文件，增加或者替换原有的版本号
	-、修改BOOT-INF/lib/下，放入新增或者修改的jar报。

2、错误：
Caused by: java.lang.IllegalStateException: Unable to open nested entry '****/lib/***-1.0-SNAPSHOT.jar'. It has been compressed and nested jar files must be stored without compression. Please check the mechanism used to create your executable jar file
解决：jar替换的时候，选择store不二次压缩。
https://www.jianshu.com/p/9df4a0bb46be



138、
1、lymaster02 NameNode 报错：
java.net.SocketTimeoutException: Call From lymaster02/10.0.24.106 to lymaster01:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.0.24.106:58698 remote=lymaster01/10.0.24.105:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout

可能解决： 修改hdfs-site.xml ，socket的超时时间，
参考资料：
https://blog.csdn.net/dehu_zhou/article/details/81533802
https://blog.csdn.net/gangchengzhong/article/details/54861082


2、lymaster02 ； NameNode 报错：
2021-11-01 09:26:24,996 WARN  client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 89069 ms (timeout=90000 ms) for a response for selectInputStreams. Succeeded so far: [10.0.24.107:8485]
2021-11-01 09:26:25,927 WARN  namenode.FSEditLog (JournalSet.java:selectInputStreams(280)) - Unable to determine input streams from QJM to [10.0.24.107:8485, 10.0.24.110:8485, 10.0.24.111:8485]. Skipping.
java.io.IOException: Timed out waiting 90000ms for a quorum of nodes to respond.

尝试解决：
	1、发现《1、》中报错，
	2、hdfs的磁盘使用了达到了93%  -> 尝试清理磁盘空间。
	3、查看datanode的journalNode日志。
		2021-11-01 09:33:53,389 WARN  namenode.FSImage (EditLogFileInputStream.java:scanEditLog(359)) - Caught exception after scanning through 0 ops from /hadoop/hdfs/journal/mycluster/current/edits_inprogress_0000000000033650169 while determining its valid length. Position was 409600
		java.io.IOException: Can't scan a pre-transactional edit log.
		
		参考资料：https://blog.csdn.net/weixin_33728268/article/details/94046305
		清理磁盘空间。
		

134、promaster02 ，RM异常宕机
	RM日志分析：
		2021-11-09 22:19:57,796 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1140)) - Client session timed out, have not heard from server in 6670ms for sessionid 0x27cf613f93610c3, closing socket connection and attempting reconnect
		2021-11-09 22:19:57,839 INFO  ha.ActiveStandbyElector (ActiveStandbyElector.java:processWatchEvent(606)) - Session disconnected. Entering neutral mode...
		2021-11-09 22:19:57,897 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
			org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
				at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
				at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
				at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:915)	
		2021-11-09 22:19:57,897 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 1
		2021-11-09 22:19:58,137 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server prodata02/10.0.24.117:2181. Will not attempt to authenticate using SASL (unknown error)
		2021-11-09 22:19:58,138 INFO  zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to prodata02/10.0.24.117:2181, initiating session
		2021-11-09 22:19:58,140 INFO  zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server prodata02/10.0.24.117:2181, sessionid = 0x17cf62491701042, negotiated timeout = 10000
		2021-11-09 22:19:58,140 INFO  ha.ActiveStandbyElector (ActiveStandbyElector.java:processWatchEvent(595)) - Session connected.
		2021-11-09 22:19:58,141 INFO  ha.ActiveStandbyElector (ActiveStandbyElector.java:fenceOldActive(931)) - Checking for any old active which needs to be fenced...
		2021-11-09 22:19:58,142 INFO  ha.ActiveStandbyElector (ActiveStandbyElector.java:fenceOldActive(952)) - Old node exists: 0a0c7961726e2d636c75737465721203726d32
		2021-11-09 22:19:58,142 INFO  ha.ActiveStandbyElector (ActiveStandbyElector.java:fenceOldActive(954)) - But old node has our own data, so don't need to fence it.
		2021-11-09 22:19:58,142 INFO  ha.ActiveStandbyElector (ActiveStandbyElector.java:writeBreadCrumbNode(878)) - Writing znode 	/yarn-leader-election/yarn-cluster/ActiveBreadCrumb to indicate that the local node is the most recent active...
		2021-11-09 22:19:58,276 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server prodata03/10.0.24.118:2181. Will not attempt to authenticate using SASL (unknown error)
		2021-11-09 22:19:58,277 INFO  zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to prodata03/10.0.24.118:2181, initiating session
		2021-11-09 22:19:58,278 INFO  zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server prodata03/10.0.24.118:2181, sessionid = 0x27cf613f93610c3, negotiated timeout = 10000
		2021-11-09 22:20:01,905 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1142)) - Unable to read additional data from server sessionid 0x17cf62491701042, likely server has closed socket, closing socket connection and attempting reconnect
		2021-11-09 22:20:02,030 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server prodata04/10.0.24.119:2181. Will not attempt to authenticate using SASL (unknown error)
		2021-11-09 22:20:02,031 INFO  zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to prodata04/10.0.24.119:2181, initiating session
		2021-11-09 22:20:02,032 INFO  zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server prodata04/10.0.24.119:2181, sessionid = 0x17cf62491701042, negotiated timeout = 10000
		2021-11-09 22:20:04,948 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1140)) - Client session timed out, have not heard from server in 6669ms for sessionid 0x27cf613f93610c3, closing socket connection and attempting reconnect
		2021-11-09 22:20:05,049 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
			org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
				at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
				at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
				at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:915)
		2021-11-09 22:20:05,049 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 2
		2021-11-09 22:20:05,725 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server prodata02/10.0.24.117:2181. Will not attempt to authenticate using SASL (unknown error)
		2021-11-09 22:20:05,726 INFO  zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to prodata02/10.0.24.117:2181, initiating session
		2021-11-09 22:20:05,726 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1142)) - Unable to read additional data from server sessionid 0x27cf613f93610c3, likely server has closed socket, closing socket connection and attempting reconnect
		2021-11-09 22:20:05,827 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
		org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
				at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
				at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
		2021-11-09 22:20:05,827 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 3
		2021-11-09 22:20:06,310 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server prodata04/10.0.24.119:2181. Will not attempt to authenticate using SASL (unknown error)
		2021-11-09 22:20:06,310 INFO  zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to prodata04/10.0.24.119:2181, initiating session
		2021-11-09 22:20:06,312 INFO  zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server prodata04/10.0.24.119:2181, sessionid = 0x27cf613f93610c3, negotiated timeout = 10000
		2021-11-09 22:20:06,351 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(1140)) - Client session timed out, have not heard from server in 26671ms for sessionid 0x17cf6249170107a, closing socket connection and attempting reconnect
		2021-11-09 22:20:06,452 INFO  state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: SUSPENDED		
		2021-11-09 22:20:06,677 INFO  zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server prodata04/10.0.24.119:2181. Will not attempt to authenticate using SASL (unknown error)
		2021-11-09 22:20:06,678 INFO  zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(864)) - Socket connection established to prodata04/10.0.24.119:2181, initiating session
		2021-11-09 22:20:06,679 INFO  zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1279)) - Session establishment complete on server prodata04/10.0.24.119:2181, sessionid = 0x17cf6249170107a, negotiated timeout = 40000
		2021-11-09 22:20:06,680 INFO  state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: RECONNECTED
		2021-11-09 22:20:08,695 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(918)) - Watcher event type: None with state:Disconnected for path:null for Service org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore in state org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: STARTED
		2021-11-09 22:20:08,695 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(935)) - ZKRMStateStore Session disconnected
		2021-11-09 22:20:08,695 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(918)) - Watcher event type: None with state:SyncConnected for path:null for Service org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore in state org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: STARTED
		2021-11-09 22:20:08,695 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(935)) - ZKRMStateStore Session disconnected
		2021-11-09 22:20:08,695 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(918)) - Watcher event type: None with state:SyncConnected for path:null for Service org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore in state org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: STARTED
		2021-11-09 22:20:08,696 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(926)) - ZKRMStateStore Session connected
		2021-11-09 22:20:08,696 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:processWatchEvent(931)) - ZKRMStateStore Session restored
		2021-11-09 22:20:08,749 WARN  ha.ActiveStandbyElector (ActiveStandbyElector.java:becomeActive(863)) - Exception handling the winning of election
		org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /yarn-leader-election/yarn-cluster/ActiveBreadCrumb
				at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
				at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
		...
		2021-11-09 22:20:10,095 INFO  resourcemanager.ResourceManager (ResourceManager.java:transitionToStandby(1070)) - Transitioning to standby state
		2021-11-09 22:20:10,151 INFO  zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x17cf6249170107a closed
		2021-11-09 22:20:10,151 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(524)) - EventThread shut down
		2021-11-09 22:20:10,161 WARN  amlauncher.ApplicationMasterLauncher (ApplicationMasterLauncher.java:run(122)) - org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher$LauncherThread interrupted. Returning.
		2021-11-09 22:20:10,166 INFO  ipc.Server (Server.java:stop(2725)) - Stopping server on 8032
		2021-11-09 22:20:10,174 INFO  ipc.Server (Server.java:run(913)) - Stopping IPC Server listener on 8032
		2021-11-09 22:20:10,174 INFO  ipc.Server (Server.java:run(1050)) - Stopping IPC Server Responder
		2021-11-09 22:20:10,183 INFO  ipc.Server (Server.java:stop(2725)) - Stopping server on 8030
		2021-11-09 22:20:10,191 INFO  ipc.Server (Server.java:run(913)) - Stopping IPC Server listener on 8030
		2021-11-09 22:20:10,194 INFO  ipc.Server (Server.java:stop(2725)) - Stopping server on 8031
		2021-11-09 22:20:10,191 INFO  ipc.Server (Server.java:run(1050)) - Stopping IPC Server Responder
		2021-11-09 22:20:10,199 INFO  ipc.Server (Server.java:run(913)) - Stopping IPC Server listener on 8031
		2021-11-09 22:20:10,199 INFO  ipc.Server (Server.java:run(1050)) - Stopping IPC Server Responder
		2021-11-09 22:20:10,204 INFO  util.AbstractLivelinessMonitor (AbstractLivelinessMonitor.java:run(135)) - NMLivelinessMonitor thread interrupted
		2021-11-09 22:20:10,205 ERROR resourcemanager.ResourceManager (ResourceManager.java:run(700)) - Returning, interrupted : java.lang.InterruptedException
		2021-11-09 22:20:10,211 INFO  event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(142)) - AsyncDispatcher is draining to stop, igonring any new events.
		2021-11-09 22:20:10,215 INFO  util.AbstractLivelinessMonitor (AbstractLivelinessMonitor.java:run(135)) - AMLivelinessMonitor thread interrupted
		2021-11-09 22:20:10,216 INFO  util.AbstractLivelinessMonitor (AbstractLivelinessMonitor.java:run(135)) - org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer thread interrupted
		2021-11-09 22:20:10,216 ERROR delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:run(659)) - ExpiredTokenRemover received java.lang.InterruptedException: sleep interrupted
		
		参考资料：https://blog.csdn.net/lsr40/article/details/112346706
			可能是：  negotiated timeout = 10000 ，协商超时？
	
	重启promaster02 ，rm后， 01的rm宕机,日志如下：
	2021-11-10 22:09:28,640 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
		org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
        at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:915)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$5.run(ZKRM
	2021-11-10 22:09:28,640 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 1
	2021-11-10 22:09:28,687 INFO  integration.RMRegistryOperationsService (RMRegistryOperationsService.java:onApplicationAttemptUnregistered(107)) - Application attempt appattempt_1636468358132_3140_000001 unregistered, purging app attempt records
	2021-11-10 22:09:28,687 INFO  integration.RMRegistryOperationsService (RMRegistryOperationsService.java:purgeRecordsAsync(198)) -  records under / with ID appattempt_1636468358132_3140_000001 and policy application-attempt: {}
	...
	2021-11-10 22:09:36,218 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
	org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
	2021-11-10 22:09:36,219 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 2
	...
	2021-11-10 22:09:36,588 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
		org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
	2021-11-10 22:09:36,588 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1230)) - Retrying operation on ZK. Retry no. 3
	...
	2021-11-10 22:25:42,053 FATAL event.AsyncDispatcher (AsyncDispatcher.java:dispatch(190)) - Error in dispatcher thread
	java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@1401f218 rejected from java.util.concurrent.ThreadPoolExecutor@1efabe76[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 154]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	...
	2021-11-10 22:25:43,646 INFO  zookeeper.ClientCnxn (ClientCnxn.java:run(524)) - EventThread shut down
	2021-11-10 22:25:43,660 INFO  ipc.Server (Server.java:stop(2725)) - Stopping server on 8032
	2021-11-10 22:25:43,674 INFO  ipc.Server (Server.java:run(913)) - Stopping IPC Server listener on 8032
	2021-11-10 22:25:43,677 INFO  ipc.Server (Server.java:stop(2725)) - Stopping server on 8030
	2021-11-10 22:25:43,677 INFO  ipc.Server (Server.java:run(1050)) - Stopping IPC Server Responder
	2021-11-10 22:25:43,680 INFO  ipc.Server (Server.java:run(1050)) - Stopping IPC Server Responder
	2021-11-10 22:25:43,682 INFO  ipc.Server (Server.java:stop(2725)) - Stopping server on 8031
	2021-11-10 22:25:43,680 INFO  ipc.Server (Server.java:run(913)) - Stopping IPC Server listener on 8030
	2021-11-10 22:25:43,687 INFO  ipc.Server (Server.java:run(913)) - Stopping IPC Server listener on 8031
	2021-11-10 22:25:43,691 ERROR resourcemanager.ResourceManager (ResourceManager.java:run(700)) - Returning, interrupted : java.lang.InterruptedException
	...
	2021-11-10 22:25:52,058 WARN  util.ShutdownHookManager (ShutdownHookManager.java:run(70)) - ShutdownHook 'CompositeServiceShutdownHook' timeout, java.util.concurrent.TimeoutException
	java.util.concurrent.TimeoutException
			at java.util.concurrent.FutureTask.get(FutureTask.java:205)
			at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:67)
	2021-11-10 22:25:52,058 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service Dispatcher failed in state STOPPED; cause: java.lang.InterruptedException
	java.lang.InterruptedException
			at java.lang.Object.wait(Native Method)
	2021-11-10 22:26:32,061 ERROR util.ShutdownHookManager (ShutdownHookManager.java:run(81)) - ShutdownHookManger shutdown forcefully.
	
	尝试处理：
		1、2021-11-11 13:55:00  重启promaster01 ，RM
		
138、模切达成周报：
			大概6个表，共140个字段， 大概10个库
			
			
			
140、
step_start3.sh		 【-->>>>  FATP 生成 start   <<<<<----  ，。。 已包含在此脚本中】	
step_start4.sh
generate_cd_by_datasources_v3.sh
BBegin-&&&&&& &&&&&&&&&&&&&&
[@1 ~@16]


/**
when '1' then 'CNC'
when '2' then '冲压'
when '3' then '模切'
when '4' then '设备'
when '5' then 'softgoods'
when '6' then '深圳结构件'
when '7' then '东莞结构件'
when '8' then '显示模组'
when '9' then '磁材'
when '10' then '汇总'
when '12' then '通信事业部'
when '13' then '赛尔康'
when '14' then '设备自动化'
when '15' then '注塑'
when '16' then '无线充电'
when '17' then '马达'
when '18' then 'FATP'
when '19' then 'FATP-EQUIP'
when '20' then 'FATP-NOBOM'


**/

			
		
需求调整：
1、模切周报 ，稼动率周报，计算每周不分白晚班的汇总  , 2 ,3 4




111、phoenix 调整
将 phoenix.query.maxGlobalMemoryPercentage 调整为40%
<property>
      <name>phoenix.query.maxGlobalMemoryPercentage</name>
      <value>40</value>
</property>


112、phoenix到hive的映射

scp /usr/hdp/2.5.3.0-37/hadoop/conf/core-site.xml  root@lymaster02:/usr/hdp/2.5.3.0-37/hive/auxlib/

scp /opt/apache-phoenix-4.9.0-HBase-1.1-bin/phoenix-4.9.0-HBase-1.1-hive.jar root@promaster02:/usr/hdp/2.5.3.0-37/hive/auxlib/;
scp /opt/apache-phoenix-4.9.0-HBase-1.1-bin/phoenix-4.9.0-HBase-1.1-client.jar root@promaster02:/usr/hdp/2.5.3.0-37/hive/auxlib/;
scp /opt/apache-phoenix-4.9.0-HBase-1.1-bin/phoenix-core-4.9.0-HBase-1.1.jar root@promaster02:/usr/hdp/2.5.3.0-37/hive/auxlib/;


添加 ： phoenix-4.9.0-HBase-1.1-hive.jar  启动hive报错

cp /usr/hdp/2.5.3.0-37/hadoop-yarn/hadoop-yarn-api-2.7.3.2.5.3.0-37.jar  /usr/hdp/2.5.3.0-37/hive/auxlib/

cp /usr/hdp/2.5.3.0-37/hadoop-yarn/lib/protobuf-java-2.5.0.jar /usr/hdp/2.5.3.0-37/hive/auxlib/



-、修改hive-site.xml
<property>
  <name>hive.aux.jars.path</name>
<value>file:///usr/hdp/2.5.3.0-37/hive/auxlib/hive-contrib-1.2.1000.2.5.3.0-37.jar,file:///usr/hdp/2.5.3.0-37/hive/auxlib/phoenix-4.13.1-HBase-1.2-hive.jar</value>
</property>

-、修改hive-env.sh


cat /usr/hdp/2.5.3.0-37/hive/conf/hive-env.sh  | grep HIVE_AUX_JARS_PATH


查看进程启动时间： ps -p 124702 -o lstart

备注：
	1、记得删除其他节点 /usr/hdp/2.5.3.0-37/hive/auxlib/ 相关的phoenix的jar。

ls /usr/hdp/2.5.3.0-37/hive/auxlib/ | grep phoenix| xargs rm -rf 

locate('剥离性',QC.FCheckProject)

参考资料：https://blog.csdn.net/madongyu1259892936/article/details/88355257
示例：
CREATE TABLE DW.DWS_AREA (
	PKID VARCHAR NOT NULL,
	CREATE_TIME TIMESTAMP,
	UPDATE_TIME TIMESTAMP,
	AREA VARCHAR,
	DIM VARCHAR(128),
	COUNT_NUM DECIMAL,
	"DAY" VARCHAR,
	CONSTRAINT DWS_AREA_PK PRIMARY KEY (PKID)
);


create external table dwservice.dws_area (pkid string,area string,dim string,count_num DECIMAL(18,2))
STORED BY 'org.apache.phoenix.hive.PhoenixStorageHandler'
TBLPROPERTIES (
  "phoenix.table.name" = "DW.DWS_AREA",
  "phoenix.zookeeper.quorum" = "prozk01,prozk02,prozk03",
  "phoenix.zookeeper.znode.parent" = "/hbase",
  "phoenix.zookeeper.client.port" = "2181",
  "phoenix.rowkeys" = "i1",
  "phoenix.column.mapping" = "pkid:PKID, area:AREA, dim:DIM, count_num:COUNT_NUM"
);

ls /usr/hdp/2.5.3.0-37/hive/auxlib/ | grep phoenix  | xargs rm -rf




113、
hbase-site.xml 备份
主机：lymaster01 :
路径：/opt/hbase-site_backup/hbase-site.xml
建好索引报错，通过代码写入报错：
unable to find index cache
1、修改hbase配置
添加
<property>
		<name>phoenix.coprocessor.maxServerCacheTimeToLiveMs</name>
		<value>6000000</value>
	</property>
	
参考资料：
	https://blog.csdn.net/u012551524/article/details/86084256
	https://blog.csdn.net/qq_36732988/article/details/87915449
scp /usr/hdp/2.5.3.0-37/hbase/conf/hbase-site.xml  root@lymaster02:/usr/hdp/2.5.3.0-37/hbase/conf/


备注：
	phoenix执行sql脚本
	
nohup /opt/apache-phoenix-4.9.0-HBase-1.1-bin/bin/psql.py  prodata01,prodata02,prodata03,prodata04:2181 test.sql 2>&1 >> test.log &
nohup /opt/apache-phoenix-4.9.0-HBase-1.1-bin/bin/psql.py  prodata01,prodata02,prodata03,prodata04:2181 DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH_2021-12-17.sql 2>&1 >> DWS_CD_BY_MATERIAL_CUT_PRICE_MONTH_2021-12-17.log &

114、2022-03-18 
dwservice.dws_hand_through_rate_cncprodt  ， 2022-03-17 和 2022-03-16 对比  "E8-CR -CRE8"  --> 	"E8-CR -CRE8二楼"

CNCPRODT	P2021080300966	800-BZS931-B0-00	610000936048	AJFYD-0221080210018891	2021-08-02	E8-CR -CRE8	M05	晚班	CR手工组-蒋冬梅	2000	0.4	125	0	OK	OK	OK	OK	OK					单小燕	10020042		2021-08-03 07:55:59					20	2022-03-16

CNCPRODT	P2021080300966	800-BZS931-B0-00	610000936048	AJFYD-0221080210018891	2021-08-02	E8-CR -CRE8二楼	M05	晚班	CR手工组-蒋冬梅	2000	0.4	125	0	OK	OK	OK	OK	OK					单小燕	10020042		2021-08-03 07:55:59					20	2022-03-17


115、alter table test.ods_eas_new_eas75_ct_ls_match  change  column cfwide at decimal(18,6);

sqlserver 日期format
  select top 10  CONVERT(varchar(100), entdate, 23) ,entdate from t_mes_transfer

116、2022-04-01  【HBase regionserver 宕机】
	3-、租约
	2022-03-31 22:10:15,199 WARN  [regionserver/prodata04/10.0.24.119:16020] wal.ProtobufLogWriter: Failed to write trailer, non-fatal, continuing...
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): Lease mismatch on /apps/hbase/data/WALs/prodata04,16020,1647414832686-splitting/prodata04%2C16020%2C1647414832686.default.1648735717583 (inode 37851475) owned by DFSClient_NONMAPREDUCE_1487528907_1 but is accessed by DFSClient_NONMAPREDUCE_1866585255_1
 
参考资料：https://www.qedev.com/bigdata/145629.html

hbase.client.scanner.timeout.period =600000
hbase.rpc.timeout = 600000
[正式目前均为3600}
	

 2-、提示session 超时
 2022-03-31 22:12:22,234 WARN  [regionserver/prodata04/10.0.24.119:16020] zookeeper.ZKUtil: regionserver:16020-0x27f7955cb8901b6, quorum=prodata04:2181,prodata02:2181,prodata03:2181, baseZNode=/hbase-unsecure Unable to list children of znode /hbase-unsecure/replication/rs/prodata04,16020,1647414832686
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/prodata04,16020,1647414832686
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1472)

参考资料”https://www.cnblogs.com/dailidong/p/7571132.html
默认情况，tickTime=2sec，那么minSessionTimeout 和 maxSessionTimeout 分别是4sec和40sec
所以在hbase中设置超时时间是没用的，必须修改zookeeper自身的maxSessionTimeout为1200000，才能真正起到加长zookeeper的session超时时间的作用
	1-、
	022-03-31 22:08:51,897 INFO  [regionserver/prodata04/10.0.24.119:16020-shortCompactions-1647424847005-SendThread(prodata03:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x27f9d634ccc0fe8, likely server has closed socket, closing socket connection and attempting reconnect
	2022-03-31 22:08:51,898 INFO  [regionserver/prodata04/10.0.24.119:16020-shortCompactions-1647424847005-SendThread(prodata03:2181)] zookeeper.ClientCnxn: 	Unable to read additional data from server sessionid 0x37fc2c9afba02ca, likely server has closed socket, closing socket connection and attempting reconnect
	.....
	2022-03-31 22:09:25,492 WARN  [regionserver/prodata04/10.0.24.119:16020-shortCompactions-1647424847005-SendThread(prodata02:2181)] zookeeper.ClientCnxn: Session 0x17f9d634bd80138 for server prodata02/10.0.24.117:2181, unexpected error, closing socket connection and attempting reconnect
		java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDis
	
117、113主机宕机，当时hivecli一直连接不上，
https://www.liangzl.com/get-article-detail-167626.html

解决： 
namenode的active节点挂了切换成standby状态失败，standby节点切换成active节点，这种情况会导致hive cli长时间连接不上hive, 用 hive --hiveconf hive.root.logger=DEBUG,console 这个命令在控制台看日志信息会发现后台一直在尝试连之前的active节点

折腾了一番没有解决这个问题，后来在 https://community.cloudera.com/t5/Community-Articles/hive-cli-access-taking-long-time-if-standby-namenode-is/ta-p/245278 找到了解决方法，需要在 yarn-site.xml 配置文件里增加一个yarn的参数 ，参数如下

yarn.timeline-service.entity-group-fs-store.retry-policy-spec=1000, 1
在此更改之后，尝试通过 hive cli 连接，初始请求将需要一些时间，但后续请求会更快。

118、树
package tree;

import java.util.ArrayList;
import java.util.List;

public class inorderTraversal {
	  
	    public static List<Integer> inorderTraversal2(TreeNode root,List<Integer> list) {
	            if(root == null){
	                return list;
	            }
	            inorderTraversal2(root.left, list);
	            list.add(root.val);
	            inorderTraversal2(root.right, list);
	            return list;
	    }
	    /**
	     * 
 	2 
  1   0 
0  0 
	     * @param args
	     */
	 public static void main(String[] args) {
	    	TreeNode root = new TreeNode(1,new TreeNode(),new TreeNode());
	    	TreeNode root2 = new TreeNode(2,root,new TreeNode());
	    	ArrayList<Integer> list = new ArrayList<Integer>();
	    	System.out.println(inorderTraversal2(root2,list));
	}
   
   
}	
/**
	定义数
**/
package tree;
public class TreeNode {
    int val;
    TreeNode left;
    TreeNode right;
    TreeNode() {}
    TreeNode(int val) { this.val = val; }
    TreeNode(int val, TreeNode left, TreeNode right) {
       this.val = val;
        this.left = left;
        this.right = right;
    }
}


120

	dolphinscheduler :
	[ERROR] 2022-04-21 00:53:32.331  - [taskAppId=TASK-72-26691-628087]:[417] - yarn applications: [application_1650463733335_0547, application_1650463733335_0548]  status failed 
java.lang.NullPointerException: null
	at org.apache.dolphinscheduler.common.utils.HadoopUtils.getApplicationStatus(HadoopUtils.java:426)
	
	
	yarn : 
	2022-04-20 22:05:05,810 ERROR metrics.SystemMetricsPublisher (SystemMetricsPublisher.java:putEntity(524)) - Error when publishing entity [YARN_APPLICATION,application_1650294960842_0608]
java.lang.RuntimeException: Failed to connect to timeline server. Connection retries limit exceeded. The posted timeline event may be missing
        at org.apa
		
121、yarn任务提交失败

2022-05-07 03:09:51,963 INFO  recovery.ZKRMStateStore (ZKRMStateStore.java:runWithRetries(1227)) - Exception while executing a ZK operation.
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)
        at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:935)
......
2022-05-07 03:12:13,644 WARN  ha.ActiveStandbyElector (ActiveStandbyElector.java:becomeActive(863)) - Exception handling the winning of election
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /yarn-leader-election/yarn-cluster/ActiveBreadCrumb
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
        at org.apache.hadoop.ha.ActiveStandbyElector$5.run(ActiveStandbyElector
......
2022-05-07 03:12:17,646 INFO  recovery.RMStateStore (RMStateStore.java:transition(199)) - Storing info for app: application_1651759818954_2988
2022-05-07 03:12:17,689 ERROR recovery.RMStateStore (RMStateStore.java:transition(205)) - Error storing app: application_1651759818954_2988
org.apache.hadoop.yarn.server.resourcemanager.recovery.StoreFencedException: RMStateStore has been fenced
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1213)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:995)
        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doStoreMultiWithRetries(ZKRMStateStore.java:1009)
		
122、shell ssh 连接查看内存，磁盘情况

-- 测试环境
param="free -mh"; list=("lyzk01" "lyzk02" "lyzk03" "lymaster01" "lymaster02") ;for ip in ${list[@]} ;do echo "**************** 【$ip】主机[$param] ****************" ;ssh $ip "$param"; done

param="df -h"; list=("lyzk01" "lyzk02" "lyzk03" "lymaster01" "lymaster02") ;for ip in ${list[@]} ;do echo "**************** 【$ip】主机[$param] ****************" ;ssh $ip "$param"; done

-- 正式环境
param="free -mh"; list=("prodata01" "prodata02" "prodata03" "prodata04" "promaster01" "promaster02" "proweb01" "proweb02") ;for ip in ${list[@]} ;do echo "**************** 【$ip】主机[$param] ****************" ;ssh $ip "$param"; done

param="df -h"; list=("prodata01" "prodata02" "prodata03" "prodata04" "promaster01" "promaster02" "proweb01" "proweb02") ;for ip in ${list[@]} ;do echo "**************** 【$ip】主机[$param] ****************" ;ssh $ip "$param"; done

-- 查看zookeeper连接数  【Too many connections】
 netstat -nap |grep 2181 |awk -F " " '{print $7}' | awk -F "/" '{print $1}'| sort| uniq -c | sort -k 1 -nr

版本2 【不能使用，谨慎测试中】 ：

 list=("lyzk01" "lyzk02" "lyzk03" "lymaster01" "lymaster02") ;for ip in ${list[@]} ;do echo "**************** 【$ip】主机 ****************" ;ssh $ip "ls /var/log/hbase | grep hbase-hbase-regionserver-$ip.log. | xargs rm -rf;ls /var/log/hadoop-yarn/yarn |grep yarn-yarn-nodemanager-$ip.log. | xargs rm -rf;ls /var/log/hadoop/hdfs |grep  hadoop-hdfs-datanode-$ip.log. | xargs rm -rf"; done


##  ls /var/log/hbase | grep hbase-hbase-regionserver-$ip.log. | xargs ;ls /var/log/hadoop-yarn/yarn |grep yarn-yarn-nodemanager-$ip.log. | xargs ;ls /var/log/hadoop/hdfs |grep  hadoop-hdfs-datanode-$ip.log. | xargs 